{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tesis_Maestria_Ing_Julio_Hallo_201020022065_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SeDYJdaWLERL",
        "a_xP0CFgCHOm",
        "0beDSnLtwit3",
        "4m96dqYpog4L",
        "MABbG_Kgxsy9",
        "5RO4KrHeLwxY",
        "jg95lAUTMFK2",
        "H3k8M5TMSzLR",
        "Dx4DEMTGTYYB",
        "OS0-dehboYcI",
        "fBOsUTdhpZyo",
        "_5PyoQFsroHT",
        "aEqaKHq_te__",
        "zqicU_60t970",
        "sJ018PA10D2i",
        "0WlkWzg80YLw",
        "qfI7qxc-3GMS",
        "15FfYBEv5aMO",
        "rRWrPV677rXR",
        "Ch6VkTg88tTH",
        "bGPYvUUKBRp3"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQ6T9q43gEB4bAOGJUrvz1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulioXa69/Thesis_Public_Repository/blob/Thesis/Tesis_Maestria_Ing_Julio_Hallo_201020022065_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeDYJdaWLERL"
      },
      "source": [
        "# ***Raw Data Set Construction***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_xP0CFgCHOm"
      },
      "source": [
        "## **Data Corpus Analysis and Data Set Construction**\n",
        "2020-07-12\n",
        "JXHALLO: Construction and Review of data sets in order to know if they are independent. Processing Raw Data from:\n",
        "\n",
        "Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "\n",
        "Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "\n",
        " ***Raw Data Set Outputs:***\n",
        "* TestDataSet\n",
        "* TrainDataSet\n",
        "* NonDupTest\n",
        "* DupTest\n",
        "* NonDupTrain\n",
        "* DupTrain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx8eMXFBCXSN"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myTE1ZDTCdw5"
      },
      "source": [
        "# 4- JXHALLO Download Data Corpus\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "url = \"/Users/juliohallo/Documents/Personal/Maestria 2018/Python/Corpus/all_gs.json\"\n",
        "\n",
        "df1 = pd.read_json(url, #compression='gzip'\n",
        "                  lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdo9B_dvCz7_"
      },
      "source": [
        "df1.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULICFom1DBJK"
      },
      "source": [
        "# 5- JXHALLO Download Data Corpus\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "url = \"/Users/juliohallo/Documents/Personal/Maestria 2018/Python/Corpus/all_train/all_train_small.json.gz\"\n",
        "\n",
        "df2 = pd.read_json(url, compression='gzip',lines=True)\n",
        "\n",
        "df2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Y3lNmMDD1y"
      },
      "source": [
        "Left_join = pd.merge(df2,  \n",
        "                     df1,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze1PZwpSDGIY"
      },
      "source": [
        "Left_join.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-v2RCrZDGlQ"
      },
      "source": [
        "Left_join.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzR0eYzEDIiM"
      },
      "source": [
        "# 6- JXHALLO Download Data Corpus\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "url = \"/Users/juliohallo/Documents/Personal/Maestria 2018/Python/Corpus/all_train/all_train_medium.json.gz\"\n",
        "\n",
        "df3 = pd.read_json(url, compression='gzip',lines=True)\n",
        "\n",
        "df3.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIvMZ3x9DLHx"
      },
      "source": [
        "# 7- JXHALLO Download Data Corpus\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "url = \"/Users/juliohallo/Documents/Personal/Maestria 2018/Python/Corpus/all_train/all_train_large.json.gz\"\n",
        "\n",
        "df4 = pd.read_json(url, compression='gzip',lines=True)\n",
        "\n",
        "df4.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iihwu76dDNm4"
      },
      "source": [
        "# 8- JXHALLO Download Data Corpus\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "url = \"/Users/juliohallo/Documents/Personal/Maestria 2018/Python/Corpus/all_train/all_train_xlarge.json.gz\"\n",
        "\n",
        "df5 = pd.read_json(url, compression='gzip',lines=True)\n",
        "\n",
        "df5.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sotx-CCuDQBE"
      },
      "source": [
        "Left_join_xl_l = pd.merge(df5,  \n",
        "                     df4,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_xl_l\n",
        "Left_join_xl_l.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v0en436DTzQ"
      },
      "source": [
        "Left_join_l_m = pd.merge(df4,  \n",
        "                     df3,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_l_m.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq00K57nDaax"
      },
      "source": [
        "Left_join_l_m = pd.merge(df4,  \n",
        "                     df3,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_l_m.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbcYNwW9DduY"
      },
      "source": [
        "Left_join_m_gs = pd.merge(df3,  \n",
        "                     df1,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_m_gs.shape \n",
        "Left_join_m_gs.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm-oefBuDfXJ"
      },
      "source": [
        "Left_join_l_gs = pd.merge(df4,  \n",
        "                     df1,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_l_gs.shape\n",
        "Left_join_l_gs.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goJ785KZDmji"
      },
      "source": [
        "Left_join_xl_gs = pd.merge(df5,  \n",
        "                     df1,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_xl_gs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjlbGg5yDonA"
      },
      "source": [
        "Left_join_xl_gs.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lApIriSUDqtA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kksMpBVP5uZ"
      },
      "source": [
        "Left_join_xl_s = pd.merge(df5,  \n",
        "                     df2,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_xl_s.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUqeCbeCP5ud"
      },
      "source": [
        "Left_join_xl_s.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX6VK1vDP5uf"
      },
      "source": [
        "Left_join_xl_m = pd.merge(df5,  \n",
        "                     df3,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_xl_m.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azq--8jzP5uh"
      },
      "source": [
        "Left_join_xl_m.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY1v7dGhP5uj"
      },
      "source": [
        "Left_join_xl_l = pd.merge(df5,  \n",
        "                     df4,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_xl_l.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K_qcOKdP5um"
      },
      "source": [
        "Left_join_xl_l.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ida9KGY8P5uo"
      },
      "source": [
        "Left_join_xl_xl = pd.merge(df5,  \n",
        "                     df5,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_xl_xl.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxc7tC2tP5uq"
      },
      "source": [
        "Left_join_xl_xl.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHkuAP4CP5ur"
      },
      "source": [
        "Left_join_l_s = pd.merge(df4,  \n",
        "                     df2,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_l_s.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9eUlGRBP5uu"
      },
      "source": [
        "Left_join_l_s.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIo7eL9EP5uv"
      },
      "source": [
        "Left_join_l_m = pd.merge(df4,  \n",
        "                     df3,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_l_m.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3l1b-vtP5ux"
      },
      "source": [
        "Left_join_l_m.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkxe8qwNP5uz"
      },
      "source": [
        "Left_join_l_l = pd.merge(df4,  \n",
        "                     df4,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_l_l.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx6KkJiRP5u1"
      },
      "source": [
        "Left_join_l_l.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN5tQo6hP5u3"
      },
      "source": [
        "Left_join_l_xl = pd.merge(df4,  \n",
        "                     df5,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_l_xl.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Wb_BJ3P5u-"
      },
      "source": [
        "Left_join_l_xl.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoMUQDmeP5vB"
      },
      "source": [
        "Left_join_m_s = pd.merge(df3,  \n",
        "                     df2,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_m_s.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaeZuAtnP5vE"
      },
      "source": [
        "Left_join_m_s.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQZ17nloP5vI"
      },
      "source": [
        "Left_join_m_l = pd.merge(df3,  \n",
        "                     df4,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_m_l.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bIDT4ZkP5vL"
      },
      "source": [
        "Left_join_m_l.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s-l3HsmP5vO"
      },
      "source": [
        "Left_join_m_xl = pd.merge(df3,  \n",
        "                     df5,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_m_xl.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5prJz-czP5vS"
      },
      "source": [
        "Left_join_m_xl.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRMgcJ8iP5vT"
      },
      "source": [
        "Left_join_s_m = pd.merge(df2,  \n",
        "                     df3,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_s_m.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIEjIHRcP5vV"
      },
      "source": [
        "Left_join_s_m.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCB0P8qXP5vX"
      },
      "source": [
        "Left_join_s_l = pd.merge(df2,  \n",
        "                     df4,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_s_l.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc2OFA4sP5vZ"
      },
      "source": [
        "Left_join_s_l.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X16YHYLDP5vh"
      },
      "source": [
        "Left_join_s_xl = pd.merge(df2,  \n",
        "                     df5,  \n",
        "                     on ='pair_id',  \n",
        "                     how ='left') \n",
        "Left_join_s_xl.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtXBV6_xP5vj"
      },
      "source": [
        "Left_join_s_xl.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L48d19-DzPe"
      },
      "source": [
        "# 9- JXHALLO Append All Dataframes\n",
        "\n",
        "# all_train_xlarge_sample.json.gz, all_train_large.json.gz, all_train_medium.json.gz, all_train_small.json.gz\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Version 1.0: 2020-07-12\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECjsPg3tP5vn"
      },
      "source": [
        "AppendedDataFile = df5.append(df4)\n",
        "AppendedDataFile.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNJrW8WeP5vp"
      },
      "source": [
        "AppendedDataFile = AppendedDataFile.append(df3)\n",
        "AppendedDataFile.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1-qohCRP5vr"
      },
      "source": [
        "AppendedDataFile = AppendedDataFile.append(df2)\n",
        "AppendedDataFile.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlqJTWZkP5vt"
      },
      "source": [
        "TotalRecords = 214736+103411+25567+9038\n",
        "print (TotalRecords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9-YPPVEP5vv"
      },
      "source": [
        "AppendedDataFile.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oydx3LXIP5vw"
      },
      "source": [
        "AppendedDataFile.sort_values(\"pair_id\", inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI-cL0qlP5vz"
      },
      "source": [
        "AppendedDataFile.drop_duplicates(subset =\"pair_id\", keep='first', inplace=True)\n",
        "AppendedDataFile.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNj9IOWAP5v1"
      },
      "source": [
        "AppendedDataFile.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51g0OeYDP5v2"
      },
      "source": [
        "print (AppendedDataFile.query('label == \"1\"').label.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdfJvvsfP5v4"
      },
      "source": [
        "print (AppendedDataFile.query('label == \"0\"').label.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4zgtqE7P5v5"
      },
      "source": [
        "#2020-07-20 Saviing AppendedDataFile using pickle\n",
        "AppendedDataFile.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/AppendedDataFile.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpXnoQ-sP5v7"
      },
      "source": [
        "#2020-07-20 Reading AppendedDataFile using pickle\n",
        "AppendedDataFile=pd.read_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/AppendedDataFile.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tih4ZjAmP5v8"
      },
      "source": [
        "print(AppendedDataFile.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-uAyJ3zP5v-"
      },
      "source": [
        "AppendedDataFile.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4rnXiqdP5wC"
      },
      "source": [
        "AppendedDataFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRCNNBwvP5wG"
      },
      "source": [
        "print (AppendedDataFile.query('category_left == \"Jewelry\"').label.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkakOKdFP5wH"
      },
      "source": [
        "AppendedDataFile['category_left'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxe82i_QP5wJ"
      },
      "source": [
        "AppendedDataFile['category_right'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MshJHO99P5wK"
      },
      "source": [
        "AppendedDataFile['label'].value_counts('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0egLmdMP5wN"
      },
      "source": [
        "AppendedDataFile['category_left'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw1sqLdUP5wO"
      },
      "source": [
        "subset_df = AppendedDataFile[(AppendedDataFile[\"category_right\"] == 'Computers_and_Accessories')&(AppendedDataFile[\"label\"] == 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWwk6Ku7P5wQ"
      },
      "source": [
        "subset_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5KKIqpaP5wR"
      },
      "source": [
        "subset_df = subset_df[(subset_df[\"label\"] == 1)]\n",
        "subset_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDjz1JviP5wV"
      },
      "source": [
        "# 2020-07-20: Create Dataframe with lable = 1 of all four categories\n",
        "# Computers_and_Accessories_Dup\n",
        "# Shoes\n",
        "# Camera_and_Photo\n",
        "# Jewelry\n",
        "# Save into pickle\n",
        "Computers_and_Accessories_Dup  = AppendedDataFile[(AppendedDataFile[\"category_left\"] == 'Computers_and_Accessories')&(AppendedDataFile[\"label\"] == 1)]\n",
        "Shoes_Dup  = AppendedDataFile[(AppendedDataFile[\"category_left\"] == 'Shoes')&(AppendedDataFile[\"label\"] == 1)]\n",
        "Camera_and_Photo_Dup  = AppendedDataFile[(AppendedDataFile[\"category_left\"] == 'Camera_and_Photo')&(AppendedDataFile[\"label\"] == 1)]\n",
        "Jewelry_Dup  = AppendedDataFile[(AppendedDataFile[\"category_left\"] == 'Jewelry')&(AppendedDataFile[\"label\"] == 1)]\n",
        "print(\"Computers_and_Accessories_Dup.shape\")\n",
        "print(Computers_and_Accessories_Dup.shape)\n",
        "print(\"Shoes_Dup.shape\")\n",
        "print(Shoes_Dup.shape)\n",
        "print(\"Camera_and_Photo_Dup.shape\")\n",
        "print(Camera_and_Photo_Dup.shape)\n",
        "print(\"Jewelry_Dup.shape\")\n",
        "print(Jewelry_Dup.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr4ygDA2P5wW"
      },
      "source": [
        "Computers_and_Accessories_Dup.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Computers_and_Accessories_Dup.pkl')\n",
        "Shoes_Dup.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Shoes_Dup.pkl')\n",
        "Camera_and_Photo_Dup.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Camera_and_Photo_Dup.pkl')\n",
        "Jewelry_Dup.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Jewelry_Dup.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvmVaMd5P5wX"
      },
      "source": [
        "# 2020-07-20: Create Dataframe with lable = 0 of all four categories\n",
        "# Computers_and_Accessories_Dup\n",
        "# Shoes\n",
        "# Camera_and_Photo\n",
        "# Jewelry\n",
        "# Save into pickle\n",
        "Computers_and_Accessories_NonDup  = AppendedDataFile[(AppendedDataFile[\"category_left\"] == 'Computers_and_Accessories')&(AppendedDataFile[\"label\"] == 0)]\n",
        "Shoes_NonDup  = AppendedDataFile[(AppendedDataFile[\"category_left\"] == 'Shoes')&(AppendedDataFile[\"label\"] == 0)]\n",
        "Camera_and_Photo_NonDup  = AppendedDataFile[(AppendedDataFile[\"category_left\"] == 'Camera_and_Photo')&(AppendedDataFile[\"label\"] == 0)]\n",
        "Jewelry_NonDup  = AppendedDataFile[(AppendedDataFile[\"category_left\"] == 'Jewelry')&(AppendedDataFile[\"label\"] == 0)]\n",
        "print(\"Computers_and_Accessories_NonDup.shape\")\n",
        "print(Computers_and_Accessories_NonDup.shape)\n",
        "print(\"Shoes_NonDup.shape\")\n",
        "print(Shoes_NonDup.shape)\n",
        "print(\"Camera_and_Photo_NonDup.shape\")\n",
        "print(Camera_and_Photo_NonDup.shape)\n",
        "print(\"Jewelry_NonDup.shape\")\n",
        "print(Jewelry_NonDup.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaLt1c4LP5wZ"
      },
      "source": [
        "Computers_and_Accessories_NonDup.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Computers_and_Accessories_NonDup.pkl')\n",
        "Shoes_NonDup.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Shoes_NonDup.pkl')\n",
        "Camera_and_Photo_NonDup.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Camera_and_Photo_NonDup.pkl')\n",
        "Jewelry_NonDup.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Jewelry_NonDup.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295Rj9luP5wc"
      },
      "source": [
        "# 2020-07-20: Create Dataframe for Train and Test with lable = 1 of all four categories\n",
        "# Computers_and_Accessories_Dup\n",
        "# Save into pickle\n",
        "\n",
        "print(\"Computers_and_Accessories_Dup.shape\")\n",
        "print(Computers_and_Accessories_Dup.shape)\n",
        "\n",
        "Computers_and_Accessories_Dup_Train = Computers_and_Accessories_Dup.sample(frac=0.7)\n",
        "print(\"Computers_and_Accessories_Dup_Train.shape\")\n",
        "print(Computers_and_Accessories_Dup_Train.shape)\n",
        "\n",
        "Computers_and_Accessories_Dup_Test = Computers_and_Accessories_Dup.append(Computers_and_Accessories_Dup_Train)\n",
        "print(\"Computers_and_Accessories_Dup_Test.shape\")\n",
        "print(Computers_and_Accessories_Dup_Test.shape)\n",
        "Computers_and_Accessories_Dup_Test.drop_duplicates(subset =\"pair_id\", keep=False, inplace=True)\n",
        "print(\"Computers_and_Accessories_Dup_Test.shape\")\n",
        "print(Computers_and_Accessories_Dup_Test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRdBOX8wP5wd"
      },
      "source": [
        "Computers_and_Accessories_Dup_Train.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Computers_and_Accessories_Dup_Train.pkl')\n",
        "Computers_and_Accessories_Dup_Test.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Computers_and_Accessories_Dup_Test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsa7lsgOP5we"
      },
      "source": [
        "# 2020-07-20: Create Dataframe for Train and Test with lable = 1 of all four categories\n",
        "# Shoes_Dup\n",
        "# Save into pickle\n",
        "print(\"Shoes_Dup.shape\")\n",
        "print(Shoes_Dup.shape)\n",
        "\n",
        "Shoes_Dup_Train = Shoes_Dup.sample(frac=0.7)\n",
        "print(\"Shoes_Dup_Train.shape\")\n",
        "print(Shoes_Dup_Train.shape)\n",
        "\n",
        "Shoes_Dup_Test = Shoes_Dup.append(Shoes_Dup_Train)\n",
        "print(\"Shoes_Dup_Test.shape\")\n",
        "print(Shoes_Dup_Test.shape)\n",
        "Shoes_Dup_Test.drop_duplicates(subset =\"pair_id\", keep=False, inplace=True)\n",
        "print(\"Shoes_Dup_Test.shape\")\n",
        "print(Shoes_Dup_Test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94ka6W3wP5wg"
      },
      "source": [
        "Shoes_Dup_Train.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Shoes_Dup_Train.pkl')\n",
        "Shoes_Dup_Test.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Shoes_Dup_Test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHMGxl4EP5wh"
      },
      "source": [
        "# 2020-07-20: Create Dataframe for Train and Test with lable = 1 of all four categories\n",
        "# Camera_and_Photo\n",
        "# Save into pickle\n",
        "print(\"Camera_and_Photo_Dup.shape\")\n",
        "print(Camera_and_Photo_Dup.shape)\n",
        "\n",
        "Camera_and_Photo_Dup_Train = Camera_and_Photo_Dup.sample(frac=0.7)\n",
        "print(\"Camera_and_Photo_Dup_Train.shape\")\n",
        "print(Camera_and_Photo_Dup_Train.shape)\n",
        "\n",
        "Camera_and_Photo_Dup_Test = Camera_and_Photo_Dup.append(Camera_and_Photo_Dup_Train)\n",
        "print(\"Camera_and_Photo_Dup_Test.shape\")\n",
        "print(Camera_and_Photo_Dup_Test.shape)\n",
        "Camera_and_Photo_Dup_Test.drop_duplicates(subset =\"pair_id\", keep=False, inplace=True)\n",
        "print(\"Camera_and_Photo_Dup_Test.shape\")\n",
        "print(Camera_and_Photo_Dup_Test.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lzR9UptP5wk"
      },
      "source": [
        "Camera_and_Photo_Dup_Train.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Camera_and_Photo_Dup_Train.pkl')\n",
        "Camera_and_Photo_Dup_Test.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Camera_and_Photo_Dup_Test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFu0u6OcP5wl"
      },
      "source": [
        "# 2020-07-20: Create Dataframe for Train and Test with lable = 1 of all four categories\n",
        "# Jewelry\n",
        "# Save into pickle\n",
        "print(\"Jewelry_Dup.shape\")\n",
        "print(Jewelry_Dup.shape)\n",
        "\n",
        "Jewelry_Dup_Train = Jewelry_Dup.sample(frac=0.7)\n",
        "print(\"Jewelry_Dup_Train.shape\")\n",
        "print(Jewelry_Dup_Train.shape)\n",
        "\n",
        "Jewelry_Dup_Test = Jewelry_Dup.append(Jewelry_Dup_Train)\n",
        "print(\"Jewelry_Dup_Test.shape\")\n",
        "print(Jewelry_Dup_Test.shape)\n",
        "Jewelry_Dup_Test.drop_duplicates(subset =\"pair_id\", keep=False, inplace=True)\n",
        "print(\"Jewelry_Dup_Test.shape\")\n",
        "print(Jewelry_Dup_Test.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60IRvUNvP5wn"
      },
      "source": [
        "Jewelry_Dup_Train.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Jewelry_Dup_Train.pkl')\n",
        "Jewelry_Dup_Test.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Jewelry_Dup_Test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoMrNRq0P5wp"
      },
      "source": [
        "# 2020-07-20: Create Dataframe for Train and Test with lable = 0, but ensure 27.27% Dups in all DataSet like in All_GS\n",
        "# Jewelry Dups = 10,027\n",
        "# Jewlry New NonDups = (10,027/0.2717)*(1-0.2727) = 26,742\n",
        "# NEW TOTAL DATASET = 36,769\n",
        "# Save into pickle\n",
        "print(\"Jewelry_NonDup.shape\")\n",
        "print(Jewelry_NonDup.shape)\n",
        "\n",
        "Jewelry_NonDup_New = Jewelry_NonDup.sample(n=26742)\n",
        "print(\"Jewelry_NonDup_New.shape\")\n",
        "print(Jewelry_NonDup_New.shape)\n",
        "\n",
        "Jewelry_NonDup_Train = Jewelry_NonDup_New.sample(frac=0.7)\n",
        "print(\"Jewelry_NonDup_Train.shape\")\n",
        "print(Jewelry_NonDup_Train.shape)\n",
        "\n",
        "Jewelry_NonDup_Test = Jewelry_NonDup_New.append(Jewelry_NonDup_Train)\n",
        "print(\"Jewelry_NonDup_Test.shape\")\n",
        "print(Jewelry_NonDup_Test.shape)\n",
        "Jewelry_NonDup_Test.drop_duplicates(subset =\"pair_id\", keep=False, inplace=True)\n",
        "print(\"Jewelry_NonDup_Test.shape\")\n",
        "print(Jewelry_NonDup_Test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqee9ITPP5wq"
      },
      "source": [
        "Jewelry_NonDup_Train.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Jewelry_NonDup_Train.pkl')\n",
        "Jewelry_NonDup_Test.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Jewelry_NonDup_Test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCRlfdt2P5ws"
      },
      "source": [
        "# 2020-07-20: Create Dataframe for Train and Test with lable = 0, but ensure 27.27% Dups in all DataSet like in All_GS\n",
        "# Camera_and_Photo Dups = 7,530\n",
        "# Camera_and_Photo New NonDups = (7,530/0.2727)*(1-0.2727) = 20,083\n",
        "# NEW TOTAL DATASET = 27,613\n",
        "# Save into pickle\n",
        "print(\"Camera_and_Photo_NonDup.shape\")\n",
        "print(Camera_and_Photo_NonDup.shape)\n",
        "\n",
        "Camera_and_Photo_NonDup_New = Camera_and_Photo_NonDup.sample(n=20083)\n",
        "print(\"Camera_and_Photo_NonDup_New.shape\")\n",
        "print(Camera_and_Photo_NonDup_New.shape)\n",
        "\n",
        "Camera_and_Photo_NonDup_Train = Camera_and_Photo_NonDup_New.sample(frac=0.7)\n",
        "print(\"Camera_and_Photo_NonDup_Train.shape\")\n",
        "print(Camera_and_Photo_NonDup_Train.shape)\n",
        "\n",
        "Camera_and_Photo_NonDup_Test = Camera_and_Photo_NonDup_New.append(Camera_and_Photo_NonDup_Train)\n",
        "print(\"Camera_and_Photo_NonDup_Test.shape\")\n",
        "print(Camera_and_Photo_NonDup_Test.shape)\n",
        "Camera_and_Photo_NonDup_Test.drop_duplicates(subset =\"pair_id\", keep=False, inplace=True)\n",
        "print(\"Camera_and_Photo_NonDup_Test.shape\")\n",
        "print(Camera_and_Photo_NonDup_Test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2XYzTjDP5wt"
      },
      "source": [
        "Camera_and_Photo_NonDup_Train.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Camera_and_Photo_NonDup_Train.pkl')\n",
        "Camera_and_Photo_NonDup_Test.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Camera_and_Photo_NonDup_Test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9pVvh7yP5wu"
      },
      "source": [
        "# 2020-07-20: Create Dataframe for Train and Test with lable = 0, but ensure 27.27% Dups in all DataSet like in All_GS\n",
        "# Shoes Dups = 4,527\n",
        "# Shoes New NonDups = (4,527/0.2727)*(1-0.2727) = 12,074\n",
        "# NEW TOTAL DATASET = 16601\n",
        "# Save into pickle\n",
        "print(\"Shoes_NonDup.shape\")\n",
        "print(Shoes_NonDup.shape)\n",
        "\n",
        "Shoes_NonDup_New = Camera_and_Photo_NonDup.sample(n=12074)\n",
        "print(\"Shoes_NonDup_New.shape\")\n",
        "print(Shoes_NonDup_New.shape)\n",
        "\n",
        "Shoes_NonDup_Train = Shoes_NonDup_New.sample(frac=0.7)\n",
        "print(\"Shoes_NonDup_Train.shape\")\n",
        "print(Shoes_NonDup_Train.shape)\n",
        "\n",
        "Shoes_NonDup_Test = Shoes_NonDup_New.append(Shoes_NonDup_Train)\n",
        "print(\"Shoes_NonDup_Test.shape\")\n",
        "print(Shoes_NonDup_Test.shape)\n",
        "Shoes_NonDup_Test.drop_duplicates(subset =\"pair_id\", keep=False, inplace=True)\n",
        "print(\"Shoes_NonDup_Test.shape\")\n",
        "print(Shoes_NonDup_Test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKnNlYIvP5wv"
      },
      "source": [
        "Shoes_NonDup_Train.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Shoes_NonDup_Train.pkl')\n",
        "Shoes_NonDup_Test.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Shoes_NonDup_Test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpK3QMftP5wy"
      },
      "source": [
        "# 2020-07-20: Create Dataframe for Train and Test with lable = 0, but ensure 27.27% Dups in all DataSet like in All_GS\n",
        "# Computers_and_Accessories Dups = 10,244\n",
        "# Computers_and_Accessories New NonDups = (10,244/0.2727)*(1-0.2727) = 27,321\n",
        "# NEW TOTAL DATASET = 37,565\n",
        "# Save into pickle\n",
        "print(\"Computers_and_Accessories_NonDup.shape\")\n",
        "print(Computers_and_Accessories_NonDup.shape)\n",
        "\n",
        "Computers_and_Accessories_NonDup_New = Computers_and_Accessories_NonDup.sample(n=27321)\n",
        "print(\"Computers_and_Accessories_NonDup_New.shape\")\n",
        "print(Computers_and_Accessories_NonDup_New.shape)\n",
        "\n",
        "Computers_and_Accessories_NonDup_Train = Computers_and_Accessories_NonDup_New.sample(frac=0.7)\n",
        "print(\"Computers_and_Accessories_NonDup_Train.shape\")\n",
        "print(Computers_and_Accessories_NonDup_Train.shape)\n",
        "\n",
        "Computers_and_Accessories_NonDup_Test = Computers_and_Accessories_NonDup_New.append(Computers_and_Accessories_NonDup_Train)\n",
        "print(\"Computers_and_Accessories_NonDup_Test.shape\")\n",
        "print(Computers_and_Accessories_NonDup_Test.shape)\n",
        "Computers_and_Accessories_NonDup_Test.drop_duplicates(subset =\"pair_id\", keep=False, inplace=True)\n",
        "print(\"Computers_and_Accessories_NonDup_Test.shape\")\n",
        "print(Computers_and_Accessories_NonDup_Test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4SkeL1vP5w2"
      },
      "source": [
        "Computers_and_Accessories_NonDup_Train.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Computers_and_Accessories_NonDup_Train.pkl')\n",
        "Computers_and_Accessories_NonDup_Test.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/Computers_and_Accessories_NonDup_Test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIQJIZ8MP5w3"
      },
      "source": [
        "# NonDupTrain = Computers_and_Accessories_NonDup_Train.append(Shoes_NonDup_Train,Camera_and_Photo_NonDup_Train,Jewelry_NonDup_Train)\n",
        "NonDupTrain = Computers_and_Accessories_NonDup_Train.append(Shoes_NonDup_Train)\n",
        "NonDupTrain = NonDupTrain.append(Camera_and_Photo_NonDup_Train)\n",
        "NonDupTrain = NonDupTrain.append(Jewelry_NonDup_Train)\n",
        "print(\"Computers_and_Accessories_NonDup_Train.shape\")\n",
        "print(Computers_and_Accessories_NonDup_Train.shape)\n",
        "print(\"Shoes_NonDup_Train.shape\")\n",
        "print(Shoes_NonDup_Train.shape)\n",
        "print(\"Camera_and_Photo_NonDup_Train.shape\")\n",
        "print(Camera_and_Photo_NonDup_Train.shape)\n",
        "print(\"Jewelry_NonDup_Train.shape\")\n",
        "print(Jewelry_NonDup_Train.shape)\n",
        "print(\"NonDupTrain.shape\")\n",
        "print(NonDupTrain.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BRkFeU2P5w7"
      },
      "source": [
        "# NonDupTest = Computers_and_Accessories_NonDup_Test.append(Shoes_NonDup_Test,Camera_and_Photo_NonDup_Test,Jewelry_NonDup_Test)\n",
        "NonDupTest = Computers_and_Accessories_NonDup_Test.append(Shoes_NonDup_Test)\n",
        "NonDupTest = NonDupTest.append(Camera_and_Photo_NonDup_Test)\n",
        "NonDupTest = NonDupTest.append(Jewelry_NonDup_Test)\n",
        "print(\"Computers_and_Accessories_NonDup_Test.shape\")\n",
        "print(Computers_and_Accessories_NonDup_Test.shape)\n",
        "print(\"Shoes_NonDup_Test.shape\")\n",
        "print(Shoes_NonDup_Test.shape)\n",
        "print(\"Camera_and_Photo_NonDup_Test.shape\")\n",
        "print(Camera_and_Photo_NonDup_Test.shape)\n",
        "print(\"Jewelry_NonDup_Test.shape\")\n",
        "print(Jewelry_NonDup_Test.shape)\n",
        "print(\"NonDupTest.shape\")\n",
        "print(NonDupTest.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHb3VIpTP5w9"
      },
      "source": [
        "# DupTrain = Computers_and_Accessories_Dup_Train.append(Shoes_Dup_Train,Camera_and_Photo_Dup_Train,Jewelry_Dup_Train)\n",
        "DupTrain = Computers_and_Accessories_Dup_Train.append(Shoes_Dup_Train)\n",
        "DupTrain = DupTrain.append(Camera_and_Photo_Dup_Train)\n",
        "DupTrain = DupTrain.append(Jewelry_Dup_Train)\n",
        "print(\"Computers_and_Accessories_Dup_Train.shape\")\n",
        "print(Computers_and_Accessories_Dup_Train.shape)\n",
        "print(\"Shoes_Dup_Train.shape\")\n",
        "print(Shoes_Dup_Train.shape)\n",
        "print(\"Camera_and_Photo_Dup_Train.shape\")\n",
        "print(Camera_and_Photo_Dup_Train.shape)\n",
        "print(\"Jewelry_Dup_Train.shape\")\n",
        "print(Jewelry_Dup_Train.shape)\n",
        "print(\"DupTrain.shape\")\n",
        "print(DupTrain.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGmpCN7_P5w-"
      },
      "source": [
        "# DupTest = Computers_and_Accessories_Dup_Test.append(Shoes_Dup_Test,Camera_and_Photo_Dup_Test,Jewelry_Dup_Test)\n",
        "DupTest = Computers_and_Accessories_Dup_Test.append(Shoes_Dup_Test)\n",
        "DupTest = DupTest.append(Camera_and_Photo_Dup_Test)\n",
        "DupTest = DupTest.append(Jewelry_Dup_Test)\n",
        "print(\"Computers_and_Accessories_Dup_Test.shape\")\n",
        "print(Computers_and_Accessories_Dup_Test.shape)\n",
        "print(\"Shoes_Dup_Test.shape\")\n",
        "print(Shoes_Dup_Test.shape)\n",
        "print(\"Camera_and_Photo_Dup_Test.shape\")\n",
        "print(Camera_and_Photo_Dup_Test.shape)\n",
        "print(\"Jewelry_Dup_Test.shape\")\n",
        "print(Jewelry_Dup_Test.shape)\n",
        "print(\"DupTest.shape\")\n",
        "print(DupTest.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf4cecbWP5xA"
      },
      "source": [
        "print (DupTest.query('label == \"1\"').label.count())\n",
        "print (DupTrain.query('label == \"1\"').label.count())\n",
        "print (NonDupTest.query('label == \"0\"').label.count())\n",
        "print (NonDupTrain.query('label == \"0\"').label.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c86gRBIIP5xB"
      },
      "source": [
        "TestDataSet = NonDupTest.append(DupTest)\n",
        "TrainDataSet = NonDupTrain.append(DupTrain)\n",
        "print (TestDataSet.query('label == \"1\"').label.count())\n",
        "print (TestDataSet.query('label == \"0\"').label.count())\n",
        "print (TrainDataSet.query('label == \"1\"').label.count())\n",
        "print (TrainDataSet.query('label == \"0\"').label.count())\n",
        "print (TrainDataSet.shape)\n",
        "print (TestDataSet.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LopDmFcP5xC"
      },
      "source": [
        "TestDataSet.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/TestDataSet.pkl')\n",
        "TrainDataSet.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/TrainDataSet.pkl')\n",
        "NonDupTest.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/NonDupTest.pkl')\n",
        "DupTest.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/DupTest.pkl')\n",
        "NonDupTrain.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/NonDupTrain.pkl')\n",
        "DupTrain.to_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/DupTrain.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0beDSnLtwit3"
      },
      "source": [
        "# ***Preprocessing Data Corpus***\n",
        "\n",
        "Word 2 Vec Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m96dqYpog4L"
      },
      "source": [
        "## Preprocessing Data Corpus: Preprocessing of TrainDataSet.pkl with Word2Vec v2\n",
        "2020-08-17\n",
        "JXHALLO: Preprocessing of TrainDataSet.pkl with Word2Vec v2: Applied transformation to create matrix (n,501) for word2vec transformation\n",
        "\n",
        " ***Preprocessed Data Set Outputs:***\n",
        "* TrainDataSetW2V_501\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bppdkSQlo7HC"
      },
      "source": [
        "2020-08-17\n",
        "\n",
        "JXHALLO: Preprocessing of TrainDataSet.pkl with Word2Vec\n",
        "v2: Applied transformation to create matrix (n,501) for word2vec transformation where:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctl740CborAL"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd6gZx73pBi6"
      },
      "source": [
        "# 0.1 - Install whoosh\n",
        "!pip install whoosh\n",
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3ivCRC0pFd8"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2v4sqQcpJjz"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWNrPjzupMwa"
      },
      "source": [
        "#2020-08-09 Reading TrainDataSet using pickle from My drive Google Drive\n",
        "TrainDataSet=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet.pkl')\n",
        "print (\"TrainDataSet.shape\")\n",
        "print (TrainDataSet.shape)\n",
        "print (\"TrainDataSet.dtypes\")\n",
        "print (TrainDataSet.dtypes)\n",
        "print (\"TrainDataSet.isnull().sum()\")\n",
        "print (TrainDataSet.isnull().sum())\n",
        "TrainDataSet = TrainDataSet.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_npc-pEpWD-"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the first argument (string)\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.1: 2020-04-10\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/len(x))))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "\n",
        "# Calculates the Wiki-words-250-with-normalization matrix for two (2) pair attributes of a data set, and adds the label and stores them in a matrix [n,3].\n",
        "# Returns the matrix with the calucalted Word2Vec result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Version 1.0: 2020-08-09\n",
        "\n",
        "    \n",
        "\n",
        "def W2V_1 (dset,\n",
        "          left1,right1,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #rows=1\n",
        "    matrix1 = np.array(np.zeros(rows*3).reshape(rows,3),dtype=object)\n",
        "    embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\")\n",
        "    for i in range(rows):\n",
        "        for j in range(3):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = embed([left1[i]])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = embed([right1[i]])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZUrJW8mpgAT"
      },
      "source": [
        "# Initialize prepocess_dataset Word2Vec Matrix with \n",
        "\n",
        "TrainDataSetW2V = W2V_1(TrainDataSet,\n",
        "                     TrainDataSet.title_left,TrainDataSet.title_right,\n",
        "                     TrainDataSet.label)\n",
        "print (\"TrainDataSetW2V\")\n",
        "print (TrainDataSetW2V)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkbJqN6ur7-U"
      },
      "source": [
        "#TestDataSet10W2V save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSetW2V', TrainDataSetW2V)\n",
        "#TrainDataSetW2V load numpy array npy in binary format\n",
        "TrainDataSetW2V = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSetW2V.npy',mmap_mode=None,allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HM9ubuZsDyR"
      },
      "source": [
        "def w2vMatrix501 (npyArray):\n",
        "  rows = len(npyArray[:])\n",
        "  columns = len(npyArray[:][0])\n",
        "  newMatrix = np.array(np.zeros(rows*501).reshape(rows,501))\n",
        "  newMatrix.shape\n",
        "  print (\"rows=%s\" %(rows))\n",
        "  print (\"columns=%s\" %(columns))\n",
        "  for i in range(rows):\n",
        "    #print(\"i=%s\" %(i))\n",
        "    for j in range(columns):\n",
        "      #print(\"j=%s\" %(j))\n",
        "      if j == 2:\n",
        "        newMatrix[i,500] = npyArray[i,j]\n",
        "        #print (\"newMatrix[i,j]=%s\" %(newMatrix[i,j]))\n",
        "        #print (\"npyArray[i,j]=%s\" %(npyArray[i,j]))\n",
        "      else:\n",
        "        for k in range(250):\n",
        "          #print(\"k=%s\" %(k))\n",
        "          if j == 1:\n",
        "            k250=k+250\n",
        "            #print(\"k250=%s\" %(k250))\n",
        "            newMatrix[i,k250] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k250]=%s\" %(newMatrix[0,k250]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "          else:\n",
        "            newMatrix[i,k] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k]=%s\" %(newMatrix[0,k]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "  #print(newMatrix)\n",
        "  return (newMatrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilrKBrKRsMBo"
      },
      "source": [
        "TrainDataSetW2V_501 = w2vMatrix501(TrainDataSetW2V)\n",
        "TrainDataSetW2V_501.shape\n",
        "TrainDataSetW2V_501.dtype\n",
        "print (TrainDataSetW2V_501.shape)\n",
        "print (TrainDataSetW2V_501.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiwT9WXLsUE1"
      },
      "source": [
        "#TrainDataSetW2V_501 save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSetW2V_501', TrainDataSetW2V_501)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5fQGNdvzEa6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MABbG_Kgxsy9"
      },
      "source": [
        "## Preprocessing Data Corpus: Preprocessing of TestDataSet.pkl with Word2Vec v2\n",
        "2020-08-17\n",
        "JXHALLO: Preprocessing of TestDataSet.pkl with Word2Vec\n",
        "v2: Applied transformation to create matrix (n,501) for word2vec transformation where:\n",
        "image.png\n",
        "https://tfhub.dev/google/Wiki-words-250-with-normalization/2\n",
        "\n",
        "**Preprocessed Data Set Outputs:**\n",
        "\n",
        "* TestDataSetW2V_501"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9evpyuazN8k"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBGc4zmuzQFh"
      },
      "source": [
        "# 0.1 - Install whoosh\n",
        "!pip install whoosh\n",
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9pweVrPzVMK"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vMZq7a-zZgJ"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USEPFN3pzbof"
      },
      "source": [
        "#2020-08-09 Reading TestDataSet10 using pickle from My drive Google Drive\n",
        "TestDataSet=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/TestDataSet.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3yFIeWnzeDL"
      },
      "source": [
        "print (\"TestDataSet.shape\")\n",
        "print (TestDataSet.shape)\n",
        "print (\"TestDataSet.dtypes\")\n",
        "print (TestDataSet.dtypes)\n",
        "print (\"TestDataSet.isnull().sum()\")\n",
        "print (TestDataSet.isnull().sum())\n",
        "TestDataSet = TestDataSet.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Fx-3jmz0kG"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the first argument (string)\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.1: 2020-04-10\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/len(x))))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "\n",
        "# Calculates the Wiki-words-250-with-normalization matrix for two (2) pair attributes of a data set, and adds the label and stores them in a matrix [n,3].\n",
        "# Returns the matrix with the calucalted Word2Vec result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Version 1.0: 2020-08-09\n",
        "\n",
        "    \n",
        "\n",
        "def W2V_1 (dset,\n",
        "          left1,right1,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #rows=1\n",
        "    matrix1 = np.array(np.zeros(rows*3).reshape(rows,3),dtype=object)\n",
        "    embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\")\n",
        "    for i in range(rows):\n",
        "        for j in range(3):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = embed([left1[i]])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = embed([right1[i]])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyfqHV2Nz4xx"
      },
      "source": [
        "# Initialize prepocess_dataset Word2Vec Matrix with \n",
        "\n",
        "TestDataSetW2V = W2V_1(TestDataSet,\n",
        "                     TestDataSet.title_left,TestDataSet.title_right,\n",
        "                     TestDataSet.label)\n",
        "print (\"TestDataSetW2V\")\n",
        "print (TestDataSetW2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCmdYz-7z7CH"
      },
      "source": [
        "#TestDataSetW2V save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/TestDataSetW2V', TestDataSetW2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQGlukjF0BfL"
      },
      "source": [
        "#TestDataSetW2V load numpy array npy in binary format\n",
        "TestDataSetW2V = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TestDataSetW2V.npy',mmap_mode=None,allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXuHSaQf0DxQ"
      },
      "source": [
        "print (TestDataSetW2V.shape)\n",
        "print (TestDataSetW2V.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fnMXUsO0FhN"
      },
      "source": [
        "def w2vMatrix501 (npyArray):\n",
        "  rows = len(npyArray[:])\n",
        "  columns = len(npyArray[:][0])\n",
        "  newMatrix = np.array(np.zeros(rows*501).reshape(rows,501))\n",
        "  newMatrix.shape\n",
        "  print (\"rows=%s\" %(rows))\n",
        "  print (\"columns=%s\" %(columns))\n",
        "  for i in range(rows):\n",
        "    #print(\"i=%s\" %(i))\n",
        "    for j in range(columns):\n",
        "      #print(\"j=%s\" %(j))\n",
        "      if j == 2:\n",
        "        newMatrix[i,500] = npyArray[i,j]\n",
        "        #print (\"newMatrix[i,j]=%s\" %(newMatrix[i,j]))\n",
        "        #print (\"npyArray[i,j]=%s\" %(npyArray[i,j]))\n",
        "      else:\n",
        "        for k in range(250):\n",
        "          #print(\"k=%s\" %(k))\n",
        "          if j == 1:\n",
        "            k250=k+250\n",
        "            #print(\"k250=%s\" %(k250))\n",
        "            newMatrix[i,k250] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k250]=%s\" %(newMatrix[0,k250]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "          else:\n",
        "            newMatrix[i,k] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k]=%s\" %(newMatrix[0,k]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "  #print(newMatrix)\n",
        "  return (newMatrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6fPgDEC0HtI"
      },
      "source": [
        "TestDataSetW2V_501 = w2vMatrix501(TestDataSetW2V)\n",
        "TestDataSetW2V_501.shape\n",
        "TestDataSetW2V_501.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5eSZaOn0KOr"
      },
      "source": [
        "print (TestDataSetW2V_501.shape)\n",
        "print (TestDataSetW2V_501.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U9ds9Jm0MEc"
      },
      "source": [
        "#TestDataSet10W2V_501 save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/TestDataSetW2V_501', TestDataSetW2V_501)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V-ELxPHGfN9"
      },
      "source": [
        "## Preprocessing Data Corpus: Preprocessing of all_gsDataSet.pkl with Word2Vec v2\n",
        "2020-08-17 JXHALLO: Preprocessing of TrainDataSet.pkl with Word2Vec\n",
        "v2: Applied transformation to create matrix (n,501) for word2vec transformation where:\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAdQAAABFCAYAAADzahc+AAAgAElEQVR4Ae3dBbh1W1cX8Gl3Y3did3d3i10gFhYKWKigiAEoCjZ2YysWgt2KoKBid3criijPb5/5P4x33jH3Xnuf897vvffb43nOWXuvNdeMMUfN+u8x7nTnwJ0Ddw7cOXDnwJ0Dz8KBrzPG+JbL3zcbY3z+McaXubGErzjG+AFjjK81xvg8V+TxqZq03b0m2cVb33CM8akvpjqe4OuOMT7d8eSPKT/3GOMrPH77pA/4/X3HGF+v8D33vsQY4xo+fNoxxreeWX+yMcbXHGMoN/TJZx3Uw98XyINZ9lcbY0iDvskY47PMz7devvGV9b+1nHPvRSY/87lEb4FnVc/Sx/XeW6BKF4tMPS8mfMZ+f6PoL109QtfwcM3vOeTjG0yb/xlm5mx91fu1zLcZY3ztYhfq868xxkg+7n+aMcbXL/bxy82yvlB96Q38+Wzf/cIxxu8ZY/zNMcY/np9/2xjjh44xfvkNjWa4/9sY45eMMT5wjPHtD+ahLIa30pceY/y+euMJn//llc79UlF/a4zxJS8lap5/zzHGb27u/7Uxxu8YY/zEMcZ7zee5992u5MNPmM75s44x/v7s078+xvgZM18C/l/HGH9h/v20ef+XjjE+Zozxt8cYv27e45jfZ36+9fLvxhif49aXn+G9KpOvmkP97qW/8R3Ve/PWK3MR9EVeLlXqrVF/OaVLdA0Pu7yeQz7+yRjj14wxPu8Yo9P7Wi57wt79zjHGP5jv5LkB2f8dY7Ap6BuNMf7ptDmuZOB7jzH+6hjjnWeaN/LlsEwz5L+itDQO1Qjme4wxPmN59kXGGN9lM+L7UdMxiyi/yhjj840xvvAY4ytNI/8FZ4f8wDHGdx1jfPoxhnsfPcZ49/k9Rf2YMcbfGGN85emYM2pi5FMf5Xym+R7H86Xy8nLlUL/obIv8kDwIQIhDF2mp9xcbY3yvMca3GWMY5SEOCi+MIuNQv+psm7Spk0iu5uvdrz7G+H5jjB/ROFQjyP8xxvg+s45GU/Xej518cO/bPlTl9J8yiA4rcVx4qc4/bCqL55wKJyp6VI9fX1+a7cWjTzlHFf+iBAx/cYzRRZfKV2dtC4/059oPcaiMTWYsyAU+4zelNJvB0Hz2yU+BR5zfrj/I5juOMcyonKMqk2t/dfVV3hefbVM3dVSfzljiy5efbf5Wy4iMkVn15JuPMX7QGEMfI+/7TCb/3xjD89zznMybJajytOPHQ44P+vYdxhj5y2yF56tsrrrZlZd8zcj8pjHGb506637XxqSP/qq7EQvSp2bEEFnDM9T1w3x0unT6q+54j8w+fYv5ecefI/qrTvi22pGj+uv9ri3qRlfZvPDQd3If0hY2qhJ5YG/ZS7KLIh/k0kyZfN6+9Ik0u3Y85DAGh8rusnM7vZfWaExQnlkszpedRuydQP0/FYdqUPCD53PX3zU/v/8VDnXV0a4t+MK5ff/JH8Wssuxe56tWHZRuV8bK+8h0fEiX16nJnUP9L2OMXznG+BNjjN97SjXGe85RzS+ejNYplX7R7CDGnOCITn74GON/jjH+wHSMOvP95ohM3ir3D+eo6LOVzH7BdM7feToJwqe8/z+jeILLWBPCfz1H1H95jGHUvRKhybP/OJ0AY0AgQpwkRfotY4x/NsbQFqMGvOG0jeK16c+OMT5hOpwPHmP8qzHGH5rO4o/Odhnl/7mZMcHSPsJI+NYRKqf0cWOMnz15JbCp94z0zR4wznhHudEHjDFEj5UEQtKvJDBQT47v540x/tIYg6P8+TPAYXR/f3npQ8YY32l+/+ljjJ9cnvmIT3j0y6Y8eM5Jdv0Qh0rZOFvEQeGzfAQT+PcHxxh/b4zxJ+ezD59pu/5gfJSvbh8xHuRyJn/Npcpk7S9BXlfflEeO/v3Mn7wrTzBVCb//15TdPzbr7Tkef+Tkj74zFUgn/sqss4CFoyQb+puMk2vBU+7JR9t+9xwdeFf/pX5VPmud6AlZ8vexs58972Sz6qZgrSsveXNIf3z2D53t2pi0rtFfRo/cM86COYHD55rtV6ed3NS8Ov39kVNHpRMwSoM6/hzRXzL1d8cY+vrPjzF+7szvGv1lgDuZogNGaeQvPOS4/0MZmAiEBc2V5KXf6el/nsFn5CO6w4ay02SVTcSLrh013zjUc3pf0/ss2DKrRWaQEa5glQ3NCJVNScBEpg2I0DUOterork/YMvYYX/Q7XV5lufNVnQ7u+NXxPjKtbV1es7kPTmMdoTJyiMP6N5OhCqF4jDqGvstMkwtFY0hQdah/eN7T4UZK3hNhRIAo1jrlK6plXNHPHGO8x4zuGSiGnECahmA4dBgSAXOYyqmE6SnL6OydphDsHCpjgRiA3z4jSw4Zicz+T3GoHC76stPo4o0/CiLaMo2akeXPaRyqdykLpY+y1HuVD6aDGXrGiVAxRpU4WYamklEJhf6O86b6vuuMTn/DNOjWvI08QoySe0hQpC8r6Y/IizoYoe/64ZJDtUQgQhQsCVQYWyPp/z0LVJe1P77pDC6Maq3/nJt+rzJJWdNfu/oq7yfNsn/1NLC+Sp/oPLzgUBkZpP/IhahfMGq0jRhmym3KnfFjxBgK7az9beoM5R7D9FHzngtjZbTX8aMke/xouYWu6J+dbDJC0c1deY8ZjjF+6gwIBL5dG2vaKrd/ZoxhNom8qZOAijFkkHf9UPPq9PecQ13lxcjwkv6aPTJwoLvWGNk8cnmN/u7awm5mNBoeat+fnsEUhyD4WUm7s2z2q+YMV+SDQ/34OXPmPSNJ/bxrR807DvWc3tf0n6IMJtge/PwjM8CrDvVHTwfLvggg/vnM5FqHGh3dtQVfMrtBvwSXVZY5/85XdTp4royV91Wmu7weeaYBMZBuMhT5zlAw+IbUlJ5Dyp8hcKVqvKpD5QBDhJVho5AfNh31JYdqWsPIkNEX8f6dWT8Oz2jwhyTzORrM9Ehu6wBTlIgiGwmIqkQ5IQJJSBksTgSJRIwQKL77ISMpRlw7GAdkWoYAhTeujJTIUb6IA1hHqO4fdaiMNUeqXjGEDzk//McL/AkJUv7tDD5yz/QYgUOmx/DGCDYBlPs+c1pIMKB/KomIf1y9caYfqkO1/oP0G37ii2gaMdIi9hDnirr+cF87KbURIoe1oyqTtb92cqM8MwRIgMJIIGUI6irRE7wI6X+BnhFoiNEy4rdZTB4MjaASz2McpV0dqqCvykoC2B0/Up6r6WkGRdvRTjYZoejmrryZxekSZyCI6dpY01bj8+PHGD9r6iZdZVvM/giidv1Q8+r0l0O1VwOZkpQGdfw5or+/dg4Gqv6q3zX6u2uLvjASQuGhz+8wZ7QEAHEiD6ke/td2C6TZ28gM3WEHQ/ZcWD7YtSPpXONQz+l90rMXpm7pmmlmZH2UTRAgsV3k+3POZ5ZgOBvLYwIpdK1DjU3dtQVfMpjADwOrKss7X9Xp4Lky4jPC+yrTXV6zuf0INZuS4lAlFkWZt0em+awHVarGqzpU051IVG5qUZQjX6MQyqCzMlUwk57W1jJtaqqLEGCkDqaMjC8jzDhzepyEyF/Utu7A9V7W7+JQOSfGHmPsnFOXOFQjLhSHipGMpZGvaR3TVnGoBAfJX1SrXSI6is2xE7wEHoxiNZLz1bMO1Tpe+CC9UTsnFKFLHq6UlTAjzlJ90l/z9mnqXVSGRJKcDMHheCl92hF+qbupjkqmn/WjfjFqEuzs+iEOlfK928yEPOAPfmczzjmHuvaHtYv3nXmRG8EQ0o9kq1KVSW1Nf+3qq14p74hDVX8yaU3X9LXy1Ue5ZJKR5Uw4lQRqRjJGHDGO6kv+5JN7jLlpZjMiDJmRnbW4Wr/IZ20vY0JWE117lj5dZZMRim7uyqt503ltQV0b56PTpcqtfuaY6AJZE0ybykS7fpiPT5dOf9meLFPoU2lQx58j+isPQao+Y6wtUeH7Nfq7awuHyvahykN9S0fZtjiImex0qe2OUY98VN2ROA51146abxzqTu/Vy85eRGfwtOoVp8kZ+yOX5IjNZXtil8zGRUfjUI34sx47s3/Nperori34Ikhkfywl/ZRZh8iyTDtf1enguTJiA8P7KtNdXo+NsRYXB+qmKDLfq0MVfVMEw3zrDOvuTUYkilIdaiJJeftsTZFCigYR5ot0TH2EGHfTt0mjPhwvss5B0JH6/cYZrRFOjF5JNB3mxKFK86HTaGnPP5oGnsOLQbWOyFkjTJWPtTKGLg41oy5pTBVai9WZqbdpGEJnNExhO4dqjYkQR1nklXsrH2xA8EwgsJLRZDYCEDTrcox8/kSRHK1peY5AP3AEyGhL3bUxAYD7RmAMRSXKw0HKQ7usB+76gaMmJ5yg/vQOB0xJ8dB3xKEyuqGM2Lr+UD75U7b6Ws9A3l9nJ6pMUtb0166+tTwO1egGGZ12I1TlkwdGk3IiU2IMnH43zWv9kWyTeSNUsmYJova3qWP36z3l4Q8DQuYZkFq/Kp+z6NPoz9SzHZny88c4drLJEFbd7MpLvq7kiyx5r2tjTbvKrXZwJsjamuAP7fphPj5d8HjVXw6I/AouzXThP9rx55L+CpYFvHRAnpmZuEZ/d21Rt4zgKg/V1476brbJs9pu9VenyEfVHWnJlRHqrh3ShOJQfe/0XsAtoGAb2BDyFBuiHpVMiWcNla7RSfaFfc7SWxyqdJZ4zlHV0V1b6AO5pl/ssXSrLHe+qtPBXRkd76tMd3mda9fZZwTnKeT9TB8kn/W7+6IiUfsR4pAylXkkfdJY62KojpAyLpHRaQSppj3ybk1fP1c+2H5uzaAjvOLMjdQuEQe2knKsX4Y4AsZl199dn53rB/nv8kqZ11w5alFviJPPSCD3Ll3P1ffSu4yRQE8etR55LzuV892VUnZEBju5wbN1xqV7/8i9nWzWdy+Vpy5Vz7o2Jj95vUz9xbOsVafMc9cjOqgP8GmlI+/mnUsyVXno2GI26+X957ju2iHv6lB9109V7zlrAdyttMpEHKo6ae+1tLaFQzW6Zp8uUWdvOh1cy9jlu8p0l9fu3fv9V4wDRp0iJ1vnd2QaMJsydmmO3jeicDTljUI2YL2eZLRQp5lez7LvZb2xOSAYsB/COmMXjL3M1pm9MFvCcXbk/jWBSpdH7r33nDky6yXQyGg2z2+5WsO9O7JbOHd/5wUOiKKOKN86Ff9CJld8MYo9Onq/Itt70jsH7hx4BhSyOxNfQQ4YwiLz5c/p9ZOvvOvnh9IeprFyrs/GlqNTQ7v8ku967cpe09zy3XqFHZzr9MYteZ17J/V/bvi1XZmmo9IvuzRH7qu3AOA58jpSXtKEX0+BaLsELUlPsvaccp96rXy/Vh9Stt2363T/tXwI/zp5q3VMmS/rmnrYaJj105dV1qV86bqlhGt4uZORtOtSmXku/VP0yLqmo4+BEE35tX9zz6mAfE75l67hzS6dZaG3K3ZA/urDLljnPDJtK+9ar/o55aZvcs39S9cur0vvrM/jC05nQQPvV3fHrS/c8j2bTXZwTXYjZpORqY+jztya1XpmdVe/p8J87fJV10AsvkyHWuvvzFo2Nuzq9Rz3a7/cml/6aN2JeGt+17wXucv65jXvJq2NKbvpMGnsZLT55Tmp8v0afah1cASmHiHz7Bqouipv1qlWR1brWMt97s/VZli+qDuWn7usI/kFlOQaXu5kJPJ5pNyn6pFduDbV2WluLbH2b+xJvZcd+UfqljThTb6vV5uYnKG2YS/t4UjVCcCMgOkSVXmo9a3vpW9sUMxRsPp89/ma/ujyqL7gtJsz8H4cqm39jiHk0GwyWCHLcj9XEb31JIJvilBUFCg1c+cpww5Mu6PsouUc7A5FDAg0Gef/Ysgob5ht9OrIgK39K0zhrm6iaTuNA5UW+K8oJ2NvhKmDsuHATtQVbkr9VogpQmqHmSgPMTzaVKdaa3l21l6Cs1vrs9ZfHbJxYC1PFKgPOhiyWcXT2cQYSDsk1QkZaVUIP9Fw+mXHD32jvYKJ9bhT7SORG4EldHYK1y3zpq4plnZ3pGxHm/QR3ok85SH/kKiboobvVe7iULVPmvSxd1f+uSdSlk7kvHOoDv2rg12Q1aF2MrjKjDJspCDj8qEn+EgfyCC5D987ffB+xzMRNn11lMmO7NWhHoGqk/cqbxyqkQSe4CuqsqEtdtTb7ev+Sqtc5XnHq5Uv1WaQ0xwpUc4KbbmT0ZTnuuqWe95jqJ1npvMIr9gcZeBpRi9xGuGltF1fuL+TEc+qfPretcd9dFSPVh2Yr58udMCuWbvQ2d5qD7Xfu/Vedajn8q1lhDfurTyht5ypUxz6MbabbdTHTmesssPW2wnNMbJnKPIAHKPWt9rY9E0cKjmzn0SdUAd9uvbHWv/56gsXa8AVUvQFX1ChlDhUi712ZAEuAH6AbH/GNBFFPQ85H5+2wtuyDcWH8bT1XkMCpVbL0LmMBeAADc7BZPc4XVv4HbPAyA4JBQNslbfV3PreuboxkIH58h5hUU+d6FwagwHxRx7SIdvbV6gvHeQemDsbghi9Cmfn6IJdc45YWOjPOaxaHt4SatGaox0fMXfRuceAd/VZ6+9sIcPTlcdg2Na+wpDNZp0uVfDVUZ0YD3WoEH4Cm/RLxw+8dN8uQO1Yt8HXPmJcnK90XtABaseaBAUcrN3D8qgwb7W+ynCURFTrzDHFxGNHbxwZgjjE8TkGRe7wpcodY+K8IyeDL4HP7Pgnerfdn7JWaMlaHwEj2bMJybGlONROBjuZ0Ud4LXrWLhu+bPUPLKegJnzv9GHHM3W2sUV/OoO9OtQcs7gkI6u80Y8VrjOyYRcsma8QopVXnVx53vGq40u1Gdon0OdUyQR5SL3k2clorUunW57jsT4nP9oqsCEzHZRkdCe83PXFTkZSnyqfu/Yk7RE96nQg77tyThyWwJe9rPaQPXnb5V4c6qV8axnhTccTQT7dJJv0LrY7O/G7ZT6zlnTVj3LQdcsfkQfBaG1DtbHpG7rHJtFPulyPEWY3dexf7Y+u/rWdPpstWSFFqy84GfLA+2l0sGEheHCgO8iyWpAogVIYjUBA4TRRzhES6JThWRBBopzSEm75IJWWT+dQPaeUpnyP1K2iklC8wH9ReId0RVBGG54hipURbKC+Oogpwh6IRQ4hsH4EkfFGtTy8PQdnt6tPrX8calce/ncwZLMqp0sE35cIVAfhV/ul4weHksP9+nZ1qPJPH6kXVKBEiYywdT4872Dean2VLehCdiVah0HaT1FBQvrlE06brEBrQZE7xjHoT4HP9Lzjn1FWB033kOPD/w6CbieDncx0cI0cas4gVr53+tDxzCyPfs+yA0d9zqFekpEqb/i/wnWmjox0ByEafnVyteNVx5dqM+JQd7B+nYymHq473fJeZuICX0dm8BAJspy/NPsR3YnR7vqCjHcyMrN7vEQ+d+15THhAj3Y6UPOon2v/xp7Ue3Go1+Qb3ux4Qs/NRKHYhfn1NRfHscwA4bmBBl2lI1Uean2rjU3fcKgZ+Bll47eBSOqp0Ng/n9Mfu/qnkgKSDm6z+oIXKsroB8kFAzBiB1mWQlxB0BFOa7He0RkoFa3M4FA1BkU5fWZARAiIUDtPuIMWS6ccqdvK/JQh6jEa5+g5CKNipB2Yj4wmTTV0EFOViUZd2oJM4TE0SGenPLw9B2e3q0+tfxSgK4/jyuhG2UFNeajJw38CFVADQqtOaIXwq/3S8UM+OUoj0rzkUGu9BCF4t4P7mlU6XZSdqT5TRdkEJGAx1e5gN0D9QMVl3SRyR45W+EwZd/wzbRN+SBNoyVofMwv4jEzRioB3MtjJjJHyCtfIWKTele+dPnQ8g1iDT6FuyjeG5oiMVHmrfR8wlFrHDkI09XBd5WrHq44v1WbEoQrwa7BgtGMas9YzOlvrsdOtKl94BL6OzKhPiM5xyDHG4WXXF6ZoOxlJXrlGPnftSTrX2Lq176JHOx2oedTPtX9jT+q9ONRr8g1vdjy51qFaolO+Kyxrjq7KQ61vtbHpGw7VLEZIP1s62tm/9Meu/slnB7dZfcEL8H4MShCC4lCtNTFAojXTPNKsSDQckwYjzinIQoFSq3BNHKo1GVSVkwGxVmZdyfsM/w5aLDCFR+pWYb4wP1MNHLaOQepjagFhftYZo5wdxFRlopFspscFAYmOanmVtx2c3a4+tf5RgK48CmdqJdQ5VGWs0H/WUQIPFgi/2i8dP8xemMLlNAQcnUNNH631iiHA8w7mLfV3rWV3DpWsBazezEDALiJ3jGMUi/xC4kId/8gv/hp5VWjJ+crpwrhpLwqE5E4GO5np4Bo5VO1Ale+dPnQ8M01rytjskOkzMlCdjnxjaNa+6GSkylvl/+pQ6REZsMaIt3he18c7udrxquNLtRlxqILADmK01jM6+8DRh/873fIeJ8/mMLbg68gMHuKlAM4yijbGaYSXXV/Yx9DJSK2Lz5HPXXtq+kt6tNOBmkf9XPs39qTei0O9Jt/wZseT6lDTnlqn+pnjw3P890dG6VyVh1rfamPTNxyq98zasNOm9VFn/9xPf+zq//D2w39LevI0khYQmamrvuA0guJMRPki7tWhyqaDLKuFcEwielNmlMw6JTJ1ItIwSksZHGpGSasBoSyiTgpEOI1OfF+hxSpM4aW6VZgvzM8uWVM98ub8QGwZVTrI3MFNdRBTmBiIRYpHkfBAnp6hWl7lbQdnt6tPrX8UoCvPhpbqUANDNqtyujBy+kG6QP/ZiGJahdBpO2Gs/dLxw+YeU2Q2ZWkXJVwpfSQwq/WKQ+W4Opi3mk8tm4JkhGoNxQjVhhJOWd39GTGhyB3H0jnUjn/eY4yVWaElH3J8+G/KUpuVpVxtR50MdjKD15QaP+RhLZ5DDfRf5TuHuurDjmdmPgS99M+uyZ1DPSIjVd4q/1eHqt0dhOgDRx42OK1yteNVx5dqM+JQOe4OYrTWUx8GMjB12ekWh8o+Vfg6DlV+1rrpLyOLzLpZf4vR3vXFTkZmNqdL5HPXnpr2kh7tdKDmUT/X/o09qffM1JluvSbf8GbHEw41y2hpD/3YEXtM3/2ZQbR+WuWh1rfa2PQNG0YPvE/PshzX2T91SH/s6l/r2cFtVl9wSisCEw2co0uQZSpjxFJJ5Oc+OlKGdEn/8NZD9NghdnC4oUt1o7AVKq2+Z479KGX6dpc+61i755fua0dXn139bykvI4q1LpQo65zrs/U7gSZYiJASyI5qH3XP3dPf2v0UYpgqVbmr99fPHf/qTuA1fb53aXYy2MnMEb6krFUf3O94pl+zAzzv3nrdyVuXH96fa08nVzterfnsbAb+d/rc1S/3Ot3iUC3vGOWHMquhjCP60PWFvDoZSRmrfF5qz8qX5FOvqw7UZ+vnrn+7e967Jt+Us+NJnh9pjz5Z+7jKw66+KcOVPnR5rG1a++NS/eXd2Y5a9v3znQOHOWBqzwjYBgwRvnW8O9058EbjQAdfZ5STKfg3Wntedn3Nyhgx1r8Pf9mF3vO/c+CthQP3KO2tpafv7bxz4M6BOwfuHLhz4M6BOwfuHLhz4I3CgWtxE4+062XkeaTcXRrrYNkQs0tj16NdsuufHaMVM3P3/qt2/1KbrWc8F+Ftt9Z4a/7OqFpjuZYutVnfOgtb+zP3lHUtT+w5sDsZWcdx5rPywQwA3uQva33WhZzPdYY6ZINTt/6e50evjn7YkPIqUOXtq1CfWgfy5ZxkSN+v9/Lsluu1snSujOe2px1e9Lny88w+Hbqzo+DhwgrISYvcowvX8uTa9Or1Fpe5LOLvmHTL/WAy3vLuy3jHudns6Nzlz8D5nT9/nzB/uNxnRwLqtv7d+6/a/XNt3mFp3tqGeqTo1jzqezvYwJqm+3yuzRBV7Px0TCcYqPWe3cRH8aVTNhAVvOQobccnL37oHigEcgTLTkybvvzZKckoWb+2q9yZR0fLEMcMWeappMwcP3pqXk95v/L2Kfm8rHedN3cEBKXv67356KaLgCb46jdlsLz03Dbauel1J/lSZPtVsGgHdUeeOX5n5/gHTlmv96xtA3E5Srfw8Nlkzm42xxucUwtRXLs5bUEOnqJndsVVLNJ0lijaFvPd7rYdRmZXTjAZ7QZz/ghkFRzGlSoOavBdOTYHuSutdfbMTq4VE7TiQkpzDpOz5r9+Bl0WcAjPONQVEzXvdPXNs1ztpGU4RWuhrs88s23fgXrlhUR7FWcYX3e4v5faLAp3jMGha9GmaBUpL32kvPq5lj2Tv3DhUFcMaXJU+5GzUm94oM6qyRPwt5Ed4pTIX8XhPSofl9osb4f9beeH46k/zEjknk1ZwSgVXTv+ElLvbq3Z7lfvqD8dy0YYRsQRLjsUnZd15reS7879IXXhcEOgGelOR0a1jhUJGiC+hFZc0jjUc/w3MmYD6KY+kWf6W/lk4tvNPo09oA/OfO5wf1Mf15Xfq16u8uwdfXIJ97mWER3AE3KjzFCXP/vW4V1XvF19Gixmea283fEm5ebq3CUYVvKf8/1kyOgJ0f0gPHV2bCZ7vOxstPwyitM+9dvVUTplGiCQew4VD1f7bEZOGnY95B6bpC07h1rxcIPdnHv6NjaHLwranfyBblRblzLDwxxh7Po0aV1XmXNv7T/2RHlI+4Ls94J8UuAOU1X05fxQxVPkcFcsUp3V4aXOch8v0nUYmV05OU8kyvi4eWbSYd6VnOELDirnL2pnbBy4DbhCV+cdhqbzlKJ/Z93UwZlShq7itq516L53DjXYo86BMuCoq+989HhhbFcc4V2fGdnrH/iqrhH2FWcYTzrc3yNtJnjB0oT6Y+cfYvyd+UJGWM4md5i5M8kLFw51xZAmzEZrIaNO9ZZW24yinOcEY2kbvLIpXcXhPSIfR9psyuxD53ldRoUTo/S5905TVuBLOwPJ8SJO17m+7jgLfRCJryRIcAaVo9XvkKCcqXuXqcTO/75reYnzzVQvPOZOTyTHL8rNMxgAACAASURBVGeAnSdVfwa6wyWNQ73Ef+c9O0xq7VIn520/bMqHIyvncH9Lc04fV35XvdzJlPOIRvFGix3u81oGWXKg/09N2f3YKUdd/owomevwrhlsdkLfA7wJKljH2443a718D24tJ8T2cGZAIPzYCJnitNmOnR1b81RuZ6MD4iC9Uad0uzrSrRUverXP7zn7AD63GRfBhvOm+sbI072dQyWfBh7snbIE2LkHNCc2x8BP/8ZhC0oDj1nbHR4adXZ9WtP6vMpc13/RDenZYPVFVT5bTNV0oIir4il2mJs6oMNLnWU9XqTLeUUGEEbm22xwG6tDpZy782AMZgz6NTihOwxNHZ/o5wgm52Pjlg+dQ02nA9BggHf1XbI6OSdQe6b0KDaF2mFOEvhAR2pH+i4Hm+UhL8akw3Q92uZAf4lQCbe1QKAWBIygMdzq2mHmru3znZNcMaTPGXTCjhgZh8p3OLxH5ONomyk1Z4Q4JCOueo+BMxol2xCZAIi8cxl5zlcfL8A9KGglsxWUM/1lmYEBMTOBp/oWGAajE8JzowrkOWO0kv6hb4FxNIKQZ4dLGqNxjv/nMKnpeQBPHK8SUBhF0WNBASMcXVjrWb9X3la93MkUPuAf6nCf56PHCx3Ak4ygje7NWHX5d7jEFYQjfZ97O8zXjjePFSofjOQcTUF02kyQAAUAhdkAcmj0trNj89XHy85G7xzq2n9wADq8aH0Z+2wGRD8Z2BgwkFP9LWgMEptgdOdQBSZkHMWh1nuxOZ5bcuEoOWuBUEeVh12fdu9E5nb9F93wbnWoVT5bTFUK2uEpGuqvWKQ6q8NLXSssnfdDjJJphq6c6lCNTHbEYAYH9Rqc0B2GJsZgFDqCyTmTvubSOdRMAQdxZlffNTPGUORn5Ex4jWB2mJNmGoycKnW4tYxJImlpIYowsEfbXIXbCIpyMwBGqUZPnCvqyp6PXrhkytdNDsTMCIOeEa/7olv1rmkpFTQh5bsfUj5UoCPycbTNUTZlnHOonguaKB/DlAAtdcuVDBqBhDhjzkf7Q3VjFb3TRjNGCT6ks+6UabvM0uT9XEXfgV3MvR0uaYzGOf6fw6Su9sBshhkZdAn3dyZ7vFR+V73cyRSHmoChg6l8zHh+IEuRU7cYcXK0y19fmU2j2/QxztO7q0Pd8XbHm7Vu1RmAsARvZxRsmpWtNTNj9LezY2t+tdwEfNJwqEGOo7vS1bTpP8Ex/oYy5cuhxj6bDoWJG1xtV9OuwUf2Ljl8DoeaZQ+BdX5oJXXLtfJw16dJm2tkbtd/dCOoZoLF8KTK52n9wChPhEEgs07U4SmKbEELmo4yv296TQd08G6pZK7SEQRDdY5U/oS6K6c61F0EIl8GM2tP5sgxjsCYYmJ8TMF0dd5haGKMESCiJAQCBbfVZzzKdMPD09f+7xyq+qE41F19Z7LHS4cJa81n7TOjRaMe6RGlf+/p5AgKIhCmwvFdX4TiUHdtTrpcK5amfuWUOFnrmpx+RpCUdC07edSrvvIuikO1HkbhBRQU0fRcHGrSxqFSHk7OSKji8B6Rj6NtjrKpY+dQGduscXGOgptzsotf2XwkCCK71rVDnKl7jBndFGRoj1GU4MU0suCK8wiRV1NdHUmXvRCgFdVVkGIUIH/GmeGOQz3C/w6TutqDGGQ61eH+mgGjrx1Vfle93MkU4xYdO+pQTaGSF3JjSteMTpe/NbyMssK36lDT9/Vex9uON13bLdXUJSv6RU4F5WYVMoLc2bE1z1pudaiWWYwmBWRsgHQ1bfqPvevwojnUKuM+R4bZCPs+yLmfT+MzYLXf6lCrzTGrQDcsIySIWttcedj16Zre9ypzXf/tsOWrfJ6EqcNU7fAUKfmKRSpqOupQMZPgqgCngLpyqkOto5T5yuOFgUnE4GaHqdrVmVB1mKDqlYhth8mpDbtRRypG+KLc7nkn3+NQd/VNHrl2mLAMQNdnBBxyiajR9LrRtuCFMorGgzO8w3TdtTl1ydVISCCE/5ScYSLADKfPHBzqyp6PXriY2oyTjEOVwBolebGWCpGJQ61prY0YoaIOh/eIfBxtM2VLoFAdau5VjFLGA6930bP64mH9nUa/HYyn+SOHRkUCn2DNWisVLFpbovAMSngtTyMHRrYjfDXLQTYyLdzhklpOkD+6xP8Ok7raAwbZkgCip3ii3kYvyOjFtGBHld9VL3cyVXWMc5AOBfd5fn28kCWje/KlP9OPXf5siGlDtkg5ptyr80zf24WdmZ+OtzvePFZqfqC3wT53iwxkbdxmJU4K7ezYfPx4qeVWh+rHAPQP2+B3jTnTmrb2X4cXzd5U+0zGOHv6il8CJoG+AJD+2t+Cfx0J7BIo1Cnf3Ks2x/vWrLPc1+VXedj1afdOlbmu/zhvMrxiy1f5fMyXkaaslTBU9LoSJl1LiXxEF+ua6K6ca8uQXhu0ZaWuzurSta++mzWWeu85P+/qu5aRqeh6v+szz7s6dztNa171c/d+fe4zI3OJd3nnmrLzTq7WbzioI3Sk3jt+H3n3Uh0iY3jDCSaI6t4T9Yvojb7PkbZn01FNp6zKFzpE0RnMc9S181z/XMP/c+V6pm7hke92zeaHMi69uz4/V+c1bfc9szRGZ7VOSdvlz0Gstivpuzw86/LJO+eu6nVpJizv69Ojuph3cvXu0XLUqdtgl7xy7WSwk7ukP3qtNscRM7M152jl4S19sb5D5zps+XP1eCnPjDozPftSCrhneufAK8ABG2PszDRquUSmrDMyupT20nMjmPxG7aW0r8pz041rEP961c0sTR1dvV7l3st5Ggc4NLvgbdbaBTdPK+H+9p0Ddw68UhxYo9pzlTPqeQ6yM7GOWJ8jz3sedw68ihzoZmxemXqacrKr6blIXpemsbqyTDucg6Wq7xj620BwC5kGuJVsEDk3jXcpXxtsbGqwaQUxgE+Bm5vZvHAxHVEBIPLQhpQ6LeXIkrIzVWRXp7rlCEbee6Ner+nnpLXW0k2tP4UHyTvXmleVY5v8jk69yaPLr+ZdP1+Ttr536TM9h/RkPfFlUurPPlzDo6fU6TnKSr1fhlxdalvKvrUdR+z4rRCf5+qe+t7Ks50fSb7nys6z8G7nG8/6gaxtJrOnXl8GLNVaJ1u07ay7lgIRdu17SW96Lj+Um3vXXCF62KXn/Jg1Lxs0ngI315XtGEuQdPLchiFb2jlNZCOITQI2hAGcsBbrDKNNK85Ovhmo7mI+1x4bKwLzVncbn3vn6LMqb119qhybxjrqzK+BfqztO1rvo+ls/iFrQUk6+t416WpbA/l4zfu3pn1qWbXezy1Xl9pUy667ny+9V58fseM2uJk6f04K3x2ndOb2WqJD3can5Hskv+jqzjee9QN5SURg520Wj41W4qk9M3LxJ3JZYcWkW2GpGAs7VHVuYAFvgaWyZd17zm06BoM4AGghRlw7CD3z63ad2XyAKkSYXbo1qjZirNN0u3wDibXjg3JMR+BjN5XHoTp0juwUzJoyIXDc5Ba4uZnd6WKHJmGqDlV/2nVnXY9DtWbFeKf9dtRB90Hvf4VD1Rf61+5cZaz8lp++Y9DtVtWHCG85eDt0Y4g7ufDuEbjFTj7sGLTb2DPU5TUfnY5G2TnpHYZPYEH2yXOlSxCR5MyxAPKOH1XevJv6VL1gjAJVlz6xmzJGSlrRMBJ5gzgzqtVnDM45vZqvvdC+tc/MUtgUZO1VWYiMd9CObECF3jO7YUey+up7OnkOyjM6Qyf1vXfw2NEfn9Fan7Wt+jGbYswUkb2qZ50szKxPl46fHtipvsIipqwdPwTEeEen2ZM6DbnW+1a5UvY5mEd1X/VuLZtDVUc2iYyHuv7q7HjS56rd8jJDWB1qZ/dWmZEH/QdcAmSGTK/2IHwn32SLrKyzkZ29cI+dseu9c6jJd9efaV+1HTvfKI8c15HvC3bKSy8DlsqRhQoLeCsslS3Jtk47buJwOmOcLeuMGKWG/uPIgPNaBIWDsuMRBJbt2+DaMMqW5yPwcLt8s50bzzjACq+mQwgJIVNXUQ5hqFQdar3/FLi55JMt7G+/OFTHaxxY51Q5VIKv7tmJ57gJPqFrHOoHT8cMJYvRX/ktP07Cxg/8oNgcsHpWOMlOLgQb+hzUG+XgdLo+VUYnHzb7OIZiG3yX16mx81+FKGP4VghEyRw7EqQ4q5nzgTUPSqx9kMSkdVSjyhsnm/pUveCgcswCrzh2x0sAPJiiz5lQZWm/Mhi0wLApo+NfrVttX+0zOuQsn/6nO2wAmcADR0kcnSHDjv7oX/cq9B5jpx/NtjBm+sGIPBCb6uAccaA86YxjKoAJtNMOZ0eg6KfjGwLatT4Oz9e25rgSvVrhNJXXyULlRcdPAWYHi5iyOn7gk/qzB/pbuxIcKm/to1vlKmXvYB47nVjLJjPpk8CecgZdf2nPCi9Y+afd9Fm6CvHZ2b1OZjh7MJpm5bTNLONqD8J3DlUd6YPZtAT9nbyzK0lr1q9zqMk3PK3yXdtYbcfON8YPtLbFSx104FNhqRiOnBOyTqfBzhJdC0tFIDLN6oCuUVh1qB2E3g6aj/AbjRKMc/BwHGqXbxiJZzkfFXg1nQIGzy8iaCPl/ZDaUzNSzAg1j4xe8PpWuDn5iCwpDTQao4SMUJ2lIrTWauNQpXf+0UF7v5yiX7QLXetQc85yx299l5GeoIYQUyAOXWS9kwuAB0fhFjv50BZT3KjLaz46XTjDwLxRtqAQQWPhQI9ARDIeAj2jCbzAUxR58zn1qXoROfacQw3YtikljqdzANI6h8i57fgnTai2j0NNnzkzTOaM9PQRfdA3eJCdx4F27KD35O8durSDwMOHnNmmM861IiMJThWZDYD5u6tP2iptjCInHH0JnKbnO1k4FbThpwCcPFoG0f7AIqasjh9GaAmsjJj/++JQlVfrfatcee8czONO72rZeJI2Of8L9rTrL+uFHbxgeOfKnrAzyCwM6EazKZ3d62TGj0gkgCS7HJL9HrEH8g3fOVRAIyjne3fyfgTeMPl2/TmLebxEV3e+MX6gtS1e6qADGfmAHHBk0tW0IiERD2Oi00KBpWI4AgtoqkUlb4Glkndg+0Rq0GBiiDi+dJDyg/izg+arBk5dGCyOMUqfNuzyDSM7PnhXJGUNM+1M+5PvOkLl3I1GTIOFRHGhI3Bz0ppy0F9GzMC+RdyE1WhLm91n/Dh4fcoImJ7kPN5jjqzkc61DzRrHjt/6LlMjjk05yM9oGcWjnVxcA7fYyYe8oxRdXg+lP/yvDoeymfJF+gQ60RGISBuK8JpjNo2vT1GVt9Sn6kXkWFoO1YgDkS99Rj4DXFKhzmIwd/yb2ZwutX0cavqMI6UvRpD03wF/wUPlQZCoZMQJCs4CvedeHOoOAo9DrW36oFkx6GUBtcA7o9BdfdJWr8YoGsniHcIDBhntZGE+3vKzg0VMWR0/9AtgmJDRah2hul/rXfO4Rq68ZyYAdahUO72rZVeeBFSm6y/T19KGYsfz3XUH8bmze6vMmKnJfoXkW+2Be+E7hxqwF/aYfu3k/Qi8YfKtfVHlO/Vxja5WO59BmOfxA61t8VKHdKQBT4GlYjiyRqgShO4WWCqdTNlQ51CzgOx5HKoIsoPmYxBExIgzM02pXivpwC7fMLLyLIGFPLTXqA+JnIM4M2+d1jIyQjUPzzCEJ9Jwpu4JUkRjl+DmRInWnQiaaWN/ojrTMSI/EWHuAxrQJ8owI8G5m+4yxWPmAMWhGj0y4OeIccZntOO3vuOQjJCNmk3tUaDK804uroFb7ORDnUAVWqPp8nqo9cN/I8uMNihblDiGj+zpEwqFX9JkLT/5cH6MGMKL4NdWeUt9ql6sDlWQg1fQnxi4HdRZhWHr+DercrrU9tU+kzeHj6xvmr4lM5UHMTjWiVboPe/Foe4g8DhUI09UdaZzqLv61LbGKO6g5HayMKvQ8lP9OljElNXxg+547t2sua0Otda75nGNXNX3Ooe607taduVJHGrXX6bcO3jB8M5VcKbdRvWm+e0LMELt7F4nM4J5gaP3lWcZwWCm2oPwnUPVz+ygpZsElp28071L8IbJt/I08l3b6HN0tcps51Bb21KhpupLT4WlYjjCBJUkSEaD18JSmQ+vDtVcegyRzqyOT7TNKOiwDpovEGHm3BmuHTzcLt841MozDjXwapwFR26dwZ/It1IdoXIwT4WbM+Vjp26lOuVb7/tVlezy5eSM7N3jRENxqNJZFzpHjDOjj3b8psyi10DnSYdHeBPq5EL/WAvRn943ZbUro5MPeYNe9G6XV8p2NYIKzFuFNYzhk6aDtKx5mDIlSxyz/jBicnypylvqU/UiciwvhoYjlQ85s1ljB3VWYdg6/tW61fbVPqNTZgrUyy+t6BNrk5UHvquTIMw0qzT4bfYD2egmqGM3LkF5Vp2pDtVxhoxQu/rUtsYo2uwiGKhwmuqzk4WH2u75yU7he4VFTFkdP+SHB4y7WR8zEkb3lWq9ax7XyFV9r4N53OlELbvyJA51119Gw4JH07rWOvXZSgY18oQHzSGylZ3d28kMZ6+f2W2OcrUH4TvHZ4nAoMesW452dvJOVy7BGybfytPI99rG6GqV2eob4wcu2ZY135OyiPKPkHW87MA7l17FVqKUL4MInFFFJcxHOvwSPNxMevWla6NMqkM9lylnX3cNJq26exYizNlQlHtHr/oLDyrFoeKbjQPX0spvDtV0vaDjEnU8y3RhfXctoz6rn/FJ2lCXV57hxSU5J0c1v7ybq+e7PpNmrU/eW69rGd7roM70neg91PEvz861r8s7763Xc9B70tLjWqf1/SPfu/qsbU0+dWd+7l267viJf7ENl/KgxwYcSH0FFqb/VtrVu6a7JFc17e5zpxNHyu7664gd39nrTgY7maFrl/QtbV31Ife7snb1yjtHr0d1Nfmdsy1J86a+XgMP95yMME1iak0E9BzEoXYG6Ja8/TKNXdLWqAlmRrO35JV3LBu81QtbmHG/vmk4wOCaJraxSlCejV5vmgbeG3LnwLUcuCW6vbaMe/o7B+4cePNywIh2nQF787b23rI7B+4cuHPgzoE7B+4cuHPgjcSBrMetiBavdxtEjHY1omuwG3eYlNp1lJJ2h/94JB87Bq3PZC2u46t7tZ1H8u3SXKqnjR/ddG3aadrcovyln1CqZedd9+rnpLG72WaZS3VL+ly7vPLs6FWZtvXnOMbR986lu7Yd5/LyLPy5lO5lP7f+Rb92ZPer3fTrnyWPa/Ryl3+9n75/Cq+r3tn9nzxrOT5bz6u78tfn6/fk5cwsXbFL9lZKXre+7z0bNu0kv2V2Ljx57v7r2vOqy/lLrV/wSh3dcEzgLUnW/ewiRtdgN9p1Zj2yEuMKpOEoZbdx3Wp99F3pHG9xzssZMQvuHV9zr7bzmjJqWjt036veWD7X7eX1Udrp8L+DzXZHHiG7+XIGbcfbYGTaQbme4T1XRup0Ls2lZy8Dl/ZWWdjVNfzZPX+97gu07PTckWDLpjZ/nzB3a/psd289xrF7/5r76ftbeb3qXQW0WevxtuUM9fqs+568nG541wK40qW9dC95XUq3e67P7Ny3o/lah1r19bn7r6vvqy7nF+v3GjzCBiNSw+3gcxTE4XA/F1XxSu3mCi6ptIw+BcpurmB5rjjAlaHS2vLNoIJeQ0ZjokLlOneVHaPOcTLSzlQ5L4uqo9Gm7Do24vNuxf2Uj3srJqV87JCzNdpZqOClGjmt2KEPpX7SWTRlRrGN8ORfd56J0CtP8r6r9xxrsBXd+dGPnmdaw9fKa2feEjh491y+tQxIPPgKeUR/iFgRXnCwRsfBJeVQV1zbnLnTTkdJjEAcAF+p60ebnWzikH/lba1TMDLjUBlmZ8WC9ardOULlSIn0tU7qsWKbrnXzfZXNFZe2vtPl17XPO3Y0O3pAHm1cOScLKaOTY89WXFv3wh/BqxEPQxf59DwzF44D5FxkJ/vSGTU5Kx39pNv0F9BCaMX+JWfn8FLzXr0Cgggwi/sMMjmjG/qukv6u5Wtvhyucd2rfn+N1x4PkUfWOXFTHxe4AJKEv+jMOFf/xvm4I7OQkeZF3OgONqRLZj30w+s0ZeefgKy60d5JXpwPJ85wdEKzbQJWZhdWedbIj39UW7vqva3/q5ar97DX5Uc/Qc8l51/aXJefRQ23QLmeoY0tPzksEdQQ3FVDBR05sVud6OCMjFPi4lC+oRTy4dEYZOtE0GsF1Ps8heFBjgSUMY12NcIEZvM88G0i5CJeDts4iOejr7JcRHINPyBz1cB5N/tWh5txRhzPpfWfqCHrFpExdOJiKIcpAddihSc/oB6tVPTps5A6DMu+7MmyUSvSow1a+Mh65Z1o7DvVSvrUM/Aq2KkMBIQcv8BQvnO0LLin+2qUb/jozVtsZQ5ypoFpO14/BlOVwKm9rndSBE+dQ1cOZMf0TVB1ndzPFLKhTx1onUXiHKVzr1smmOgWXNkGYd3b5de0jp4IhMm8Xt/N2O1mo9enkmAw4D+jMX8XZDX+cZa042cmPwdIOPFN+J/sM5Iqn6v2PmGdOoc6wBxyIAM/5WEAgoOQANRjh7PBSU4967Rzqii0rPdnTv2ZoKtAGnj4Fd7XjQa1f1TtBRRwX/tEVszhGwRDFOFRwe2Ax9YVzzhziTk6SlzKiL7Vs7SUjiAN15te0MKdVcaE9T16dDnh+yQ7goXzpVmfPVtk5VarBJZbH2n+79icPV3LE5lZc7+eS867tL1POo4d4ufrOFuu0w4jk7QlToOSMtEQFFMH8fg6pG7lyKIneYLhqMMFROKr4t/PWafRrtGlExlE7SM5wMFTKTSTnEDqjyhA5OI8oDQfTOdQOZ3KHSTmzO10qfNcOO7Smr3BVKzYyRWWMjL6NXoBOrNFqzcvnla/1Xtp5bb7qEJjFOFQjhRgwziS4pPi74tqqQ9q51jff1anrxwqBV3lb61QFNXUyulCmUc7OmKROndyKnEM72fQ8qD9J69rlJ4Do2scABsKTjpjNIPOrLNT8fe7k2Bpkh7Mb/tCLLiBlFCsuaif7HZ6qWaOPKhWjY4JZDjVHQo7gpZYsHj92DnXFlt3hJeONAAjhufaslL7f8brjwZpH/R7HZfTI9hh56Fe84FAF92whYs8EgJ2ckLvkNZO/5sIeWJpC9B1ogfU5AfOKC528Oh04YgcE5AY5qLNnq+zMpKdL1VcOde2/XftrHt5bcb2fQ853bX+Zch49bLF8OzzCDiOSI2B0VloNP0SLur4i8oImQuBjcIwAA8+W/DDGFKuoz5WC6yhCbQQa0phgSpp2QZy5M5RxNO5lhCq/FV837z+8/ZD/uoZahchoIZtVTPcEOzTvu1bFTjuN/vDMO54H49fVFOg5Wvkqbe6lndfmy3mJJlEc6g6XlDFbcW29l3Y+5PLa/7t+POdQU6cIqsgvcJhKoIym/hiToDMxsOqIUqdObuuU6E425dE51C4/U+WdnBpJwl6uVGU+slCf+6wNqxybjutwdsMfDrVbY2YU7QcIdbKvrKxlJx3AdLMBoQR8nEiwf4/gpeb9eu0caqaAg9yzw0uuMmgmwghupfT9jtcdD9Y86vc4Ln1J7vDKrBnHzKFmzdY76me6r5MTcpe8av71syM37JSpZTaTExbImBlacaGTV6cDR+xAdaidPVtlp9az2kI8Wftv1/6ah/cyGGN7yNNzyPmu7S9TzqOHne9ssU6NWjosXOt6WdsEPWXO3/SXa0aomGhKSAcyrqZw4MsS+BjJzqEymH6KLdMjDAqnw6HCjDQaNiVqCsgIljBTfASO8MM3DrXDmWTcCbL85BtMypnd6WLKJ792sMMOrekv4T92GJT1/fVzx9fci0P1zjX5cqiiQhSHaj0LL9zXZ3hhnQZ/V1xb76WdD7m89v+uHyumbOVtrVMElUPV/6aE1MlUEbJc8G7zs35VR5Q67eR2JjtdOtn0oHOoXX5438mpGRuH/U2VMoqmXavMn3OoqxzvcG3DHw5V+1diFMlDqJP9Dk/VqIhemQUy+rJMY4TGoeIBYlQ7vFTp8yMaM+kLl86hZh08DtV3Rh6POBn9agmpyuDOoabvd7zuePBCBZcvcVycGjuB5MGZZ8qXrdPuAJd0cuJ58prZtBezbPC9328+3eFCJ6+dDlyyA9WhdvZslZ1a2aqvHOPaf7v21zy8t+J6P5ecd21/bjmvbYketli+HR4hR0PJRbvWpPJbdKYkrCu4L1MUvNL8xI57plQZRIppvYEDrbiIvgf/9iGXh/9wUL3nT4Rm3Y1DtZ5G4Rn+TAFRNgKtDGuLhN3UUdYWpRVJEZQOX7fDpKx1qXiYO+zQmv4S/mOHQVnfXz93fM09xjvtvCZfzivGLw5VuRwYoay4pBXzUhmidJR2zq/tpetHo9Bg5lbe1jpFUNXHOh854EzzU102AMjDKIHDikNNnXZyWyvZyabnwaWtaXf5de2zZsPYqZs62+BTZf6cQ13lmMGiY9pVcXbDHw614mSnzmQ9wYd7O9lnIOWvrtbGkTVC3xk+cJZZQ82MAAfR4aVyZN3088z2tNYbA+yekVi+x6G63+ElVxm8FXd1x4PUb73C5bXsJIAwM2a9UKDE7rEx7JAflPAs8IM7OUleaxn1O/tm/4VADe1woZPXTgcu2QEONUtunT1bZWdW53Sp+tr13679NQ9yZbaAvTYY885zyfmu7c8p57Ut0cPOdz6my7Tb443Z6A4JJOuZSUvZOjLCuJY4WyPbEIGj6NauajmJXqU/QgzaSms71ueMZK3LufYwQITkEnX12L1T25s03T3Prsk3ebkaocYwnMMlzTtH27n2o/f1YTZnrLxN/vVqTbfyP3msbV3rpB86ua15n+vLms7nLr+ufdLu+mfNM9/PyXH2ISTtrdeVX/LJTFDNU//ol3N0SWfOvXvpmT47okM1n7Xv67P6ueNBfd59mGYtGQAABx1JREFUVpcOo1eZHZ86OenyvXQPH+wd2JF+2rVnd7/L6xodOKKv59rPoRrgdPb6jS7nne/s+P1K3LO2WaPuVMqvW2R9J/fu14co2khi/cuU0sojxuGOS7py5fX7fpfj14/X95LechzI9Phbrgb3ku8ceB05YGR1aVT3OlbnXtSdA3cO3Dnw5uGAaYXnItNLOVBc87wGxir12UHi1Xx3n22eersnwp89BeKsq5fpAm26hewOzNRT+HOEp7fwIfnfUk/OOpCQt7x/5B1rUNpld6WpLDzNX6aZTB8DD3AkoZKpKhuJHDJHycuO3jvdOXDnwJ0DT+KABep1a/1TMuQ06lGa5HUUHrDWJ2tOyePo1S5BGwre/UpYwjX/uptwfXbLd9PW9cjCNXnYWGJK3BbxwCRaq8imjy6vW/lgHftW4qCyierWPC695ygSwBCIWo4C2ZRmU48/G+YEdTaWOHrkiJUdhsiOTTsonRd130YDx4WkfeeZ5n65c+DOgTsHDnNghU8KTJydYYiBrrB7GXFUiLMO+sm9c1BlyrXx5BLEWK0Ph8rgGY3koPCs5mn0YaSxkhGKc4J2ZzLuynXIXx4hIxtb9dElqDLHf4x+bBfPJg15ZRTnmdENPhlFOmr09gtMnHO9zlI6a8uhHuWptig3kImOPNh1FphEDnUH7dbxAf9XiDn1f8cJgYgf5MBxGnxbyc5qfazM0Cov1aF2fMIrZ0Tt1iNTzuPpWzirAWZQNnmy9T79lPJcOVT1QI4FvMP8nIvvzvAh9eFwkfc4YOSKlyg/tj6/3i93Dtw5cOfAZQ44orDCJwUmzmjmCExVB/0k0nck4hxUWY62cJJGjzuIsVofaS1wV0g8rTTyYDArdFlab5r2QydCCEOtXGhN4LDsIkaOCXHcl6DKjFA7aMGcE5OXQ/fSyduZRUeHOHQjINOLjqw46uN8m+MaHKpReIWN63hqB58NWoy+YyOAxzmXCuXHoa7QYA8tfPgVl5UPdt45bxiIOQGJvgBFCY5OPchBoBWTl6sjFdLa8ORqWreTl+pQOz7hlaNRADHg/TrCgycQXbIDmSyRU0ctnBldp5CrQ5UOmIejNxCpjE4hdgEoDwHnEDg5lhLcW7w0MkV3hzoZcb/cOXDnwHEOdPBJFdXmEkyVdSnGzlmlCqt3BKqsOtScL+0gxmp9ONQVEm8HXVa54LwdY4xS7vtOrE7rkDH0l6DKGP8OTi7vy7861I8vW8WBCqjrx0x+SetsaRxqYON2PN1BJq5IJis02KnR81/HBw41EHMwW+EWm0Y2agyCVBBpal7O4wVNCaRhICNzblRwIpg44lA5emSUyakio1nTuEigYOoWGYEa2VeqDhU/jXbBxoFbU0cBk+AxJD8j45xhdl8A5Du6O9TJiPvlzoE7B45zoINPqg7sEkzVDvrpCFRZHFumcdXaaGiFGKv1qWkZTYADO+iyyoXOkXAYnByDq77oElQZh7pCC3qPQw1oAoOfEWp+MEAaQAWMvJFqRsYcVxyqNVG04+kOMnF1qCs02Mz2dOn4UCHmJOLUoDJBuTFCRZ1DBfphur5SJy+rQ135hFcfNDMBWhEwfKNlo2/EAaZdQDnMJlSqDjUbtTwHIUdm/OBCAjH3jYhN0RvFJgBxDbbq3aFW7t4/3zlw58AhDnTwSRUm7ghMVQf9tIMqq5WqDjUwd51DrfVhHJM2DtWaXQddVsvqHInnNq2A/gIQji5BlTH+gVA0BWv6EZmGNkJnpDnOONS6mScO1fR0HIKRUxwqPoY6ngos8My0cYVMvAQNljxdOz5UiDlrlUbuyAhRwIEC8Ta/ni4fMMYJutIXjve95+hRGciaJJD76lA7PlWennOo2Wx1zqFypmTB2rKRvoALspAfVHA215qxtVkwmsgyg1kCZFpYm9DdoU5G3C93Dtw5cJwDHXySXbmBibNBhQMAtWU6zAaVFaaKY7NuZSenKWLQXTbZdFBltWZxqBxKnKQ1uHWEWutT08ahyrODLqtlMfKZVk65njPm1g9zBlPbOqjC5LWDk7PWB04Rn6w9ytd0aXWo1ueMUE37guCyHqqsONQKYNHxVB06yMRL0GCpu2vHBw41EHMckj5UFzuzM00q8DB6r4RXMJQDk5cjQKu8aG92+XZ8qjytDtURoIxQK+QZHgQOM/WpI1QjbHwPzJm1Uv3LeQoQTGkLTpC1dDvaOXrTw2QX3R3qZMT9cufAnQPXc4AxrGSkZTNH6AhMVQd7lV2wyefW61qfLh9G0+jtOahry6V8tbXy7Ej6S2m6enQ85QiNyJ6LOJbssJXnOYi3rj7n5OVaPh1pU3WoqW8H3ybQ05aV1jbcHerKofv3OwfuHLhz4M6BtwoOmDo3irUm/VQydW2dO9PyT83v/v6dA3cO3Dlw58CdA3cO3Dlw58CdA3cOHOHAJwLdVKNydIOE+wAAAABJRU5ErkJggg==)\n",
        "\n",
        "https://tfhub.dev/google/Wiki-words-250-with-normalization/2\n",
        "\n",
        "**Preprocessed Data Set Outputs:**\n",
        "\n",
        "* all_gsDataSetW2V_501\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6DKRmg1kM3O"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ48fjWwjtUp"
      },
      "source": [
        "# 0.1 - Install whoosh\n",
        "!pip install whoosh\n",
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools \n",
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c1_LqwFkhzQ"
      },
      "source": [
        "#2020-08-17 Reading all_gs.jason using pd.read_json from My drive Google Drive\n",
        "\n",
        "url = \"/content/drive/My Drive/Python/Corpus/all_gs.json\"\n",
        "\n",
        "all_gsDataSet = pd.read_json(url,lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ5Eutc8R-Z6"
      },
      "source": [
        "print (\"all_gsDataSet.shape\")\n",
        "print (all_gsDataSet.shape)\n",
        "print (\"all_gsDataSet.dtypes\")\n",
        "print (all_gsDataSet.dtypes)\n",
        "print (\"all_gsDataSet.isnull().sum()\")\n",
        "print (all_gsDataSet.isnull().sum())\n",
        "all_gsDataSet = all_gsDataSet.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOWqclzQk35R"
      },
      "source": [
        "all_gsDataSet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ2eUWKo1qBM"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the first argument (string)\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.1: 2020-04-10\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/len(x))))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "\n",
        "# Calculates the Wiki-words-250-with-normalization matrix for two (2) pair attributes of a data set, and adds the label and stores them in a matrix [n,3].\n",
        "# Returns the matrix with the calucalted Word2Vec result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Version 1.0: 2020-08-09\n",
        "\n",
        "    \n",
        "\n",
        "def W2V_1 (dset,\n",
        "          left1,right1,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #rows=1\n",
        "    matrix1 = np.array(np.zeros(rows*3).reshape(rows,3),dtype=object)\n",
        "    embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\")\n",
        "    for i in range(rows):\n",
        "        for j in range(3):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = embed([left1[i]])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = embed([right1[i]])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBgn2wVTssH5"
      },
      "source": [
        "all_gsDataSet.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l94zcQG-sxiV"
      },
      "source": [
        "all_gsDataSet.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXHO2UA0s3Ow"
      },
      "source": [
        "all_gsDataSet.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSwjuOZZrnFE"
      },
      "source": [
        "# Initialize prepocess_dataset Word2Vec Matrix with \n",
        "\n",
        "all_gsDataSetW2V = W2V_1(all_gsDataSet,\n",
        "                     all_gsDataSet.title_left,all_gsDataSet.title_right,\n",
        "                     all_gsDataSet.label)\n",
        "print (\"all_gsDataSetW2V\")\n",
        "print (all_gsDataSetW2V)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWUukP963oph"
      },
      "source": [
        "all_gsDataSetW2V.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwrxLqoU8k_R"
      },
      "source": [
        "all_gsDataSetW2V[0,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLnlc_DM81Ur"
      },
      "source": [
        "all_gsDataSet.label.loc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njxAbHPgKTrm"
      },
      "source": [
        "all_gsDataSetW2V[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "736CCdGkO-TY"
      },
      "source": [
        "#TestDataSet10W2V save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/all_gsDataSetW2V', all_gsDataSetW2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm26805ccV0L"
      },
      "source": [
        "#TrainDataSetW2V load numpy array npy in binary format\n",
        "all_gsDataSetW2V = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/all_gsDataSetW2V.npy',mmap_mode=None,allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAgoGuhyccya"
      },
      "source": [
        "print (all_gsDataSetW2V.shape)\n",
        "print (all_gsDataSetW2V.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRCruoCjcdZE"
      },
      "source": [
        "def w2vMatrix501 (npyArray):\n",
        "  rows = len(npyArray[:])\n",
        "  columns = len(npyArray[:][0])\n",
        "  newMatrix = np.array(np.zeros(rows*501).reshape(rows,501))\n",
        "  newMatrix.shape\n",
        "  print (\"rows=%s\" %(rows))\n",
        "  print (\"columns=%s\" %(columns))\n",
        "  for i in range(rows):\n",
        "    #print(\"i=%s\" %(i))\n",
        "    for j in range(columns):\n",
        "      #print(\"j=%s\" %(j))\n",
        "      if j == 2:\n",
        "        newMatrix[i,500] = npyArray[i,j]\n",
        "        #print (\"newMatrix[i,j]=%s\" %(newMatrix[i,j]))\n",
        "        #print (\"npyArray[i,j]=%s\" %(npyArray[i,j]))\n",
        "      else:\n",
        "        for k in range(250):\n",
        "          #print(\"k=%s\" %(k))\n",
        "          if j == 1:\n",
        "            k250=k+250\n",
        "            #print(\"k250=%s\" %(k250))\n",
        "            newMatrix[i,k250] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k250]=%s\" %(newMatrix[0,k250]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "          else:\n",
        "            newMatrix[i,k] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k]=%s\" %(newMatrix[0,k]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "  #print(newMatrix)\n",
        "  return (newMatrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU_M8bUPcgoa"
      },
      "source": [
        "all_gsDataSetW2V_501 = w2vMatrix501(all_gsDataSetW2V)\n",
        "all_gsDataSetW2V_501.shape\n",
        "all_gsDataSetW2V_501.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-63eJ4fcjNj"
      },
      "source": [
        "print (all_gsDataSetW2V_501.shape)\n",
        "print (all_gsDataSetW2V_501.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNuZZurScl7H"
      },
      "source": [
        "all_gsDataSetW2V[4399,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmvJ6Pq-cnq2"
      },
      "source": [
        "all_gsDataSetW2V[4399,1][0,249]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfXxMBxpcprv"
      },
      "source": [
        "all_gsDataSetW2V_501[4399,500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYaIshTbcsI3"
      },
      "source": [
        "#TrainDataSetW2V_501 save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/all_gsDataSetW2V_501', all_gsDataSetW2V_501)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV6a17HnHjrf"
      },
      "source": [
        "## Preprocessing Data Corpus: Preprocessing of TrainDataSet.pkl to Create 90% Hyperparameter Training Data Set with Word2Vec v2\n",
        "2020-11-07 JXHALLO: Preprocessing of TrainDataSet90.pkl with Word2Vec\n",
        "v2: Applied transformation to create matrix (n,501) for word2vec transformation where:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "https://tfhub.dev/google/Wiki-words-250-with-normalization/2\n",
        "\n",
        "**Preprocessed Data Set Outputs:**\n",
        "\n",
        "* TrainDataSet90W2V_501"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGxYsO4FI0i7"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwmd_IFWI0i7"
      },
      "source": [
        "# 0.1 - Install whoosh\n",
        "!pip install whoosh\n",
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools \n",
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iWVOMiHI0i8"
      },
      "source": [
        "#2020-08-09 Reading TestDataSet10 using pickle from My drive Google Drive\n",
        "ValidationDataSet10=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGVEoAeZI0i9"
      },
      "source": [
        "ValidationDataSet10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKrb7J4n1bJO"
      },
      "source": [
        "#2020-08-07 Reading TrainDataSet using pickle from My drive Google Drive\n",
        "TrainDataSet=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2cTuxj01qBV"
      },
      "source": [
        "print (\"TrainDataSet.shape\")\n",
        "print (TrainDataSet.shape)\n",
        "print (\"TrainDataSet.dtypes\")\n",
        "print (TrainDataSet.dtypes)\n",
        "print (\"TrainDataSet.isnull().sum()\")\n",
        "print (TrainDataSet.isnull().sum())\n",
        "TrainDataSet = TrainDataSet.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2JQsmnctYUf"
      },
      "source": [
        "print (\"ValidationDataSet10.shape\")\n",
        "print (ValidationDataSet10.shape)\n",
        "print (\"ValidationDataSet10.dtypes\")\n",
        "print (ValidationDataSet10.dtypes)\n",
        "print (\"ValidationDataSet10.isnull().sum()\")\n",
        "print (ValidationDataSet10.isnull().sum())\n",
        "ValidationDataSet10 = ValidationDataSet10.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGstrsskyOyb"
      },
      "source": [
        "#2020-11-07 JXHALLO: Removing from TrainDataSet the ValidationDataSet10 records.\n",
        "#TrainDataSet90 = pd.merge (TrainDataSet, ValidationDataSet10, how='outer', indicator=True)\n",
        "cond = TrainDataSet['pair_id'].isin(ValidationDataSet10['pair_id'])\n",
        "TrainDataSet.drop(TrainDataSet[cond].index, inplace = True)\n",
        "TrainDataSet90 = TrainDataSet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VUV62UzySxj"
      },
      "source": [
        "print (\"TrainDataSet90.shape\")\n",
        "print (TrainDataSet90.shape)\n",
        "print (\"TrainDataSet90.dtypes\")\n",
        "print (TrainDataSet90.dtypes)\n",
        "print (\"TrainDataSet90.isnull().sum()\")\n",
        "print (TrainDataSet90.isnull().sum())\n",
        "TrainDataSet90 = TrainDataSet90.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x55gXkHxyVCX"
      },
      "source": [
        "#2020-11-07 Saving TrainDataSet90 using pickle from My drive Google Drive\n",
        "TrainDataSet90.to_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41s9O5jyI0i-"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "\n",
        "# Calculates the Wiki-words-250-with-normalization matrix for two (2) pair attributes of a data set, and adds the label and stores them in a matrix [n,3].\n",
        "# Returns the matrix with the calucalted Word2Vec result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Version 1.0: 2020-08-09\n",
        "\n",
        "    \n",
        "\n",
        "def W2V_1 (dset,\n",
        "          left1,right1,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #rows=1\n",
        "    matrix1 = np.array(np.zeros(rows*3).reshape(rows,3),dtype=object)\n",
        "    embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\")\n",
        "    for i in range(rows):\n",
        "        for j in range(3):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = embed([left1[i]])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = embed([right1[i]])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGLJ_7VdI0i-"
      },
      "source": [
        "TrainDataSet90.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qsnvkMnI0jA"
      },
      "source": [
        "TrainDataSet90.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKp0DtI4I0jA"
      },
      "source": [
        "TrainDataSet90.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvRIgbJqI0jA"
      },
      "source": [
        "# Initialize prepocess_dataset Word2Vec Matrix with \n",
        "\n",
        "TrainDataSet90W2V = W2V_1(TrainDataSet90,\n",
        "                     TrainDataSet90.title_left,TrainDataSet90.title_right,\n",
        "                     TrainDataSet90.label)\n",
        "print (\"TrainDataSet90\")\n",
        "print (TrainDataSet90)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt2zDKndI0jA"
      },
      "source": [
        "TrainDataSet90W2V.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiPcyqkPI0jB"
      },
      "source": [
        "TrainDataSet90W2V[0,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_XjC2slI0jB"
      },
      "source": [
        "TrainDataSet90W2V.label.loc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Ul91f2I0jB"
      },
      "source": [
        "TrainDataSet90W2V[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnubEm7RI0jB"
      },
      "source": [
        "#TestDataSet10W2V save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V', TrainDataSet90W2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8kgrJhHR7e4"
      },
      "source": [
        "#TestDataSet10W2V load numpy array npy in binary format\n",
        "TrainDataSet90W2V = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V.npy',mmap_mode=None,allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azuRyAdOTFRG"
      },
      "source": [
        "a = TrainDataSet90W2V[0]\n",
        "-5.41110002e-02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jib3pfUZc4Al"
      },
      "source": [
        "row_count = len(TrainDataSet90W2V[:])\n",
        "col_count = len(TrainDataSet90W2V[:][0])\n",
        "print (\"Row_Count:%d   Col_Count:%d \" %(row_count,col_count))\n",
        "\n",
        "row_count = len(a[:])\n",
        "col_count = len(a[:][0])\n",
        "print (\"Row_Count:%d   Col_Count:%d \" %(row_count,col_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu4_zcMzbumJ"
      },
      "source": [
        "a = a.reshape(1,3)\n",
        "row_count = len(a[:])\n",
        "col_count = len(a[:][0])\n",
        "print (\"Row_Count:%d   Col_Count:%d \" %(row_count,col_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljs1owHmZxBy"
      },
      "source": [
        "a[0,0][0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJuK7fegbFiq"
      },
      "source": [
        "b [0,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swzjLRmCbXv2"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAHQEdPCY0F4"
      },
      "source": [
        "b = np.array(np.zeros(1*501).reshape(1,501))\n",
        "b[0,0] = TrainDataSet90W2V[0][0][0][0]\n",
        "b[0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWC7ozDY7T1p"
      },
      "source": [
        "b = np.array(np.zeros(1*501).reshape(1,501))\n",
        "b.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDelh8PbmwMp"
      },
      "source": [
        "rows = len(TrainDataSet90W2V[:])\n",
        "columns = len(TrainDataSet90W2V[:][0])\n",
        "b = np.array(np.zeros(rows*501).reshape(rows,501))\n",
        "b.shape\n",
        "print (\"rows=%s\" %(rows))\n",
        "print (\"columns=%s\" %(columns))\n",
        "for i in range(rows):\n",
        "  print(\"i=%s\" %(i))\n",
        "  for j in range(columns):\n",
        "    print(\"j=%s\" %(j))\n",
        "    if j == 2:\n",
        "      b[i,j] = ValidationDataSet10W2V[i,j]\n",
        "      print (\"b[i,j]=%s\" %(b[i,j]))\n",
        "      print (\"ValidationDataSet10W2V[i,j]=%s\" %(ValidationDataSet10W2V[i,j]))\n",
        "    else:\n",
        "      for k in range(250):\n",
        "        print(\"k=%s\" %(k))\n",
        "        b[i,k] = ValidationDataSet10W2V[i,j][0,k]\n",
        "        print (\"b[i,k]=%s\" %(b[0,k]))\n",
        "        print (\"ValidationDataSet10W2V[i,j][0,k]=%s\" %(ValidationDataSet10W2V[i,j][0,k]))\n",
        "print(b)\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItBzrOBVo90N"
      },
      "source": [
        "def w2vMatrix501 (npyArray):\n",
        "  rows = len(npyArray[:])\n",
        "  columns = len(npyArray[:][0])\n",
        "  newMatrix = np.array(np.zeros(rows*501).reshape(rows,501))\n",
        "  newMatrix.shape\n",
        "  print (\"rows=%s\" %(rows))\n",
        "  print (\"columns=%s\" %(columns))\n",
        "  for i in range(rows):\n",
        "    #print(\"i=%s\" %(i))\n",
        "    for j in range(columns):\n",
        "      #print(\"j=%s\" %(j))\n",
        "      if j == 2:\n",
        "        newMatrix[i,500] = npyArray[i,j]\n",
        "        #print (\"newMatrix[i,j]=%s\" %(newMatrix[i,j]))\n",
        "        #print (\"npyArray[i,j]=%s\" %(npyArray[i,j]))\n",
        "      else:\n",
        "        for k in range(250):\n",
        "          #print(\"k=%s\" %(k))\n",
        "          if j == 1:\n",
        "            k250=k+250\n",
        "            #print(\"k250=%s\" %(k250))\n",
        "            newMatrix[i,k250] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k250]=%s\" %(newMatrix[0,k250]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "          else:\n",
        "            newMatrix[i,k] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k]=%s\" %(newMatrix[0,k]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "  #print(newMatrix)\n",
        "  return (newMatrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-RdrlQGC1nM"
      },
      "source": [
        "TrainDataSet90W2V_501 = w2vMatrix501(TrainDataSet90W2V)\n",
        "TrainDataSet90W2V_501.shape\n",
        "TrainDataSet90W2V_501.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-uSqaKVD014"
      },
      "source": [
        "print (TrainDataSet90W2V_501.shape)\n",
        "print (TrainDataSet90W2V_501.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEzgtvASPO9j"
      },
      "source": [
        "TrainDataSet90W2V[3544,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQxHEpSd7vFg"
      },
      "source": [
        "TrainDataSet90W2V[3544,1][0,249]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqfKknSNENAO"
      },
      "source": [
        "TrainDataSet90W2V_501[3544,500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az_AvlhlRju5"
      },
      "source": [
        "#TestDataSet10W2V_501 save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V_501', TrainDataSet90W2V_501)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l4eFt_xJXfc"
      },
      "source": [
        "##Preprocessing of ValidationDataSet10.pkl with Word2Vec v2 to Create 10% Hyperparameter Validation Data Set with Word2Vec v2\n",
        "\n",
        "2020-10-23 JXHALLO: Applied transformation to create matrix (n,501) for word2vec transformation where:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "https://tfhub.dev/google/Wiki-words-250-with-normalization/2\n",
        "\n",
        "**Preprocessed Data Set Outputs:**\n",
        "\n",
        "* ValidationDataSet10W2V_501"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_NsPJY-KM3l"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnPhk23EKM3m"
      },
      "source": [
        "# 0.1 - Install whoosh\n",
        "!pip install whoosh\n",
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw5Kt9yOkBLD"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrU-i5YhkCNW"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw7sbop8KM3m"
      },
      "source": [
        "#2020-08-09 Reading TestDataSet10 using pickle from My drive Google Drive\n",
        "ValidationDataSet10=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTJQrUIRKM3n"
      },
      "source": [
        "ValidationDataSet10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBj_g1wSKM3n"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "\n",
        "# Calculates the Wiki-words-250-with-normalization matrix for two (2) pair attributes of a data set, and adds the label and stores them in a matrix [n,3].\n",
        "# Returns the matrix with the calucalted Word2Vec result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Version 1.0: 2020-08-09\n",
        "\n",
        "    \n",
        "\n",
        "def W2V_1 (dset,\n",
        "          left1,right1,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #rows=1\n",
        "    matrix1 = np.array(np.zeros(rows*3).reshape(rows,3),dtype=object)\n",
        "    embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\")\n",
        "    for i in range(rows):\n",
        "        for j in range(3):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = embed([left1[i]])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = embed([right1[i]])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdUymwu_KM3n"
      },
      "source": [
        "ValidationDataSet10.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10XBl9h-KM3n"
      },
      "source": [
        "ValidationDataSet10.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_az18GrKM3n"
      },
      "source": [
        "ValidationDataSet10.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHjHDiYiKM3o"
      },
      "source": [
        "# Initialize prepocess_dataset Word2Vec Matrix with \n",
        "\n",
        "ValidationDataSet10W2V = W2V_1(ValidationDataSet10,\n",
        "                     ValidationDataSet10.title_left,ValidationDataSet10.title_right,\n",
        "                     ValidationDataSet10.label)\n",
        "print (\"ValidationDataSet10\")\n",
        "print (ValidationDataSet10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QalX4qtyKM3o"
      },
      "source": [
        "ValidationDataSet10W2V.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMlzQO4uKM3o"
      },
      "source": [
        "ValidationDataSet10W2V[0,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87VVEO-PKM3o"
      },
      "source": [
        "ValidationDataSet10.label.loc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCTc-KCuKM3o"
      },
      "source": [
        "ValidationDataSet10W2V[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muqfxJNsKM3o"
      },
      "source": [
        "#TestDataSet10W2V save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V', ValidationDataSet10W2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etvTlevIKM3o"
      },
      "source": [
        "#TestDataSet10W2V load numpy array npy in binary format\n",
        "ValidationDataSet10W2V = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V.npy',mmap_mode=None,allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4ZpjIJqKM3o"
      },
      "source": [
        "a = ValidationDataSet10W2V[0]\n",
        "-5.41110002e-02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIe92acPKM3p"
      },
      "source": [
        "row_count = len(ValidationDataSet10W2V[:])\n",
        "col_count = len(ValidationDataSet10W2V[:][0])\n",
        "print (\"Row_Count:%d   Col_Count:%d \" %(row_count,col_count))\n",
        "\n",
        "row_count = len(a[:])\n",
        "col_count = len(a[:][0])\n",
        "print (\"Row_Count:%d   Col_Count:%d \" %(row_count,col_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGHXu1UrKM3p"
      },
      "source": [
        "a = a.reshape(1,3)\n",
        "row_count = len(a[:])\n",
        "col_count = len(a[:][0])\n",
        "print (\"Row_Count:%d   Col_Count:%d \" %(row_count,col_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49XMgr2XKM3p"
      },
      "source": [
        "a[0,0][0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv-vfp_6KM3p"
      },
      "source": [
        "b [0,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xewe3GkxKM3p",
        "outputId": "5383c8db-da2c-4363-8cb4-5d30539cbf21"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0bJo5ERKM3p"
      },
      "source": [
        "b = np.array(np.zeros(1*501).reshape(1,501))\n",
        "b[0,0] = ValidationDataSet10W2V[0][0][0][0]\n",
        "b[0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNcbLnOlKM3p"
      },
      "source": [
        "b = np.array(np.zeros(1*501).reshape(1,501))\n",
        "b.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7c0SZn5KM3q"
      },
      "source": [
        "rows = len(ValidationDataSet10W2V[:])\n",
        "columns = len(ValidationDataSet10W2V[:][0])\n",
        "b = np.array(np.zeros(rows*501).reshape(rows,501))\n",
        "b.shape\n",
        "print (\"rows=%s\" %(rows))\n",
        "print (\"columns=%s\" %(columns))\n",
        "for i in range(rows):\n",
        "  print(\"i=%s\" %(i))\n",
        "  for j in range(columns):\n",
        "    print(\"j=%s\" %(j))\n",
        "    if j == 2:\n",
        "      b[i,j] = ValidationDataSet10W2V[i,j]\n",
        "      print (\"b[i,j]=%s\" %(b[i,j]))\n",
        "      print (\"ValidationDataSet10W2V[i,j]=%s\" %(ValidationDataSet10W2V[i,j]))\n",
        "    else:\n",
        "      for k in range(250):\n",
        "        print(\"k=%s\" %(k))\n",
        "        b[i,k] = ValidationDataSet10W2V[i,j][0,k]\n",
        "        print (\"b[i,k]=%s\" %(b[0,k]))\n",
        "        print (\"ValidationDataSet10W2V[i,j][0,k]=%s\" %(ValidationDataSet10W2V[i,j][0,k]))\n",
        "print(b)\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5lmUBFFKM3q"
      },
      "source": [
        "def w2vMatrix501 (npyArray):\n",
        "  rows = len(npyArray[:])\n",
        "  columns = len(npyArray[:][0])\n",
        "  newMatrix = np.array(np.zeros(rows*501).reshape(rows,501))\n",
        "  newMatrix.shape\n",
        "  print (\"rows=%s\" %(rows))\n",
        "  print (\"columns=%s\" %(columns))\n",
        "  for i in range(rows):\n",
        "    #print(\"i=%s\" %(i))\n",
        "    for j in range(columns):\n",
        "      #print(\"j=%s\" %(j))\n",
        "      if j == 2:\n",
        "        newMatrix[i,500] = npyArray[i,j]\n",
        "        #print (\"newMatrix[i,j]=%s\" %(newMatrix[i,j]))\n",
        "        #print (\"npyArray[i,j]=%s\" %(npyArray[i,j]))\n",
        "      else:\n",
        "        for k in range(250):\n",
        "          #print(\"k=%s\" %(k))\n",
        "          if j == 1:\n",
        "            k250=k+250\n",
        "            #print(\"k250=%s\" %(k250))\n",
        "            newMatrix[i,k250] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k250]=%s\" %(newMatrix[0,k250]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "          else:\n",
        "            newMatrix[i,k] = npyArray[i,j][0,k]\n",
        "            #print (\"newMatrix[i,k]=%s\" %(newMatrix[0,k]))\n",
        "            #print (\"npyArray[i,j][0,k]=%s\" %(npyArray[i,j][0,k]))\n",
        "  #print(newMatrix)\n",
        "  return (newMatrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYxH-Sk5KM3q"
      },
      "source": [
        "ValidationDataSet10W2V_501 = w2vMatrix501(ValidationDataSet10W2V)\n",
        "ValidationDataSet10W2V_501.shape\n",
        "ValidationDataSet10W2V_501.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i4-sHrWKM3q"
      },
      "source": [
        "print (ValidationDataSet10W2V_501.shape)\n",
        "print (ValidationDataSet10W2V_501.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUr_txpmKM3q"
      },
      "source": [
        "ValidationDataSet10W2V[3544,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81w4bdkyKM3q"
      },
      "source": [
        "ValidationDataSet10W2V[3544,1][0,249]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQfn_dT_KM3q"
      },
      "source": [
        "ValidationDataSet10W2V_501[3544,500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbkQCcblKM3q"
      },
      "source": [
        "#TestDataSet10W2V_501 save as numpy array npy in binary format\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V_501', ValidationDataSet10W2V_501)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RO4KrHeLwxY"
      },
      "source": [
        "# ***Preprocessing Data Corpus***\n",
        "\n",
        "DL Similarity Matrix Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg95lAUTMFK2"
      },
      "source": [
        "##Preprocessing of TrainDataSet.pkl with DL Similarity Matrix\n",
        "2020-10-23 JXHALLO: Re Preprocesing DL TrainingDataSet\n",
        "Adjusting DL Function to use max len of strings as denominator in order to ensure DL returns a number between [0,1]\n",
        "\n",
        "**Preprocessed Data Set Outputs:**\n",
        "\n",
        "* TrainDataSetDL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZDPvOC62fE_"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXBKsvix1qBB"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkHJuQL11qBE"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUYCAevw1qBH"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEJUhJvW1xQ0"
      },
      "source": [
        "# 3.1 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnlCEYXKUCld"
      },
      "source": [
        "x = 'JULIO'\n",
        "y = 'HALLOLARREA'\n",
        "lenx = len(x)\n",
        "print (lenx)\n",
        "leny = len(y)\n",
        "print (len(y))\n",
        "z = max ((len(x)),(len(y)))\n",
        "print (z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T94TVkscOKTO"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlN1hiR21qBQ"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWciVFOz1qBS"
      },
      "source": [
        "#2020-07-20 Reading AppendedDataFile using pickle\n",
        "#TestDataSet=pd.read_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/TestDataSet.pkl')\n",
        "import io\n",
        "\n",
        "TestDataSet=pd.read_pickle(io.BytesIO(uploaded['TestDataSet.pkl']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vxs7xXNMknG"
      },
      "source": [
        "TestDataSet=pd.DataFrame(None)\n",
        "print (\"TestDataSet.shape\")\n",
        "print (TestDataSet.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74O00zJVOKTO"
      },
      "source": [
        "#2020-08-07 Reading TrainDataSet using pickle from My drive Google Drive\n",
        "TrainDataSet=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMY_J02vOKTP"
      },
      "source": [
        "print (\"TrainDataSet.shape\")\n",
        "print (TrainDataSet.shape)\n",
        "print (\"TrainDataSet.dtypes\")\n",
        "print (TrainDataSet.dtypes)\n",
        "print (\"TrainDataSet.isnull().sum()\")\n",
        "print (TrainDataSet.isnull().sum())\n",
        "TrainDataSet = TrainDataSet.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SauF4Qgn1qBY"
      },
      "source": [
        "# 4.1- JXHALLO Preprocess Data Corpus to dataFileAll\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Will just create a [n,24] dataframe\n",
        "\n",
        "data2 = dataFileAll(TrainDataSet)\n",
        "print (data2.shape)\n",
        "print (data2.dtypes)\n",
        "print (data2.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpIRQhvR1qBa"
      },
      "source": [
        "# Initialize prepocess_dataset DL Similarity Matrix \n",
        "\n",
        "TrainDataSetDL = DLM1(data2,\n",
        "                     data2.brandListLeft,data2.brandListRight,\n",
        "                     data2.categoryListLeft,data2.categoryListRight,\n",
        "                     data2.descriptionListLeft,data2.descriptionListRight,\n",
        "                     data2.titleListLeft,data2.titleListRight,\n",
        "                     data2.gtin8ListLeft,data2.gtin8ListRight,\n",
        "                     data2.gtin12ListLeft,data2.gtin12ListRight,\n",
        "                     data2.gtin13ListLeft,data2.gtin13ListRight,\n",
        "                     data2.gtin14ListLeft,data2.gtin14ListRight,\n",
        "                     data2.mpnListLeft,data2.mpnListRight,\n",
        "                     data2.skuListLeft,data2.skuListRight,\n",
        "                     data2.labelList)\n",
        "print (\"TrainDataSetDL\")\n",
        "print (TrainDataSetDL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i32Wla57GkZS"
      },
      "source": [
        "TrainDataSetDL[35563]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqtj0ukw1qBc"
      },
      "source": [
        "print (data2.labelList[35563])\n",
        "print (data2.categoryListLeft[35563])\n",
        "print (data2.categoryListRight[35563])\n",
        "print (DL(data2.categoryListLeft[35563],data2.categoryListRight[35563]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3VBPD2D1qBg"
      },
      "source": [
        "print (data2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng9pjwWJpoSX"
      },
      "source": [
        "\n",
        "print (data2.dtypes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOuHIefEpo7e"
      },
      "source": [
        "\n",
        "print (data2.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAfhWl6Of5cW"
      },
      "source": [
        "#numpy.save(file, arr, allow_pickle=True, fix_imports=True)[source]\n",
        "\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSetDL', TrainDataSetDL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtvU82pDOexM"
      },
      "source": [
        "##Preprocessing of TestDataSet10.pkl with DL Similarity Matrix\n",
        "2020-10-23 JXHALLO: Re Preprocesing DL TestDataSet\n",
        "Adjusting DL Function to use max len of strings as denominator in order to ensure DL returns a number between [0,1]\n",
        "\n",
        "**Preprocessed Data Set Outputs:**\n",
        "\n",
        "* TestDataSetDL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQSC25WPOvqY"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwxujkitOvqY"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7vT5ltDOvqZ"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddMwmgqHOvqZ"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH4zX6WvOvqZ"
      },
      "source": [
        "# 3.1 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrXwHB_MOvqZ"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXdh16SfOvqa"
      },
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBgrLPD1Ovqa"
      },
      "source": [
        "#2020-07-20 Reading AppendedDataFile using pickle\n",
        "#TestDataSet=pd.read_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/TestDataSet.pkl')\n",
        "import io\n",
        "\n",
        "TestDataSet=pd.read_pickle(io.BytesIO(uploaded['TestDataSet.pkl']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmDtFKC9Ovqa"
      },
      "source": [
        "TestDataSet=pd.DataFrame(None)\n",
        "print (\"TestDataSet.shape\")\n",
        "print (TestDataSet.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly9cXQY1Ovqa"
      },
      "source": [
        "#2020-08-07 Reading AppendedDataFile using pickle from My drive Google Drive\n",
        "TestDataSet=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/TestDataSet.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkqnJqutOvqa"
      },
      "source": [
        "print (\"TestDataSet.shape\")\n",
        "print (TestDataSet.shape)\n",
        "print (\"TestDataSet.dtypes\")\n",
        "print (TestDataSet.dtypes)\n",
        "print (\"TestDataSet.isnull().sum()\")\n",
        "print (TestDataSet.isnull().sum())\n",
        "TestDataSet = TestDataSet.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoxLczW2Ovqa"
      },
      "source": [
        "# 4.1- JXHALLO Preprocess Data Corpus to dataFileAll\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Will just create a [n,24] dataframe\n",
        "\n",
        "data2 = dataFileAll(TestDataSet)\n",
        "print (data2.shape)\n",
        "print (data2.dtypes)\n",
        "print (data2.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47hjjyC8Ovqb"
      },
      "source": [
        "# Initialize prepocess_dataset DL Similarity Matrix \n",
        "\n",
        "TestDataSetDL2 = DLM1(data2,\n",
        "                     data2.brandListLeft,data2.brandListRight,\n",
        "                     data2.categoryListLeft,data2.categoryListRight,\n",
        "                     data2.descriptionListLeft,data2.descriptionListRight,\n",
        "                     data2.titleListLeft,data2.titleListRight,\n",
        "                     data2.gtin8ListLeft,data2.gtin8ListRight,\n",
        "                     data2.gtin12ListLeft,data2.gtin12ListRight,\n",
        "                     data2.gtin13ListLeft,data2.gtin13ListRight,\n",
        "                     data2.gtin14ListLeft,data2.gtin14ListRight,\n",
        "                     data2.mpnListLeft,data2.mpnListRight,\n",
        "                     data2.skuListLeft,data2.skuListRight,\n",
        "                     data2.labelList)\n",
        "print (\"TestDataSetDL2\")\n",
        "print (TestDataSetDL2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7guoAoB9ZzYn"
      },
      "source": [
        "TestDataSetDL = TestDataSetDL2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56I4qRZcbHK"
      },
      "source": [
        "print (\"TestDataSetDL\")\n",
        "print (TestDataSetDL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78U5YFdKOvqb"
      },
      "source": [
        "TestDataSetDL[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWATLoM0Ovqb"
      },
      "source": [
        "print (data2.labelList[0])\n",
        "print (data2.categoryListLeft[0])\n",
        "print (data2.categoryListRight[0])\n",
        "print (DL(data2.categoryListLeft[0],data2.categoryListRight[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYx1-_Z0VuYs"
      },
      "source": [
        "print (data2.labelList[0])\n",
        "print (data2.descriptionListLeft[0])\n",
        "print (data2.descriptionListRight[0])\n",
        "print (DL(data2.descriptionListLeft[0],data2.descriptionListRight[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ybr824JZWFy"
      },
      "source": [
        "print (len(data2.descriptionListLeft[0]))\n",
        "print (len(data2.descriptionListRight[0]))\n",
        "print (damerau_levenshtein (data2.descriptionListLeft[0],data2.descriptionListRight[0]))\n",
        "print ((damerau_levenshtein (data2.descriptionListLeft[0],data2.descriptionListRight[0]))/(len(data2.descriptionListRight[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0B_7VxwOvqe"
      },
      "source": [
        "#numpy.save(file, arr, allow_pickle=True, fix_imports=True)[source]\n",
        "#TestDataSet10DL = asarray (TestDataSet10DL)\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/TestDataSetDL', TestDataSetDL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQnimQ1wPhT1"
      },
      "source": [
        "##Preprocessing of all_gsDataSet with DL Similarity Matrix\n",
        "2020-10-23 JXHALLO: Re Preprocesing DL all_gsDataSet\n",
        "Adjusting DL Function to use max len of strings as denominator in order to ensure DL returns a number between [0,1]\n",
        "\n",
        "**Preprocessed Data Set Outputs:**\n",
        "\n",
        "* all_gsDataSetDL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUHO1YLHQn-Z"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGQvUZazQn-Z"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ktir3PFQn-Z"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OlBZBfiiQn-Z",
        "outputId": "2fda1037-d449-4d67-ec90-8d1bcf8c12a1"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZqIimnIfQn-a",
        "outputId": "c48dae8b-31a1-47d5-a137-896b703bd1e4"
      },
      "source": [
        "# 3.1 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aUfvEcYQn-a"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9SJvCF_Qn-a"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8syTPvf2Qn-a"
      },
      "source": [
        "#2020-07-20 Reading AppendedDataFile using pickle\n",
        "#TestDataSet=pd.read_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/TestDataSet.pkl')\n",
        "import io\n",
        "\n",
        "TestDataSet=pd.read_pickle(io.BytesIO(uploaded['TestDataSet.pkl']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58OqF0YkQn-a"
      },
      "source": [
        "TestDataSet=pd.DataFrame(None)\n",
        "print (\"TestDataSet.shape\")\n",
        "print (TestDataSet.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odV6Ebu_Qn-b"
      },
      "source": [
        "#2020-08-17 Reading all_gs using pickle from My drive Google Drive\n",
        "#all_gsDataSet=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet.pkl')\n",
        "\n",
        "url = \"/content/drive/My Drive/Python/Corpus/all_gs.json\"\n",
        "\n",
        "all_gsDataSet = pd.read_json(url,lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y-Z-CwXQn-b"
      },
      "source": [
        "print (\"all_gsDataSet.shape\")\n",
        "print (all_gsDataSet.shape)\n",
        "print (\"all_gsDataSet.dtypes\")\n",
        "print (all_gsDataSet.dtypes)\n",
        "print (\"all_gsDataSet.isnull().sum()\")\n",
        "print (all_gsDataSet.isnull().sum())\n",
        "all_gsDataSet = all_gsDataSet.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoeq2hiEQn-b"
      },
      "source": [
        "# 4.1- JXHALLO Preprocess Data Corpus to dataFileAll\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Will just create a [n,24] dataframe\n",
        "\n",
        "data2 = dataFileAll(all_gsDataSet)\n",
        "print (data2.shape)\n",
        "print (data2.dtypes)\n",
        "print (data2.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IAGtyw5Qn-b"
      },
      "source": [
        "# Initialize prepocess_dataset DL Similarity Matrix \n",
        "\n",
        "all_gsDataSetDL = DLM1(data2,\n",
        "                     data2.brandListLeft,data2.brandListRight,\n",
        "                     data2.categoryListLeft,data2.categoryListRight,\n",
        "                     data2.descriptionListLeft,data2.descriptionListRight,\n",
        "                     data2.titleListLeft,data2.titleListRight,\n",
        "                     data2.gtin8ListLeft,data2.gtin8ListRight,\n",
        "                     data2.gtin12ListLeft,data2.gtin12ListRight,\n",
        "                     data2.gtin13ListLeft,data2.gtin13ListRight,\n",
        "                     data2.gtin14ListLeft,data2.gtin14ListRight,\n",
        "                     data2.mpnListLeft,data2.mpnListRight,\n",
        "                     data2.skuListLeft,data2.skuListRight,\n",
        "                     data2.labelList)\n",
        "print (\"all_gsDataSetDL\")\n",
        "print (all_gsDataSetDL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0unPsXFHQn-b"
      },
      "source": [
        "all_gsDataSetDL[4399]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTFm8poLQn-b"
      },
      "source": [
        "print (data2.labelList[0])\n",
        "print (data2.categoryListLeft[0])\n",
        "print (data2.categoryListRight[0])\n",
        "print (DL(data2.categoryListLeft[0],data2.categoryListRight[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UprB3o6eQn-b"
      },
      "source": [
        "print (data2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsCbgwi0Qn-c"
      },
      "source": [
        "\n",
        "print (data2.dtypes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mmpDdy3Qn-c"
      },
      "source": [
        "\n",
        "print (data2.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9rBEzbnQn-c"
      },
      "source": [
        "#numpy.save(file, arr, allow_pickle=True, fix_imports=True)[source]\n",
        "\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/all_gsDataSetDL', all_gsDataSetDL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAl_r7xp9pE1"
      },
      "source": [
        "all_gsDataSetDL[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvwhZnxY9qyv"
      },
      "source": [
        "all_gsDataSetDL.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rdGCc7kQ2WM"
      },
      "source": [
        "## Preprocessing Data Corpus: Preprocessing of TrainDataSet.pkl to Create 90% Hyperparameter Training Data Set with DL Similarity Matrix\n",
        "2020-11-07 JXHALLO: Re Preprocesing DL TrainingDataSet\n",
        "Adjusting DL Function to use max len of strings as denominator in order to ensure DL returns a number between [0,1]\n",
        "Removing from train data set the validation data set records (10%) to have a clean 90%\n",
        "\n",
        "\n",
        "**Preprocessed Data Set Outputs:**\n",
        "\n",
        "* TrainDataSet90DL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x61YBXTsRjPm"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deTCQt7VRjPn"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGXZZAvQRjPn"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3023kNCRjPn"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXEV1yRVRjPo"
      },
      "source": [
        "# 3.1 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na_SyzH5RjPo"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwHSstjgRjPq"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdv6td_4RjPq"
      },
      "source": [
        "#2020-07-20 Reading AppendedDataFile using pickle\n",
        "#TestDataSet=pd.read_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/TestDataSet.pkl')\n",
        "import io\n",
        "\n",
        "TestDataSet=pd.read_pickle(io.BytesIO(uploaded['TestDataSet.pkl']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZDqkxn4RjPq"
      },
      "source": [
        "TestDataSet=pd.DataFrame(None)\n",
        "print (\"TestDataSet.shape\")\n",
        "print (TestDataSet.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjgYZokuRjPq"
      },
      "source": [
        "#2020-08-07 Reading TrainDataSet using pickle from My drive Google Drive\n",
        "TrainDataSet=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7P5IO5ztTUr"
      },
      "source": [
        "#2020-08-07 Reading AppendedDataFile using pickle from My drive Google Drive\n",
        "TrainDataSet90=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCXSMrvNxnwM"
      },
      "source": [
        "#2020-11-07 Reading TrainDataSet90 using pickle from My drive Google Drive\n",
        "ValidationDataSet10=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqWaQiwhRjPq"
      },
      "source": [
        "print (\"TrainDataSet.shape\")\n",
        "print (TrainDataSet.shape)\n",
        "print (\"TrainDataSet.dtypes\")\n",
        "print (TrainDataSet.dtypes)\n",
        "print (\"TrainDataSet.isnull().sum()\")\n",
        "print (TrainDataSet.isnull().sum())\n",
        "TrainDataSet = TrainDataSet.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsTBhCxVRjPr"
      },
      "source": [
        "print (\"ValidationDataSet10.shape\")\n",
        "print (ValidationDataSet10.shape)\n",
        "print (\"ValidationDataSet10.dtypes\")\n",
        "print (ValidationDataSet10.dtypes)\n",
        "print (\"ValidationDataSet10.isnull().sum()\")\n",
        "print (ValidationDataSet10.isnull().sum())\n",
        "ValidationDataSet10 = ValidationDataSet10.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6ReAoPmt7dC"
      },
      "source": [
        "#2020-11-07 JXHALLO: Removing from TrainDataSet the ValidationDataSet10 records.\n",
        "#TrainDataSet90 = pd.merge (TrainDataSet, ValidationDataSet10, how='outer', indicator=True)\n",
        "cond = TrainDataSet['pair_id'].isin(ValidationDataSet10['pair_id'])\n",
        "TrainDataSet.drop(TrainDataSet[cond].index, inplace = True)\n",
        "TrainDataSet90 = TrainDataSet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d28vhPHZvp5R"
      },
      "source": [
        "print (\"TrainDataSet90.shape\")\n",
        "print (TrainDataSet90.shape)\n",
        "print (\"TrainDataSet90.dtypes\")\n",
        "print (TrainDataSet90.dtypes)\n",
        "print (\"TrainDataSet90.isnull().sum()\")\n",
        "print (TrainDataSet90.isnull().sum())\n",
        "TrainDataSet90 = TrainDataSet90.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWcLw0GERjPs"
      },
      "source": [
        "# 4.1- JXHALLO Preprocess Data Corpus to dataFileAll\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Will just create a [n,24] dataframe\n",
        "\n",
        "data2 = dataFileAll(TrainDataSet90)\n",
        "print (data2.shape)\n",
        "print (data2.dtypes)\n",
        "print (data2.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNrUnQ5wRjPs"
      },
      "source": [
        "# Initialize prepocess_dataset DL Similarity Matrix \n",
        "\n",
        "TrainDataSet90DL = DLM1(data2,\n",
        "                     data2.brandListLeft,data2.brandListRight,\n",
        "                     data2.categoryListLeft,data2.categoryListRight,\n",
        "                     data2.descriptionListLeft,data2.descriptionListRight,\n",
        "                     data2.titleListLeft,data2.titleListRight,\n",
        "                     data2.gtin8ListLeft,data2.gtin8ListRight,\n",
        "                     data2.gtin12ListLeft,data2.gtin12ListRight,\n",
        "                     data2.gtin13ListLeft,data2.gtin13ListRight,\n",
        "                     data2.gtin14ListLeft,data2.gtin14ListRight,\n",
        "                     data2.mpnListLeft,data2.mpnListRight,\n",
        "                     data2.skuListLeft,data2.skuListRight,\n",
        "                     data2.labelList)\n",
        "print (\"TrainDataSet90DL\")\n",
        "print (TrainDataSet90DL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwyUQV1cRjPs"
      },
      "source": [
        "TrainDataSet90DL[35563]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIQBDZzHRjPs"
      },
      "source": [
        "print (data2.labelList[35563])\n",
        "print (data2.categoryListLeft[35563])\n",
        "print (data2.categoryListRight[35563])\n",
        "print (DL(data2.categoryListLeft[35563],data2.categoryListRight[35563]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-LvUx0aRjPs"
      },
      "source": [
        "print (data2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUeVXqiiRjPt"
      },
      "source": [
        "\n",
        "print (data2.dtypes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0BzE2I8RjPt"
      },
      "source": [
        "\n",
        "print (data2.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekRJPLG-RjPt"
      },
      "source": [
        "#numpy.save(file, arr, allow_pickle=True, fix_imports=True)[source]\n",
        "\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90DL', TrainDataSet90DL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSYAY7l2RjPt"
      },
      "source": [
        "TrainDataSet90DL[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZzcDPCWRjPt"
      },
      "source": [
        "TrainDataSet90DL.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOQREvkBRzMs"
      },
      "source": [
        "## Preprocessing Data Corpus: Preprocessing of TrainDataSet.pkl to Create 10% Hyperparameter Validation Data Set with DL Similarity Matrix\n",
        "2020-11-07 JXHALLO: Re Preprocesing DL TrainingDataSet\n",
        "Adjusting DL Function to use max len of strings as denominator in order to ensure DL returns a number between [0,1]\n",
        "Removing from train data set the train data set records (90%) to have a clean 10%\n",
        "\n",
        "\n",
        "**Preprocessed Data Set Outputs:**\n",
        "\n",
        "* ValidationDataSet10DL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcyPuzKJSO3E",
        "outputId": "75cd0698-4686-4058-cabe-08dce2e0c883"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.6/dist-packages (2.7.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvMUq0t3SO3E"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FYZ_2ZrSO3F"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd8rN5EISO3F",
        "outputId": "fb316ba0-f44e-42f4-af9e-dbd92efadb70"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srma3QZlSO3F",
        "outputId": "013a6eeb-b3d6-4de3-a096-639c4d458837"
      },
      "source": [
        "# 3.1 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7go_TpK6SO3F"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aRq4u4xSO3H"
      },
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8aFCUXiSO3H"
      },
      "source": [
        "#2020-07-20 Reading AppendedDataFile using pickle\n",
        "#TestDataSet=pd.read_pickle('/Users/juliohallo/Documents/Personal/Maestria 2018/Python/20200720_DataAnalysis/TestDataSet.pkl')\n",
        "import io\n",
        "\n",
        "ValidationDataSet10=pd.read_pickle(io.BytesIO(uploaded['ValidationDataSet10.pkl']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzYeaoeVSO3H"
      },
      "source": [
        "ValidationDataSet10=pd.DataFrame(None)\n",
        "print (\"ValidationDataSet10.shape\")\n",
        "print (ValidationDataSet10.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpHcqiggSO3H"
      },
      "source": [
        "#2020-08-07 Reading AppendedDataFile using pickle from My drive Google Drive\n",
        "ValidationDataSet10=pd.read_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U87R6ljSO3H"
      },
      "source": [
        "print (\"ValidationDataSet10.shape\")\n",
        "print (ValidationDataSet10.shape)\n",
        "print (\"ValidationDataSet10.dtypes\")\n",
        "print (ValidationDataSet10.dtypes)\n",
        "print (\"ValidationDataSet10.isnull().sum()\")\n",
        "print (ValidationDataSet10.isnull().sum())\n",
        "ValidationDataSet10 = ValidationDataSet10.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC292wkTSO3I"
      },
      "source": [
        "# 4.1- JXHALLO Preprocess Data Corpus to dataFileAll\n",
        "\n",
        "# all_train_xlarge_sample.json\n",
        "# Complete Data Catalog available at: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# Data Corpus Explanation available at: http://webdatacommons.org/largescaleproductcorpus/v2/index.html\n",
        "# Will just create a [n,24] dataframe\n",
        "\n",
        "data2 = dataFileAll(ValidationDataSet10)\n",
        "print (data2.shape)\n",
        "print (data2.dtypes)\n",
        "print (data2.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbriwx86SO3I"
      },
      "source": [
        "# Initialize prepocess_dataset DL Similarity Matrix \n",
        "\n",
        "ValidationDataSet10DL2 = DLM1(data2,\n",
        "                     data2.brandListLeft,data2.brandListRight,\n",
        "                     data2.categoryListLeft,data2.categoryListRight,\n",
        "                     data2.descriptionListLeft,data2.descriptionListRight,\n",
        "                     data2.titleListLeft,data2.titleListRight,\n",
        "                     data2.gtin8ListLeft,data2.gtin8ListRight,\n",
        "                     data2.gtin12ListLeft,data2.gtin12ListRight,\n",
        "                     data2.gtin13ListLeft,data2.gtin13ListRight,\n",
        "                     data2.gtin14ListLeft,data2.gtin14ListRight,\n",
        "                     data2.mpnListLeft,data2.mpnListRight,\n",
        "                     data2.skuListLeft,data2.skuListRight,\n",
        "                     data2.labelList)\n",
        "print (\"ValidationDataSet10DL2\")\n",
        "print (ValidationDataSet10DL2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4LfoNwKSO3I"
      },
      "source": [
        "ValidationDataSet10DL = ValidationDataSet10DL2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJlCjoqySO3I"
      },
      "source": [
        "print (\"ValidationDataSet10DL\")\n",
        "print (ValidationDataSet10DL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxKGWoACSO3I"
      },
      "source": [
        "ValidationDataSet10DL[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRioxyVESO3I"
      },
      "source": [
        "print (data2.labelList[0])\n",
        "print (data2.categoryListLeft[0])\n",
        "print (data2.categoryListRight[0])\n",
        "print (DL(data2.categoryListLeft[0],data2.categoryListRight[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwgkkZcDSO3I"
      },
      "source": [
        "print (data2.labelList[0])\n",
        "print (data2.descriptionListLeft[0])\n",
        "print (data2.descriptionListRight[0])\n",
        "print (DL(data2.descriptionListLeft[0],data2.descriptionListRight[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqKFUkrgSO3I"
      },
      "source": [
        "print (len(data2.descriptionListLeft[0]))\n",
        "print (len(data2.descriptionListRight[0]))\n",
        "print (damerau_levenshtein (data2.descriptionListLeft[0],data2.descriptionListRight[0]))\n",
        "print ((damerau_levenshtein (data2.descriptionListLeft[0],data2.descriptionListRight[0]))/(len(data2.descriptionListRight[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtiMg-QNSO3J"
      },
      "source": [
        "print (data2.loc[3544])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkJ8ymsQSO3J"
      },
      "source": [
        "\n",
        "print (data2.dtypes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fxqBkujSO3J"
      },
      "source": [
        "\n",
        "print (data2.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsSIW01tzGXP"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-08-07 JUHO: Saving TestDataSetDL preprocessed data using DL Similarity\n",
        "ValidationDataSet10DL.to_pickle('content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10DL.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKYCEJIggwd3"
      },
      "source": [
        "\n",
        "# save numpy array as npy file\n",
        "#from numpy import asarray\n",
        "#from numpy import save\n",
        "\n",
        "# define data\n",
        "data = asarray([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
        "# save to npy file\n",
        "#save('content/drive/My Drive/Python/20200720_DataAnalysis/data.npy', data)\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/data', data, allow_pickle=True, fix_imports=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbRpk_ead4V5"
      },
      "source": [
        "data[0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltr4_Eb4SO3J"
      },
      "source": [
        "#numpy.save(file, arr, allow_pickle=True, fix_imports=True)[source]\n",
        "#TestDataSet10DL = asarray (TestDataSet10DL)\n",
        "np.save('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10DL', ValidationDataSet10DL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZLd-RhEiT95"
      },
      "source": [
        "ValidationDataSet10DL[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbQbMOx7LRFO"
      },
      "source": [
        "ValidationDataSet10DL.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3k8M5TMSzLR"
      },
      "source": [
        "# ***Hyperparameter Estimation***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx4DEMTGTYYB"
      },
      "source": [
        "## ***Hyperparameter Estimation MLP DL Similarity Matrix***\n",
        "\n",
        "MLP - DL Similarity Matrix Preprocessing\n",
        "\n",
        "2020-11-08\n",
        "JUHO: MLP Architecture\n",
        "Data Sets:\n",
        "Train: TrainDataSet90DL\n",
        "Validation: ValidationDataSetDL\n",
        "Preprocessing Method: DL Data Set Matrixes\n",
        "Using TALOS Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoJwl2pjTflO"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh\n",
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools \n",
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# For the current version: \n",
        "!pip install --upgrade tensorflow\n",
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZhpf5y-nDXt"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMusiVNGnGcX"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZsgHNTSnUO1"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10DL = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10DL.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4sNXpUb6HbI"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90DL = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90DL.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKDpXAZXnUO1"
      },
      "source": [
        "ValidationDataSet10DL[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6L5sg-u6WJI"
      },
      "source": [
        "TrainDataSet90DL[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqXJUTcRnUO2"
      },
      "source": [
        "ValidationDataSet10DL.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoQPHevv6e45"
      },
      "source": [
        "TrainDataSet90DL.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3YX96IZ8uFd"
      },
      "source": [
        "ValidationDataSet10DL_data,ValidationDataSet10DL_label=np.split(ValidationDataSet10DL,[10],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0yGQlkm6iAr"
      },
      "source": [
        "TrainDataSet90DL_data,TrainDataSet90DL_label=np.split(TrainDataSet90DL,[10],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE9VapOVBh2W"
      },
      "source": [
        "print (ValidationDataSet10DL_data.shape)\n",
        "print (ValidationDataSet10DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORPgAWhjaRLz"
      },
      "source": [
        "np.count_nonzero(ValidationDataSet10DL_label == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5v15NnI6yWW"
      },
      "source": [
        "print (TrainDataSet90DL_data.shape)\n",
        "print (TrainDataSet90DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XubCi9gPbDxx"
      },
      "source": [
        "ValidationDataSet10DL_data[0,2]\n",
        "print (ValidationDataSet10DL_data.max())\n",
        "print (ValidationDataSet10DL_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(ValidationDataSet10DL_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(ValidationDataSet10DL_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gRaKGCo68Wd"
      },
      "source": [
        "TrainDataSet90DL_data[0,2]\n",
        "print (TrainDataSet90DL_data.max())\n",
        "print (TrainDataSet90DL_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(TrainDataSet90DL_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(TrainDataSet90DL_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEnGgWfhS-Zw"
      },
      "source": [
        "ValidationDataSet10DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AND7xueg7H3B"
      },
      "source": [
        "TrainDataSet90DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvdyX44hOMus"
      },
      "source": [
        "print (ValidationDataSet10DL_data.dtype)\n",
        "print (ValidationDataSet10DL_label.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ClY7EOG7Nry"
      },
      "source": [
        "print (TrainDataSet90DL_data.dtype)\n",
        "print (TrainDataSet90DL_label.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2amoNFlKndZ4"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbIF0xeLHm9P"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVmuiIKqH8J4"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOk7F_PWJJmG"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i40PBQmR8p4c"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnTs5kDVOS7R"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ny1ZaWDjWtK"
      },
      "source": [
        "# Defining the Model\n",
        "\n",
        "def multi_layer_percetron_FFNN_DL_pOpt(x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "    # next we can build the model exactly like we would normally do it\n",
        "    model = Sequential()\n",
        "    # model.add(Input(shape=10))\n",
        "    model.add(Dense(params['first_neuron'],\n",
        "                    input_dim=x_train.shape[1],\n",
        "                    activation=params['activation'],\n",
        "                    kernel_initializer='normal'))\n",
        "        # if we want to also test for number of layers and shapes, that's possible\n",
        "\n",
        "    model.add(Dropout(params['dropout']))\n",
        "        # if we want to also test for number of layers and shapes, that's possible\n",
        "    \n",
        "    \n",
        "    hidden_layers(model, params, 1)\n",
        "\n",
        "   \n",
        "    # then we finish again with completely standard Keras way\n",
        "    model.add(Dense(1, activation=params['last_activation'],\n",
        "                    kernel_initializer='normal'))\n",
        "    \n",
        "    model.compile(loss=params['losses'],\n",
        "                  # here we add a regulizer normalization function from Talos\n",
        "                  optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
        "                  metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                  )\n",
        "    plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "    model.summary()             \n",
        "    \n",
        "    history = model.fit(x_train,\n",
        "                        y_train, \n",
        "                        validation_data=(x_val, y_val),\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        #callbacks=[ExperimentLogCallback('multi_layer_percetron_FFNN', params)]\n",
        "                        )\n",
        "    \n",
        "    \n",
        "    # finally we have to make sure that history object and model are returned\n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dx6laVvmJd3"
      },
      "source": [
        "# then we can go ahead and set the parameter space\n",
        "p = {'lr': (0.5, 5, 2),\n",
        "     'first_neuron':[500,600,700,800],\n",
        "     'hidden_layers':[1,2,3,4],\n",
        "     'batch_size': [1000],\n",
        "     'epochs': [100,200,1000],\n",
        "     'dropout': (0, 0.5, 3),\n",
        "     'weight_regulizer':[None],\n",
        "     'emb_output_dims': [None],\n",
        "     'shape':['brick'],\n",
        "     'shapes':['brick'],\n",
        "     'optimizer': [Adam,Nadam,RMSprop],\n",
        "     'losses': ['binary_crossentropy'],\n",
        "     'activation':['relu', 'elu'],\n",
        "     'last_activation': ['sigmoid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69kns-f53_o_"
      },
      "source": [
        "# Parameters P. Estimating Architecture Parameters as First Neuron and Hidden Layers\n",
        "pArch = {'lr': [0.5],\n",
        "     'first_neuron':[10],\n",
        "     'hidden_layers':[1,2,3,4,5,6,7,8,9,10],\n",
        "     'batch_size': [1000],\n",
        "     'epochs': [10,100,1000],\n",
        "     'dropout': [0.5],\n",
        "     'weight_regulizer':[None],\n",
        "     'emb_output_dims': [None],\n",
        "     'shape':['brick'],\n",
        "     'shapes':['funnel','brick','triangle'],\n",
        "     'optimizer': [Adam],\n",
        "     'losses': ['binary_crossentropy'],\n",
        "     'activation':['elu'],\n",
        "     'last_activation': ['sigmoid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb3uRIgbyZZe"
      },
      "source": [
        "# Parameters P. Estimating Architecture Parameters as First Neuron and Hidden Layers\n",
        "pOpt = {'lr': (0.5, 5, 2),\n",
        "     'first_neuron':[10],\n",
        "     'hidden_layers':[4],\n",
        "     'batch_size': [1000],\n",
        "     'epochs': [100],\n",
        "     'dropout': (0, 0.5, 3),\n",
        "     'weight_regulizer':[None],\n",
        "     'emb_output_dims': [None],\n",
        "     'shape':['brick'],\n",
        "     'shapes':['brick'],\n",
        "     'optimizer': [SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam],\n",
        "    #  'optimizer': [Adadelta],\n",
        "     'losses': ['binary_crossentropy', 'logcosh'],\n",
        "    #  'activation':['relu','tanh', 'selu', 'elu', 'exponential'],\n",
        "     'activation':['relu','tanh', 'selu', 'elu', 'sigmoid', 'softmax', 'softplus', 'softsign'],\n",
        "     'last_activation': ['sigmoid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuvW-z9Mumai"
      },
      "source": [
        "print (ValidationDataSet10DL_data.shape)\n",
        "print (ValidationDataSet10DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqiE25I17ds6"
      },
      "source": [
        "print (TrainDataSet90DL_data.shape)\n",
        "print (TrainDataSet90DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivRPDQtfw6TT"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90DL_data\n",
        "y = TrainDataSet90DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WX-HzWh7oiO"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10DL_data\n",
        "y_val = ValidationDataSet10DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vu_IJShuuCs"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=multi_layer_percetron_FFNN_DL_pOpt,\n",
        "            params=pOpt,\n",
        "            experiment_name='multi_layer_percetron_FFNN_DL_pOpt',\n",
        "            fraction_limit=0.5,\n",
        "            # time_limit = \"2020-11-22 16:00\",\n",
        "            round_limit=400,\n",
        "            print_params=True,\n",
        "            reduction_metric='val_f1score',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_f1score',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpCRwwIiCMlb"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvTyDsTB5SG3"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/MyDrive/Python/20200720_DataAnalysis/20201109_multi_layer_percetron_FFNN_DL_pOpt.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGbnp8CbI_V"
      },
      "source": [
        "analyze_object=pd.read_pickle('/content/drive/My Drive/Python/20201122_DataAnalysis/20201109_multi_layer_percetron_FFNN_DL_pOpt.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4-NBLUYcr_P"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20201109_multi_layer_percetron_FFNN_DL_pOpt.xlsx', sheet_name='20201122_MLP_FFNN_DL_pOpt', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0969YP9Cawo"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS0-dehboYcI"
      },
      "source": [
        "## ***Hyperparameter Estimation MLP - W2V_501***\n",
        "MLP - W2V_501 Preprocessing\n",
        "\n",
        "2021-01-09 JUHO: MLP Architecture\n",
        "Data Sets:\n",
        "Train: TrainDataSet90W2V_501\n",
        "Validation: ValidationDataSet10W2V_501\n",
        "Preprocessing Method: W2V_501 Data Set Matrixes\n",
        "Using TALOS Library\n",
        "Optimization Hyperparameters Estimation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBOsUTdhpZyo"
      },
      "source": [
        "### **Architecture Estimation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9DqA79QpcVh"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh\n",
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools \n",
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# For the current version: \n",
        "!pip install --upgrade tensorflow\n",
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ8R4G4Hp16F"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmc5hGhCp6tA"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUkvPBhwrPiH"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeV0JDsyrPiH"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZEIYb6PrPiI"
      },
      "source": [
        "ValidationDataSet10W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZUT7exErPiL"
      },
      "source": [
        "TrainDataSet90W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv9gKUq2rPiL"
      },
      "source": [
        "ValidationDataSet10W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2BItx5orPiL"
      },
      "source": [
        "TrainDataSet90W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVzVu-VKrPiM"
      },
      "source": [
        "ValidationDataSet10W2V_501_data,ValidationDataSet10W2V_501_label=np.split(ValidationDataSet10W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV7aVGNxrPiM"
      },
      "source": [
        "TrainDataSet90W2V_501_data,TrainDataSet90W2V_501_label=np.split(TrainDataSet90W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvza6sycrPiM"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29su9F5IrPiM"
      },
      "source": [
        "np.count_nonzero(ValidationDataSet10W2V_501_label == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfKWwe_frPiN"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAptCHS8rPiN"
      },
      "source": [
        "ValidationDataSet10W2V_501_data[0,2]\n",
        "print (ValidationDataSet10W2V_501_data.max())\n",
        "print (ValidationDataSet10W2V_501_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(ValidationDataSet10W2V_501_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(ValidationDataSet10W2V_501_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4UWkxu9rPiN"
      },
      "source": [
        "TrainDataSet90W2V_501_data[0,2]\n",
        "print (TrainDataSet90W2V_501_data.max())\n",
        "print (TrainDataSet90W2V_501_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(TrainDataSet90W2V_501_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(TrainDataSet90W2V_501_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bZtwJSLrPiO"
      },
      "source": [
        "ValidationDataSet10W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Cd6lpFmrPiO"
      },
      "source": [
        "TrainDataSet90W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJcpSFUhrPiO"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.dtype)\n",
        "print (ValidationDataSet10W2V_501_label.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_CUWaiqrPiO"
      },
      "source": [
        "print (TrainDataSet90W2V_501.dtype)\n",
        "print (TrainDataSet90W2V_501.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szGfie0rrXC1"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FyHmkyxrfa1"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciBDDcrNrfa2"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP1B9V5Vrfa2"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BZYR9hPrfa2"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDCzRUDBrfa2"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xm6rhKErfa2"
      },
      "source": [
        "# Defining the Model\n",
        "\n",
        "def multi_layer_percetron_FFNN_W2V_501_pArch6(x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "    # next we can build the model exactly like we would normally do it\n",
        "    model = Sequential()\n",
        "    # model.add(Input(shape=10))\n",
        "    model.add(Dense(params['first_neuron'],\n",
        "                    input_dim=x_train.shape[1],\n",
        "                    activation=params['activation'],\n",
        "                    kernel_initializer='normal'))\n",
        "        # if we want to also test for number of layers and shapes, that's possible\n",
        "\n",
        "    model.add(Dropout(params['dropout']))\n",
        "        # if we want to also test for number of layers and shapes, that's possible\n",
        "    \n",
        "    \n",
        "    hidden_layers(model, params, 1)\n",
        "\n",
        "   \n",
        "    # then we finish again with completely standard Keras way\n",
        "    model.add(Dense(1, activation=params['last_activation'],\n",
        "                    kernel_initializer='normal'))\n",
        "    \n",
        "    model.compile(loss=params['losses'],\n",
        "                  # here we add a regulizer normalization function from Talos\n",
        "                  optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
        "                  metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                  )\n",
        "    plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "    model.summary()             \n",
        "    \n",
        "    history = model.fit(x_train,\n",
        "                        y_train, \n",
        "                        validation_data=(x_val, y_val),\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        #callbacks=[ExperimentLogCallback('multi_layer_percetron_FFNN', params)]\n",
        "                        )\n",
        "    \n",
        "    \n",
        "    # finally we have to make sure that history object and model are returned\n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF_tMTCmrfa2"
      },
      "source": [
        "# then we can go ahead and set the parameter space\n",
        "p = {'lr': (0.5, 5, 2),\n",
        "     'first_neuron':[500,600,700,800],\n",
        "     'hidden_layers':[1,2,3,4],\n",
        "     'batch_size': [1000],\n",
        "     'epochs': [100,200,1000],\n",
        "     'dropout': (0, 0.5, 3),\n",
        "     'weight_regulizer':[None],\n",
        "     'emb_output_dims': [None],\n",
        "     'shape':['brick'],\n",
        "     'shapes':['brick'],\n",
        "     'optimizer': [Adam,Nadam,RMSprop],\n",
        "     'losses': ['binary_crossentropy'],\n",
        "     'activation':['relu', 'elu'],\n",
        "     'last_activation': ['sigmoid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grdhu00Xrfa2"
      },
      "source": [
        "# Parameters P. Estimating Architecture Parameters as First Neuron and Hidden Layers\n",
        "pArch = {'lr': [0.5],\n",
        "     'first_neuron':[500],\n",
        "     'hidden_layers':[1,2,3,4,5],\n",
        "     'batch_size': [1000],\n",
        "     'epochs': [10,100],\n",
        "     'dropout': [0.5],\n",
        "     'weight_regulizer':[None],\n",
        "     'emb_output_dims': [None],\n",
        "     'shape':['brick'],\n",
        "     'shapes':['funnel','brick','triangle'],\n",
        "     'optimizer': [Adam],\n",
        "     'losses': ['binary_crossentropy'],\n",
        "     'activation':['elu'],\n",
        "     'last_activation': ['sigmoid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCWnvfEzIAJl"
      },
      "source": [
        "# Parameters P. Estimating Architecture Parameters as First Neuron and Hidden Layers\n",
        "pArch2 = {'lr': [0.5],\n",
        "     'first_neuron':[500],\n",
        "     'hidden_layers':[2,3,4,5],\n",
        "     'batch_size': [1000],\n",
        "     'epochs': [1000],\n",
        "     'dropout': [0.5],\n",
        "     'weight_regulizer':[None],\n",
        "     'emb_output_dims': [None],\n",
        "     'shape':['brick'],\n",
        "     'shapes':['brick'],\n",
        "     'optimizer': [Adam],\n",
        "     'losses': ['binary_crossentropy'],\n",
        "     'activation':['elu'],\n",
        "     'last_activation': ['sigmoid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lle8s0rEqd5Y"
      },
      "source": [
        "# Parameters P. Estimating Architecture Parameters as First Neuron and Hidden Layers\n",
        "pArch6 = {'lr': [0.5],\n",
        "     'first_neuron':[500],\n",
        "     'hidden_layers':[1,2,3,4,5],\n",
        "     'batch_size': [1000],\n",
        "     'epochs': [1000],\n",
        "     'dropout': [0.5],\n",
        "     'weight_regulizer':[None],\n",
        "     'emb_output_dims': [None],\n",
        "     'shape':['brick'],\n",
        "     'shapes':['brick'],\n",
        "     'optimizer': [Adam],\n",
        "     'losses': ['binary_crossentropy'],\n",
        "     'activation':['elu'],\n",
        "     'last_activation': ['sigmoid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd13IbMjrfa3"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOPOykxqrfa3"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjsgCTtMrfa3"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90W2V_501_data\n",
        "y = TrainDataSet90W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUtdwxnErfa3"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10W2V_501_data\n",
        "y_val = ValidationDataSet10W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdBsSrTKrfa3"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=multi_layer_percetron_FFNN_W2V_501_pArch6,\n",
        "            params=pArch6,\n",
        "            experiment_name='multi_layer_percetron_FFNN_W2V_501_pArch6',\n",
        "            # fraction_limit=0.001,\n",
        "            # round_limit=1,\n",
        "            print_params=True,\n",
        "            reduction_metric='val_custom_f1',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_custom_f1',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSOgRYU21kQ0"
      },
      "source": [
        "print (t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0qpT49Vr0_l"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcQXf6_jr0_l"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/20201217_multi_layer_percetron_FFNN_W2V_501_pArch6.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTGXaOAkr0_l"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HfHKevpr0_l"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20201217_multi_layer_percetron_FFNN_W2V_501_pArch6.xlsx', sheet_name='20201217_mlp_W2V_501_pArch6', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5PyoQFsroHT"
      },
      "source": [
        "### ***Optimization Estimation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyDKsK8GrtjU"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivqyExPer_Iv"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZc1lDWpsB3V"
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMV3nd8asEVf"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh\n",
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools \n",
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg1x-xRTsShl"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr-w-_xnsWZG"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnZq5teEsdZK"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJE8O6KKsdZK"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SucjMg0IsdZL"
      },
      "source": [
        "ValidationDataSet10W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT969gjBsdZL"
      },
      "source": [
        "TrainDataSet90W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFyrT6J5sdZL"
      },
      "source": [
        "ValidationDataSet10W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqDp679ZsdZM"
      },
      "source": [
        "TrainDataSet90W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ryxtM77sdZM"
      },
      "source": [
        "ValidationDataSet10W2V_501_data,ValidationDataSet10W2V_501_label=np.split(ValidationDataSet10W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mffytOYsdZM"
      },
      "source": [
        "TrainDataSet90W2V_501_data,TrainDataSet90W2V_501_label=np.split(TrainDataSet90W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YScaPlMJsdZM"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ75Dk4sdZM"
      },
      "source": [
        "np.count_nonzero(ValidationDataSet10W2V_501_label == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "164Gya71sdZM"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC4hzxRksdZN"
      },
      "source": [
        "ValidationDataSet10W2V_501_data[0,2]\n",
        "print (ValidationDataSet10W2V_501_data.max())\n",
        "print (ValidationDataSet10W2V_501_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(ValidationDataSet10W2V_501_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(ValidationDataSet10W2V_501_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaZ2YZAWsdZN"
      },
      "source": [
        "TrainDataSet90W2V_501_data[0,2]\n",
        "print (TrainDataSet90W2V_501_data.max())\n",
        "print (TrainDataSet90W2V_501_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(TrainDataSet90W2V_501_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(TrainDataSet90W2V_501_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQnaIJqJsdZN"
      },
      "source": [
        "ValidationDataSet10W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B8z9ANSsdZN"
      },
      "source": [
        "TrainDataSet90W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoaobP8ksdZN"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.dtype)\n",
        "print (ValidationDataSet10W2V_501_label.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U69YO_rtsdZO"
      },
      "source": [
        "print (TrainDataSet90W2V_501.dtype)\n",
        "print (TrainDataSet90W2V_501.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kowQXE9LsmmD"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osLnaO9Lswlg"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC1C0PG-swlh"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro4dFHa_swlh"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsvWnSyZswlh"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftkt-I1_swlh"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBBKG-94swli"
      },
      "source": [
        "# Defining the Model\n",
        "\n",
        "def multi_layer_percetron_FFNN_W2V_501_pOpt1(x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "    # next we can build the model exactly like we would normally do it\n",
        "    model = Sequential()\n",
        "    # model.add(Input(shape=10))\n",
        "    model.add(Dense(params['first_neuron'],\n",
        "                    input_dim=x_train.shape[1],\n",
        "                    activation=params['activation'],\n",
        "                    kernel_initializer='normal'))\n",
        "        # if we want to also test for number of layers and shapes, that's possible\n",
        "\n",
        "    model.add(Dropout(params['dropout']))\n",
        "        # if we want to also test for number of layers and shapes, that's possible\n",
        "    \n",
        "    \n",
        "    hidden_layers(model, params, 1)\n",
        "\n",
        "   \n",
        "    # then we finish again with completely standard Keras way\n",
        "    model.add(Dense(1, activation=params['last_activation'],\n",
        "                    kernel_initializer='normal'))\n",
        "    \n",
        "    model.compile(loss=params['losses'],\n",
        "                  # here we add a regulizer normalization function from Talos\n",
        "                  optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
        "                  metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                  )\n",
        "    plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "    model.summary()             \n",
        "    \n",
        "    history = model.fit(x_train,\n",
        "                        y_train, \n",
        "                        validation_data=(x_val, y_val),\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        #callbacks=[ExperimentLogCallback('multi_layer_percetron_FFNN', params)]\n",
        "                        )\n",
        "    \n",
        "    \n",
        "    # finally we have to make sure that history object and model are returned\n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bVtoIxrt7jT"
      },
      "source": [
        "# Parameters P. Estimating Optimization Parameters as Learning Rate, dropout, optimizer, losses, activation\n",
        "pOpt1 = {'lr': [0.5,2.75],\n",
        "     'first_neuron':[500],\n",
        "     'hidden_layers':[10],\n",
        "     'batch_size': [1000],\n",
        "     'epochs': [1000],\n",
        "     'dropout': [0,0.1666,0.3333],\n",
        "     'weight_regulizer':[None],\n",
        "     'emb_output_dims': [None],\n",
        "     'shape':['brick'],\n",
        "     'shapes':['brick'],\n",
        "     'optimizer': [SGD,RMSprop,Adam,Adadelta,Adagrad,Adamax,Nadam],\n",
        "     'losses': ['binary_crossentropy','logcosh'],\n",
        "     'activation':['relu','softmax','softplus','softsing','tanh','selu','elu'],\n",
        "     'last_activation': ['sigmoid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc6MuFIuswlk"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKZ3Wtxgswlk"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_S1-DP_swll"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90W2V_501_data\n",
        "y = TrainDataSet90W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjr3_KC2swll"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10W2V_501_data\n",
        "y_val = ValidationDataSet10W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UJhwWaCswll"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=multi_layer_percetron_FFNN_W2V_501_pOpt1,\n",
        "            params=pOpt1,\n",
        "            experiment_name='multi_layer_percetron_FFNN_W2V_501_pOpt1',\n",
        "            # fraction_limit=0.001,\n",
        "            # round_limit=1,\n",
        "            time_limit = \"2021-01-11 20:30\",\n",
        "            print_params=True,\n",
        "            reduction_metric='val_custom_f1',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_custom_f1',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w56Ajfeaswlm"
      },
      "source": [
        "print (t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4czCag_swlm"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFweo6vJswlm"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/20210109_multi_layer_percetron_FFNN_W2V_501_pOpt1.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXDt880oswlm"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX8TVwbpswlm"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20210109_multi_layer_percetron_FFNN_W2V_501_pOpt1.xlsx', sheet_name='20210109_mlp_W2V_501_pOpt1', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEqaKHq_te__"
      },
      "source": [
        "## ***Hyperparameter Estimation LSTM - DL Similarity Matrix***\n",
        "LSTM - DL Similarity Matrix Preprocessing\n",
        "\n",
        "2020-12-13\n",
        "JUHO: LSTM Architecture\n",
        "Data Sets:\n",
        "Train: TrainDataSet90DL\n",
        "Validation: ValidationDataSet10DL\n",
        "Preprocessing Method: DL Data Set Matrixes\n",
        "Using TALOS Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqicU_60t970"
      },
      "source": [
        "### **Architecture Estimation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbmeL5AKlDOC"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhsqKR4nleE0"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbKMntguly2o"
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxn413dDuOB0"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7opgeBAuOB0"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF7GYRu2uOB0"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EdwSUlB2FZz"
      },
      "source": [
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fLEYwifuOB0"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYbRFh-E6-f2"
      },
      "source": [
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPEc-HBruF4V"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks-PVCwDuqie"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHjeDl9Uuwsf"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10DL = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10DL.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwhKWVR6uwsf"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90DL = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90DL.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiq3mEjTuwsf"
      },
      "source": [
        "ValidationDataSet10DL[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRnG1WP8uwso"
      },
      "source": [
        "TrainDataSet90DL[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-cTcduFuwso"
      },
      "source": [
        "ValidationDataSet10DL.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USsLiZd_uwso"
      },
      "source": [
        "TrainDataSet90DL.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37sSPuawuwsp"
      },
      "source": [
        "ValidationDataSet10DL_data,ValidationDataSet10DL_label=np.split(ValidationDataSet10DL,[10],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dReBryHPuwsp"
      },
      "source": [
        "TrainDataSet90DL_data,TrainDataSet90DL_label=np.split(TrainDataSet90DL,[10],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JX9QYeluwsp"
      },
      "source": [
        "print (ValidationDataSet10DL_data.shape)\n",
        "print (ValidationDataSet10DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fVkl0N_uwsp"
      },
      "source": [
        "np.count_nonzero(ValidationDataSet10DL_label == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEX6zC2Luwsp"
      },
      "source": [
        "print (TrainDataSet90DL_data.shape)\n",
        "print (TrainDataSet90DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfK8Yq1SYMah"
      },
      "source": [
        "y = np.expand_dims(TrainDataSet90DL_data, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTTwGuwZYd9B"
      },
      "source": [
        "print (y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AddKNEA2YoXQ"
      },
      "source": [
        "print (y[74231])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niQTX_YSuwsq"
      },
      "source": [
        "ValidationDataSet10DL_data[0,2]\n",
        "print (ValidationDataSet10DL_data.max())\n",
        "print (ValidationDataSet10DL_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(ValidationDataSet10DL_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(ValidationDataSet10DL_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFkSGcFhuwsq"
      },
      "source": [
        "TrainDataSet90DL_data[0,2]\n",
        "print (TrainDataSet90DL_data.max())\n",
        "print (TrainDataSet90DL_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(TrainDataSet90DL_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(TrainDataSet90DL_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3n1DLZOuwsq"
      },
      "source": [
        "ValidationDataSet10DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5on8vsNuwsq"
      },
      "source": [
        "TrainDataSet90DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mip1CYmnuwsq"
      },
      "source": [
        "print (ValidationDataSet10DL_data.dtype)\n",
        "print (ValidationDataSet10DL_label.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_PXegzcuwsr"
      },
      "source": [
        "print (TrainDataSet90DL_data.dtype)\n",
        "print (TrainDataSet90DL_label.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W12zDLcu7C6"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylUYto5TvAwP"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UwO4ojdvAwP"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewPW9IgMvAwP"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics\n",
        "from talos.utils import best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQdH32SMvAwP"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls0gkCQKvAwQ"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAFS81i2vCw1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc0FFuWGvV5F"
      },
      "source": [
        "**2020-12-13 Using Nuew AutoModel Talos function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9PShzI43FYZ"
      },
      "source": [
        "class AutoModelJXHL:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2])                    \n",
        "                           )\n",
        "            )\n",
        "        \n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8tvCzi9mp1r"
      },
      "source": [
        "class AutoModelJXHL2:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'])\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auFB9WITzbdH"
      },
      "source": [
        "class AutoModelJXHL3:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'])\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A44brBUtGGn"
      },
      "source": [
        "autoParams2 = {'activation': ['elu'],\n",
        "               'batch_size': [32, 64, 128],\n",
        "               'dropout': [0.5],\n",
        "               'epochs': [10, 100, 1000],\n",
        "               'first_neuron': [10],\n",
        "               'hidden_layers': [0],\n",
        "               'kernel_initializer': ['normal'],\n",
        "               'last_activation': ['sigmoid'],\n",
        "               'losses': ['binary_crossentropy'],\n",
        "               'lr': [0.5],\n",
        "               'network': ['lstm'],\n",
        "               'optimizer': [Adam],\n",
        "               'shapes': ['brick'],\n",
        "               'weight_regulizer':[None],\n",
        "               'emb_output_dims': [None],\n",
        "               'shape':['brick']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "178QHnUQvJbe"
      },
      "source": [
        "autoParams2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LJXbDjYR-bJ"
      },
      "source": [
        "autoModel1= ta.autom8.AutoModel(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I76yzWQ4Ay2"
      },
      "source": [
        "autoModel1JXHL= AutoModelJXHL(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1JXHL').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS7dOJM8nhDa"
      },
      "source": [
        "autoModel2JXHL= AutoModelJXHL2(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1JXHL2').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW517fn90NDt"
      },
      "source": [
        "autoModel3JXHL= AutoModelJXHL3(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel3JXHL').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UvcefT4wBDK"
      },
      "source": [
        "print (ValidationDataSet10DL_data.shape)\n",
        "print (ValidationDataSet10DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvXgNCgswBDK"
      },
      "source": [
        "print (TrainDataSet90DL_data.shape)\n",
        "print (TrainDataSet90DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4J5B7VLwBDL"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90DL_data\n",
        "y = TrainDataSet90DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVQvwUnmSqmW"
      },
      "source": [
        "x = np.expand_dims(x, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tin4dwJRSzMr"
      },
      "source": [
        "print (x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFADRqabwBDL"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10DL_data\n",
        "y_val = ValidationDataSet10DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsDh2Il7TCDg"
      },
      "source": [
        "x_val = np.expand_dims(x_val, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgyuZx0CTFK-"
      },
      "source": [
        "print (x_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWyaUag_wBDM"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=autoModel3JXHL,\n",
        "            params=autoParams2,\n",
        "            experiment_name='autoModel3JXHL',\n",
        "            # fraction_limit=0.5,\n",
        "            # time_limit = \"2020-11-22 16:00\",\n",
        "            # round_limit=2,\n",
        "            print_params=True,\n",
        "            reduction_metric='val_f1score',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_f1score',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrgkaUozwBDM"
      },
      "source": [
        "print (t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZyMOYmiwBDN"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5ypr65qwBDN"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/MyDrive/Python/20200720_DataAnalysis/20201217_autoModel3JXHL_LSTM_DL_autoParams2.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPCEljUmwBDN"
      },
      "source": [
        "analyze_object=pd.read_pickle('/content/drive/My Drive/Python/20201122_DataAnalysis/20201109_multi_layer_percetron_FFNN_DL_pOpt.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHGOwz0twBDN"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20201217_autoModel3JXHL_LSTM_DL_autoParams2.xlsx', sheet_name='20201217_autoModel3JXHL_LSTM_DL', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9DWEWumwBDN"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jQgBTx3wS1I"
      },
      "source": [
        "### **Optmizatin Estimation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z5JVkOLwiBo"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XRIVqPjwiBp"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIsLTfluwiBp"
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDlzZIRKwiBp",
        "outputId": "ee99696b-d7ac-4cf2-e8e9-c2dd6ff7607e"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.6/dist-packages (2.7.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm6HvcA-wiBp"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI-SHYnOwiBp"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TEXjeCEwiBq"
      },
      "source": [
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtHrbd3TwiBq"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGBUTQpSwiBq"
      },
      "source": [
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IZuZpG4wlbS"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDbR1n37wp4d"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPdZDqjBwzwy"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10DL = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10DL.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfM7Ggzhwzwz"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90DL = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90DL.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V5D3XUwwzwz"
      },
      "source": [
        "ValidationDataSet10DL[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYvAtJkQwzw0"
      },
      "source": [
        "TrainDataSet90DL[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXZLAFeNwzw0"
      },
      "source": [
        "ValidationDataSet10DL.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNvoCea_wzw1"
      },
      "source": [
        "TrainDataSet90DL.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOUJTqB1wzw1"
      },
      "source": [
        "ValidationDataSet10DL_data,ValidationDataSet10DL_label=np.split(ValidationDataSet10DL,[10],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuRT6G8fwzw1"
      },
      "source": [
        "TrainDataSet90DL_data,TrainDataSet90DL_label=np.split(TrainDataSet90DL,[10],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sApfabTDwzw1"
      },
      "source": [
        "print (ValidationDataSet10DL_data.shape)\n",
        "print (ValidationDataSet10DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6SGHWwnwzw1"
      },
      "source": [
        "np.count_nonzero(ValidationDataSet10DL_label == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYRprA8jwzw1"
      },
      "source": [
        "print (TrainDataSet90DL_data.shape)\n",
        "print (TrainDataSet90DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVnYcfetwzw1"
      },
      "source": [
        "y = np.expand_dims(TrainDataSet90DL_data, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2MgUL2Fwzw2"
      },
      "source": [
        "print (y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmYJqswiwzw2"
      },
      "source": [
        "print (y[74231])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bJBrtkawzw2"
      },
      "source": [
        "ValidationDataSet10DL_data[0,2]\n",
        "print (ValidationDataSet10DL_data.max())\n",
        "print (ValidationDataSet10DL_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(ValidationDataSet10DL_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(ValidationDataSet10DL_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAV3W5Stwzw2"
      },
      "source": [
        "TrainDataSet90DL_data[0,2]\n",
        "print (TrainDataSet90DL_data.max())\n",
        "print (TrainDataSet90DL_data.argmax())\n",
        "\n",
        "max_index_col = np.argmax(TrainDataSet90DL_data, axis=0)\n",
        "print(max_index_col)\n",
        "\n",
        "max_index_row = np.argmax(TrainDataSet90DL_data, axis=1)\n",
        "\n",
        "print(max_index_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MsRodqFwzw2"
      },
      "source": [
        "ValidationDataSet10DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zg9KSMQwzw2"
      },
      "source": [
        "TrainDataSet90DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2i_Bie8wzw2"
      },
      "source": [
        "print (ValidationDataSet10DL_data.dtype)\n",
        "print (ValidationDataSet10DL_label.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2Zmri0owzw3"
      },
      "source": [
        "print (TrainDataSet90DL_data.dtype)\n",
        "print (TrainDataSet90DL_label.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns2ooDBZw9eM"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDWWhuqJxBF_"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdsHBMWMxBGA"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H9DHeL0xBGA"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics\n",
        "from talos.utils import best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxyNvvgLxBGA"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmVxztjPxBGA"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U3fvAUdxHza"
      },
      "source": [
        "class AutoModelJXHL:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2])                    \n",
        "                           )\n",
        "            )\n",
        "        \n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlvthnQzxHzb"
      },
      "source": [
        "class AutoModelJXHL2:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'])\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xCntPZOxHzb"
      },
      "source": [
        "class AutoModelJXHL3:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'])\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4IMU_amxg9E"
      },
      "source": [
        "# class to Tune Optimization Hyperparameters on LSTM 3 Layers + Dropout + Dense Output layer\n",
        "class AutoModelJXHL4:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True,\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           dropout = params['LSTM_dropout'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout']\n",
        "                           )\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],                    \n",
        "                           return_sequences=True,\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           dropout = params['LSTM_dropout'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout']\n",
        "                           )\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           dropout = params['LSTM_dropout'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout']\n",
        "                           )\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix_zEmAfxPOW"
      },
      "source": [
        "autoParams2 = {'activation': ['elu'],\n",
        "               'batch_size': [32, 64, 128],\n",
        "               'dropout': [0.5],\n",
        "               'epochs': [10, 100, 1000],\n",
        "               'first_neuron': [10],\n",
        "               'hidden_layers': [0],\n",
        "               'kernel_initializer': ['normal'],\n",
        "               'last_activation': ['sigmoid'],\n",
        "               'losses': ['binary_crossentropy'],\n",
        "               'lr': [0.5],\n",
        "               'network': ['lstm'],\n",
        "               'optimizer': [Adam],\n",
        "               'shapes': ['brick'],\n",
        "               'weight_regulizer':[None],\n",
        "               'emb_output_dims': [None],\n",
        "               'shape':['brick']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocfb7JQQ2iXL"
      },
      "source": [
        "autoParams4 = {'activation': ['elu'],\n",
        "               'batch_size': [32],\n",
        "               'dropout': [0.3333],\n",
        "               'epochs': [1000],\n",
        "               'first_neuron': [10],\n",
        "               'hidden_layers': [0],\n",
        "               'kernel_initializer': ['normal'],\n",
        "               'last_activation': ['sigmoid'],\n",
        "               'losses': ['binary_crossentropy'],\n",
        "               'lr': [0.5],\n",
        "               'network': ['lstm'],\n",
        "               'optimizer': [SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam],\n",
        "               'shapes': ['brick'],\n",
        "               'weight_regulizer':[None],\n",
        "               'emb_output_dims': [None],\n",
        "               'shape':['brick'],\n",
        "               'LSTM_activation': ['tanh'],\n",
        "               'LSTM_recurrent_activation': ['sigmoid'],\n",
        "               'LSTM_dropout': [0],\n",
        "               'LSTM_recurrent_dropout': [0]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxCz1XXdxPOX"
      },
      "source": [
        "autoParams4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Cnxid6y2AZ"
      },
      "source": [
        "autoModel1= ta.autom8.AutoModel(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsSOh73Qy2Aa"
      },
      "source": [
        "autoModel1JXHL= AutoModelJXHL(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1JXHL').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfSFEnrpy2Aa"
      },
      "source": [
        "autoModel2JXHL= AutoModelJXHL2(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1JXHL2').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXd4UkzRy2Aa"
      },
      "source": [
        "autoModel3JXHL= AutoModelJXHL3(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel3JXHL').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3T2UVKby2Aa"
      },
      "source": [
        "autoModel4JXHL= AutoModelJXHL4(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel4JXHL').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uc3Xc2Ly9w6"
      },
      "source": [
        "print (ValidationDataSet10DL_data.shape)\n",
        "print (ValidationDataSet10DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFVjX7v-y9w6"
      },
      "source": [
        "print (TrainDataSet90DL_data.shape)\n",
        "print (TrainDataSet90DL_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNfB2UG7y9w7"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90DL_data\n",
        "y = TrainDataSet90DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhpZUyl3y9w7"
      },
      "source": [
        "x = np.expand_dims(x, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sGadejfy9w8"
      },
      "source": [
        "print (x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEw2sVWby9w8"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10DL_data\n",
        "y_val = ValidationDataSet10DL_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdIps0tey9w8"
      },
      "source": [
        "x_val = np.expand_dims(x_val, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfPRXO-3y9w8"
      },
      "source": [
        "print (x_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmgXA5Sa-_am"
      },
      "source": [
        "# Python3 code to demonstrate  \n",
        "# Getting current date and time using   \n",
        "# now().  \n",
        "    \n",
        "# importing datetime module for now()  \n",
        "import datetime  \n",
        "    \n",
        "# using now() to get current time  \n",
        "current_time = datetime.datetime.now()  \n",
        "    \n",
        "# Printing value of now.  \n",
        "print (\"Time now at greenwich meridian is : \"\n",
        "                                    , end = \"\")  \n",
        "print (current_time) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWl9hwYzzJFg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HuchuCCzJXU"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=autoModel4JXHL,\n",
        "            params=autoParams4,\n",
        "            experiment_name='autoModel4JXHL',\n",
        "            # fraction_limit=0.5,\n",
        "            time_limit = \"2021-03-09 12:00\",\n",
        "            # round_limit=2,\n",
        "            print_params=True,\n",
        "            reduction_metric='val_f1score',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_f1score',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNU2bn10zJXV"
      },
      "source": [
        "print (t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7VKaPNJzJXV"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx8P5KM1zJXV"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/MyDrive/Python/20200720_DataAnalysis/20210305_autoModel4JXHL_Arch2_LSTM_DL_Final.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSe4o_yz5uWt"
      },
      "source": [
        "/content/drive/MyDrive/Python/20200720_DataAnalysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oggZExZhzJXW"
      },
      "source": [
        "analyze_object=pd.read_pickle('/content/drive/My Drive/Python/20201122_DataAnalysis/20201109_multi_layer_percetron_FFNN_DL_pOpt.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6akk9W3bzJXW"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20210305_autoModel4JXHL_Arch2_LSTM_DL_Final.xlsx', sheet_name='20210305_autoModel4JXHL_Arch', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjkIJr0hzJXW"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ018PA10D2i"
      },
      "source": [
        "## ***Hyperparameter Estimation LSTM - W2V_501***\n",
        "\n",
        "2021-01-25\n",
        "JUHO: LSTM Architecture\n",
        "Data Sets:\n",
        "Train: TrainDataSet90W2V_501\n",
        "Validation: ValidationDataSet10W2V_501\n",
        "Preprocessing Method: W2V_501 Data Set Matrixes\n",
        "Using TALOS Library\n",
        "Tests using AutoModel, AutoParams, AutoScan\n",
        "Introducing New LSTM Hyperparameters Estimation:\n",
        "LSTM_activation = tanh\n",
        "LSTM_recurrent_activation = sigmoid\n",
        "LSTM_recurrent_dropout = 0\n",
        "LSTM_dropout = 0.3333, 0.5, 0.8333\n",
        "Using New Data Transformation Array for Data (3D) with:\n",
        "Samples: Number of Data Records\n",
        "Timesteps: 2\n",
        "Features: 250\n",
        "Using Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WlkWzg80YLw"
      },
      "source": [
        "### **Architecture Estimation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_QdNtPeaV8I"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVQOs76Aaf1F"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpoM9uUm0gRF"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84w7h8c_0gRF"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x11Nvre0gRF"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-iMCooB0gRF"
      },
      "source": [
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66nJpnld0gRF"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7JAgqT30gRF"
      },
      "source": [
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANR-6mNf0kyf"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svrPHk3j0oXL"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ4CjZKh0umQ"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiVJz5ZN0umQ"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlNJAy8A0umQ"
      },
      "source": [
        "ValidationDataSet10W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcJQDU310umR"
      },
      "source": [
        "TrainDataSet90W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho_eaHOZ0umR"
      },
      "source": [
        "ValidationDataSet10W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OASj6K9U0umR"
      },
      "source": [
        "TrainDataSet90W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq9-geEE0umR"
      },
      "source": [
        "ValidationDataSet10W2V_501_data,ValidationDataSet10W2V_501_label=np.split(ValidationDataSet10W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAVjsdYu0umR"
      },
      "source": [
        "TrainDataSet90W2V_501_data,TrainDataSet90W2V_501_label=np.split(TrainDataSet90W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlakvpo00umS"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSgjhywV0umS"
      },
      "source": [
        "np.count_nonzero(ValidationDataSet10W2V_501_label == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Kb7Qmt0umS"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca7B_MKE0y3S"
      },
      "source": [
        "**New TrainDataSet90W2V_501_data and ValidationDataSet10W2V_501_data**\n",
        "Reshape Numpy Array to (len(array),2,250)\n",
        "Used for New Input for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BREfvX601TKW"
      },
      "source": [
        "TrainDataSet90W2V_501_data = np.reshape(TrainDataSet90W2V_501_data, (len(TrainDataSet90W2V_501_data),2,250))\n",
        "ValidationDataSet10W2V_501_data = np.reshape(ValidationDataSet10W2V_501_data, (len(ValidationDataSet10W2V_501_data),2,250)) \n",
        "\n",
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_rZCfpy1X6H"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyRP6nl_1eph"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETcJU_jB1eph"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrlVb5Cr1eph"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUtaDoB71epi"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiP8CVCY1epi"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pfo74761gbG"
      },
      "source": [
        "class AutoModelJXHL:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                           unroll = False,\n",
        "                           use_bias = True                    \n",
        "                           )\n",
        "            )\n",
        "        \n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QylLunYG1j0C"
      },
      "source": [
        "class AutoModelJXHL2:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                           unroll = False,\n",
        "                           use_bias = True,                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                           unroll = False,\n",
        "                           use_bias = True\n",
        "                           )\n",
        "            )\n",
        "\n",
        "        # model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpYnhK771m98"
      },
      "source": [
        "class AutoModelJXHL3:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'])\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8BEdyQM1sb3"
      },
      "source": [
        "autoParams2 = {'activation': ['relu'],\n",
        "               'LSTM_activation': ['tanh'],\n",
        "               'LSTM_recurrent_activation': ['sigmoid'],\n",
        "               'LSTM_recurrent_dropout': [0],\n",
        "               'batch_size': [128],\n",
        "               'dropout': [0],\n",
        "               'epochs': [100,1000],\n",
        "               'first_neuron': [1,10,20],\n",
        "               'hidden_layers': [0],\n",
        "               'kernel_initializer': ['normal'],\n",
        "               'last_activation': ['sigmoid'],\n",
        "               'losses': ['binary_crossentropy'],\n",
        "               'lr': [0.5],\n",
        "               'network': ['lstm'],\n",
        "               'optimizer': [Adam],\n",
        "               'shapes': ['brick'],\n",
        "               'weight_regulizer':[None],\n",
        "               'emb_output_dims': [None],\n",
        "               'shape':['brick']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI81FbhkwDv0"
      },
      "source": [
        "autoParams2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQiG_oAR1vmd"
      },
      "source": [
        "autoModel1= ta.autom8.AutoModel(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAPFAJMz1vme"
      },
      "source": [
        "autoModel1JXHL_W2V_501= AutoModelJXHL(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0QOTXy61vme"
      },
      "source": [
        "autoModel2JXHL_W2V_501= AutoModelJXHL2(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel2JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_BOrjU8buG_"
      },
      "source": [
        "autoModel3JXHL_W2V_501= AutoModelJXHL3(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel3JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFTTzAVP14LF"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdAXQjry14LG"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2q9FD3714LG"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90W2V_501_data\n",
        "y = TrainDataSet90W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_0CE49_waCn"
      },
      "source": [
        "print (x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDU0Sir414LH"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10W2V_501_data\n",
        "y_val = ValidationDataSet10W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpS_EIPZwcKr"
      },
      "source": [
        "print (x_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaNwkj3214LH"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=autoModel1JXHL_W2V_501,\n",
        "            params=autoParams2,\n",
        "            experiment_name='autoModel1JXHL_W2V_501',\n",
        "            # fraction_limit=0.5,\n",
        "            # time_limit = \"2020-11-22 16:00\",\n",
        "            # round_limit=2,\n",
        "            print_params=True,\n",
        "            reduction_metric='val_f1score',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_f1score',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMtiDZAN14LI"
      },
      "source": [
        "print (t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD0DRdp214LJ"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCNRJMeZ14LJ"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/20210131_autoModel1JXHL_W2V_501_LSTM_autoParams2_v3.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbkhQBOL14LJ"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20210131_autoModel1JXHL_W2V_501_LSTM_autoParams2_v3.xlsx', sheet_name='20210131_autoModel1JXHL', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2eeiEK714LJ"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfI7qxc-3GMS"
      },
      "source": [
        "### **Optimization Estimation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYzKYXHk3PAk"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srbUUT9O3PAl"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwlCT4Nt3PAl"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG4vSxTj3PAl"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7kIflgs3PAl"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjqG1Oqt3PAm"
      },
      "source": [
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tpo4aWy3PAm"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8HQIYRa3PAm"
      },
      "source": [
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L042vxN3TWb"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zv-Dsjt3W77"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C7B9mK43ZZz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_7vrznu3dn9"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ADPGqXW3dn9"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAoTpkjL3dn9",
        "outputId": "4df996ee-b201-421c-b2ea-f73428e81c4c"
      },
      "source": [
        "ValidationDataSet10W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0sHR-cR3doG",
        "outputId": "9e6d5d6a-24ff-4170-ca06-2e4b80088bc6"
      },
      "source": [
        "TrainDataSet90W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IM1o2b_3doG",
        "outputId": "7cf4597b-2cd1-4c5d-c668-735e5e251ba3"
      },
      "source": [
        "ValidationDataSet10W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8242, 501)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3kufcyR3doG",
        "outputId": "392882ee-9de9-4c78-b39b-3ac8ddd5d133"
      },
      "source": [
        "TrainDataSet90W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(74232, 501)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWO7ISgW3doH"
      },
      "source": [
        "ValidationDataSet10W2V_501_data,ValidationDataSet10W2V_501_label=np.split(ValidationDataSet10W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw-1y-2p3doH"
      },
      "source": [
        "TrainDataSet90W2V_501_data,TrainDataSet90W2V_501_label=np.split(TrainDataSet90W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF4JKofQ3doH",
        "outputId": "5dd8acfd-3940-4147-da58-5940882ebec0"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8242, 500)\n",
            "(8242, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvNP1vR43doH",
        "outputId": "ef568366-4d5e-49cd-bb91-ba290ea01728"
      },
      "source": [
        "print(np.count_nonzero(ValidationDataSet10W2V_501_label == 0))\n",
        "print(np.count_nonzero(ValidationDataSet10W2V_501_label == 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5979\n",
            "2263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qUpi8tA3doH",
        "outputId": "5677789c-6cb3-4a4e-bf21-19f17a1ad0ee"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(74232, 500)\n",
            "(74232, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdfB1IKW3fSO"
      },
      "source": [
        "**New TrainDataSet90W2V_501_data and ValidationDataSet10W2V_501_data **\n",
        "Reshape Numpy Array to (len(array),2,250)\n",
        "Used for New Input for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoeRl1cK3k9S"
      },
      "source": [
        "TrainDataSet90W2V_501_data = np.reshape(TrainDataSet90W2V_501_data, (len(TrainDataSet90W2V_501_data),2,250))\n",
        "ValidationDataSet10W2V_501_data = np.reshape(ValidationDataSet10W2V_501_data, (len(ValidationDataSet10W2V_501_data),2,250)) \n",
        "\n",
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP7Cg30D3sct"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkDjIAgO3xII"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcfIlJHb3xII"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjVOs6Ya3xII"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B81v4O3v3xIJ"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD6elAXX3xIJ"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acIsKt0A3zLt"
      },
      "source": [
        "class AutoModelJXHL:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                           unroll = False,\n",
        "                           use_bias = True                    \n",
        "                           )\n",
        "            )\n",
        "        \n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR4yDY2T3217"
      },
      "source": [
        "class AutoModelJXHL2:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                           dropout = params['LSTM_dropout'],\n",
        "                           unroll = False,\n",
        "                           use_bias = True,                    \n",
        "                           return_sequences=True\n",
        "                           )\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                           dropout = params['LSTM_dropout'],\n",
        "                           unroll = False,\n",
        "                           use_bias = True\n",
        "                           )\n",
        "            )\n",
        "\n",
        "        # model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        # from talos.model.hidden_layers import hidden_layers\n",
        "        # hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZKaXKdY37OO"
      },
      "source": [
        "class AutoModelJXHL3:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'])\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LZef-PM4COv"
      },
      "source": [
        "autoParams2 = {'activation': ['relu'],\n",
        "               'LSTM_activation': ['relu'],\n",
        "               'LSTM_recurrent_activation': ['sigmoid'],\n",
        "               'LSTM_recurrent_dropout': [0.5],\n",
        "               'LSTM_dropout': [0],\n",
        "               'batch_size': [32],\n",
        "               'dropout': [0],\n",
        "               'epochs': [2000],\n",
        "               'first_neuron': [512],\n",
        "               'hidden_layers': [0],\n",
        "               'kernel_initializer': ['normal'],\n",
        "               'last_activation': ['sigmoid'],\n",
        "               'losses': ['binary_crossentropy'],\n",
        "               'lr': [0.5],\n",
        "               'network': ['lstm'],\n",
        "               'optimizer': [Adam],\n",
        "               'shapes': ['brick'],\n",
        "               'weight_regulizer':[None],\n",
        "               'emb_output_dims': [None],\n",
        "               'shape':['brick']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5f-RX5s4COv"
      },
      "source": [
        "autoParams2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCeiHE9l4G7D"
      },
      "source": [
        "autoModel1= ta.autom8.AutoModel(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_MTWKbY4G7E"
      },
      "source": [
        "autoModel1JXHL_W2V_501= AutoModelJXHL(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDKiXzsA4G7E"
      },
      "source": [
        "autoModel2JXHL_W2V_501= AutoModelJXHL2(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel2JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2zyeuJY4G7E"
      },
      "source": [
        "autoModel3JXHL_W2V_501= AutoModelJXHL3(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel3JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmQ22SsU4IW-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1hUAQkO4OL5",
        "outputId": "a0a8419f-6804-4710-be6a-e2b8acacfc94"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8242, 2, 250)\n",
            "(8242, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04jc06QH4OL6",
        "outputId": "fc739c66-928c-43d9-f6ed-227a7b27cbfe"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(74232, 2, 250)\n",
            "(74232, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TANMdNpm4OL6"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90W2V_501_data\n",
        "y = TrainDataSet90W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ustT65J4OMB",
        "outputId": "4bc6d65c-c3e1-4868-ff72-f2a345f53dbc"
      },
      "source": [
        "print (x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(74232, 2, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4I3Ym1C4OMC"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10W2V_501_data\n",
        "y_val = ValidationDataSet10W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tENfSfF4OMC"
      },
      "source": [
        "print (x_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU8BF3gI4OME"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=autoModel2JXHL_W2V_501,\n",
        "            params=autoParams2,\n",
        "            experiment_name='autoModel2JXHL_W2V_501',\n",
        "            # fraction_limit=0.5,\n",
        "            time_limit = \"2021-04-04 06:00\",\n",
        "            # round_limit=2,\n",
        "            print_params=True,\n",
        "            reduction_metric='val_f1score',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_f1score',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmtujIpg4OMU"
      },
      "source": [
        "print (t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIBkJ-9O4OMU"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzTyDmjX4OMX"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/20210401_autoModel2JXHL_W2V_501_LSTM_Opt_autoParams2_v1.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPVZG6LJ4OMd"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20210401_autoModel2JXHL_W2V_501_LSTM_Opt_autoParams2_v1.xlsx', sheet_name='20210401_W2V_LSTM_Opt', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsqiSodQ4OMe"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15FfYBEv5aMO"
      },
      "source": [
        "### ***Hyperparameter Estimation Adding New Dense Layer***\n",
        "2021-04-11\n",
        "JUHO: LSTM Architecture\n",
        "Data Sets:\n",
        "Train: TrainDataSet90W2V_501\n",
        "Validation: ValidationDataSet10W2V_501\n",
        "Preprocessing Method: W2V_501 Data Set Matrixes\n",
        "Using TALOS Library\n",
        "Tests using AutoModel, AutoParams, AutoScan\n",
        "Introducing New LSTM Hyperparameters Estimation:\n",
        "LSTM_activation = tanh\n",
        "LSTM_recurrent_activation = sigmoid\n",
        "LSTM_recurrent_dropout = 0\n",
        "LSTM_dropout = 0.3333, 0.5, 0.8333\n",
        "Using New Data Transformation Array for Data (3D) with:\n",
        "Samples: Number of Data Records\n",
        "Timesteps: 2\n",
        "Features: 250\n",
        "Adding a New LSTM Layer automodelJXHL2\n",
        "Adding a New Dense Layer before output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyxVOLdP5sBC"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uS_rTiJ5sBD"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awdmeKk55sBD"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuE3D7oa5sBD"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKhAi5JR5sBE"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShL6I14j5sBE"
      },
      "source": [
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSKPHn715sBE"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ThD4nT5sBE"
      },
      "source": [
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ririyetj5xIs"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yByc2_ee50ae"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sYbVz0c562C"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1NtgN7T562D"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGmlpAA2562D"
      },
      "source": [
        "ValidationDataSet10W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzu0raYf562E"
      },
      "source": [
        "TrainDataSet90W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OrOAAHD562E"
      },
      "source": [
        "ValidationDataSet10W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpXtbB8f562E"
      },
      "source": [
        "TrainDataSet90W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ajB3u3562E"
      },
      "source": [
        "ValidationDataSet10W2V_501_data,ValidationDataSet10W2V_501_label=np.split(ValidationDataSet10W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPe26trS562E"
      },
      "source": [
        "TrainDataSet90W2V_501_data,TrainDataSet90W2V_501_label=np.split(TrainDataSet90W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6NKrN7Z562E"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlypeWp1562F"
      },
      "source": [
        "print(np.count_nonzero(ValidationDataSet10W2V_501_label == 0))\n",
        "print(np.count_nonzero(ValidationDataSet10W2V_501_label == 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blGmYL3k562F"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjdyvaa45_j-"
      },
      "source": [
        "**# New TrainDataSet90W2V_501_data and ValidationDataSet10W2V_501_data**\n",
        "Reshape Numpy Array to (len(array),2,250)\n",
        "Used for New Input for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZERhZbkF6Gh7"
      },
      "source": [
        "TrainDataSet90W2V_501_data = np.reshape(TrainDataSet90W2V_501_data, (len(TrainDataSet90W2V_501_data),2,250))\n",
        "ValidationDataSet10W2V_501_data = np.reshape(ValidationDataSet10W2V_501_data, (len(ValidationDataSet10W2V_501_data),2,250)) \n",
        "\n",
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BTwHLhdjkRB"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEsl91wk6MYA"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqQP5xyl6MYA"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ViOg2R56MYB"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80mBzLWN6MYB"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKHkiJ3V6MYB"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eItDtmz6UFJ"
      },
      "source": [
        "class AutoModelJXHL:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                           unroll = False,\n",
        "                           use_bias = True                    \n",
        "                           )\n",
        "            )\n",
        "        \n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH5VSmNr6UFK"
      },
      "source": [
        "class AutoModelJXHL2:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                           dropout = params['LSTM_dropout'],\n",
        "                           unroll = False,\n",
        "                           use_bias = True,                    \n",
        "                           return_sequences=True\n",
        "                           )\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           activation = params['LSTM_activation'],\n",
        "                           recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                           recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                           dropout = params['LSTM_dropout'],\n",
        "                           unroll = False,\n",
        "                           use_bias = True\n",
        "                           )\n",
        "            )\n",
        "\n",
        "        # New Dense Layer Proposed by Teacher Gloria Alvarez\n",
        "        \n",
        "        model.add(Dense(((params['first_neuron'])/2),\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        # from talos.model.hidden_layers import hidden_layers\n",
        "        # hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCfFRD0h6UFL"
      },
      "source": [
        "class AutoModelJXHL3:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'])\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0WX7gpi6UFL"
      },
      "source": [
        "autoParams2 = {'activation': ['relu'],\n",
        "               'LSTM_activation': ['relu'],\n",
        "               'LSTM_recurrent_activation': ['sigmoid'],\n",
        "               'LSTM_recurrent_dropout': [0.5],\n",
        "               'LSTM_dropout': [0],\n",
        "               'batch_size': [32],\n",
        "               'dropout': [0],\n",
        "               'epochs': [1000],\n",
        "               'first_neuron': [512],\n",
        "               'hidden_layers': [0],\n",
        "               'kernel_initializer': ['normal'],\n",
        "               'last_activation': ['sigmoid'],\n",
        "               'losses': ['binary_crossentropy'],\n",
        "               'lr': [0.5],\n",
        "               'network': ['lstm'],\n",
        "               'optimizer': [Adam],\n",
        "               'shapes': ['brick'],\n",
        "               'weight_regulizer':[None],\n",
        "               'emb_output_dims': [None],\n",
        "               'shape':['brick']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujP3Q8OL6UFL"
      },
      "source": [
        "autoParams2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMzOlNci6UFL"
      },
      "source": [
        "autoModel1= ta.autom8.AutoModel(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne3_4R5L6UFL"
      },
      "source": [
        "autoModel1JXHL_W2V_501= AutoModelJXHL(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG7xRt0F6UFL"
      },
      "source": [
        "autoModel2JXHL_W2V_501= AutoModelJXHL2(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel2JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BVvuvn46UFM"
      },
      "source": [
        "autoModel3JXHL_W2V_501= AutoModelJXHL3(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel3JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PcUBN2d6hvE"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OP06JeT6hvN"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNUd-vxW6hvO"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90W2V_501_data\n",
        "y = TrainDataSet90W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82TEm7_n6hvO"
      },
      "source": [
        "print (x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrkVgySl6hvO"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10W2V_501_data\n",
        "y_val = ValidationDataSet10W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzsLjViG6hvP"
      },
      "source": [
        "print (x_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0jUNBrj6hvP"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=autoModel2JXHL_W2V_501,\n",
        "            params=autoParams2,\n",
        "            experiment_name='autoModel2JXHL_W2V_501',\n",
        "            # fraction_limit=0.5,\n",
        "            time_limit = \"2021-04-12 19:00\",\n",
        "            # round_limit=2,\n",
        "            print_params=True,\n",
        "            reduction_metric='val_f1score',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_f1score',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRl0HoX6bQ2n"
      },
      "source": [
        "t.best_model(metric='val_f1score', asc=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENIoOLXH6hvQ"
      },
      "source": [
        "print (t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXivllOr6hvQ"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IIP8ZDi6hvQ"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/20210412_autoModel2JXHL_W2V_501_LSTM_Opt_autoParams2_v10.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9PToE_A6hvQ"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20210412_autoModel2JXHL_W2V_501_LSTM_Opt_autoParams2_v1.xlsx', sheet_name='20210412_W2V_LSTM_Opt', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x28HIXYk6hvR"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRWrPV677rXR"
      },
      "source": [
        "## ***Hyperparameter Estimation BiLSTM - W2V_501***\n",
        "2021-02-02\n",
        "JUHO: LSTM Biderectional Architecture 2 Layers\n",
        "Data Sets:\n",
        "Train: TrainDataSet90W2V_501\n",
        "Validation: ValidationDataSet10W2V_501\n",
        "Preprocessing Method: W2V_501 Data Set Matrixes\n",
        "Using TALOS Library\n",
        "Tests using AutoModel, AutoParams, AutoScan\n",
        "Introducing New LSTM Hyperparameters Estimation:\n",
        "LSTM_activation = tanh\n",
        "LSTM_recurrent_activation = sigmoid\n",
        "LSTM_recurrent_dropout = 0\n",
        "LSTM_dropout = 0.3333, 0.5, 0.8333\n",
        "Using New Data Transformation Array for Data (3D) with:\n",
        "Samples: Number of Data Records\n",
        "Timesteps: 2\n",
        "Features: 250\n",
        "Using Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch6VkTg88tTH"
      },
      "source": [
        "### **Architecture Estimation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ySzPLqJ83qm"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWOefFwP83qn"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NfjCBCa83qn"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkFwTKf283qo"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ukN8E583qo"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZT07WVy83qp"
      },
      "source": [
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PovITDXD83qp"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61v4UVU_83qq"
      },
      "source": [
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1n7TEeu88H6"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq6YVuS68_t1"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irc9Kl1k9IrQ"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85Bww4Nu9IrQ"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL3zU9oI9IrQ"
      },
      "source": [
        "ValidationDataSet10W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_UWyqQ89IrR"
      },
      "source": [
        "TrainDataSet90W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDnNJF3w9IrR"
      },
      "source": [
        "ValidationDataSet10W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAfP8HLM9IrR"
      },
      "source": [
        "TrainDataSet90W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cgV1xQH9IrS"
      },
      "source": [
        "ValidationDataSet10W2V_501_data,ValidationDataSet10W2V_501_label=np.split(ValidationDataSet10W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc5MSiNt9IrS"
      },
      "source": [
        "TrainDataSet90W2V_501_data,TrainDataSet90W2V_501_label=np.split(TrainDataSet90W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsYwzKAO9IrS"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JRq58Hw9IrS"
      },
      "source": [
        "print(np.count_nonzero(ValidationDataSet10W2V_501_label == 0))\n",
        "print(np.count_nonzero(ValidationDataSet10W2V_501_label == 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5eogNO39IrT"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mMmc3tF9QCV"
      },
      "source": [
        "** New TrainDataSet90W2V_501_data and ValidationDataSet10W2V_501_data**\n",
        "Reshape Numpy Array to (len(array),2,250)\n",
        "Used for New Input for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdFXCsB79NZ-"
      },
      "source": [
        "TrainDataSet90W2V_501_data = np.reshape(TrainDataSet90W2V_501_data, (len(TrainDataSet90W2V_501_data),2,250))\n",
        "ValidationDataSet10W2V_501_data = np.reshape(ValidationDataSet10W2V_501_data, (len(ValidationDataSet10W2V_501_data),2,250)) \n",
        "\n",
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dn5K4WQ9aPt"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LMqAtbb9aPu"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vEEOzcJ9aPu"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking, Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMALqYP39aPu"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-sXSJx19aPu"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTRLX50d9aPu"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo8ss7kZ9g-h"
      },
      "source": [
        "class AutoModelJXHL:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(Bidirectional(LSTM(params['first_neuron'],\n",
        "                                         activation = params['LSTM_activation'],\n",
        "                                         recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                                         recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                                         dropout = params['LSTM_dropout'],\n",
        "                                         unroll = False,\n",
        "                                         use_bias = True\n",
        "                                         ),\n",
        "                      input_shape =(x_train.shape[1],x_train.shape[2])\n",
        "                      )\n",
        "            )\n",
        "        \n",
        "        # model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # # add hidden layers to the model\n",
        "        # from talos.model.hidden_layers import hidden_layers\n",
        "        # hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1kFvXhO9g-i"
      },
      "source": [
        "class AutoModelJXHL2:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(Bidirectional(LSTM(params['first_neuron'],\n",
        "                                         activation = params['LSTM_activation'],\n",
        "                                         recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                                         recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                                         dropout = params['LSTM_dropout'],\n",
        "                                         unroll = False,\n",
        "                                         use_bias = True,\n",
        "                                         return_sequences=True\n",
        "                                         ),\n",
        "                      input_shape =(x_train.shape[1],x_train.shape[2])\n",
        "                      )\n",
        "            )\n",
        "            model.add(Bidirectional(LSTM(params['first_neuron'],\n",
        "                                         activation = params['LSTM_activation'],\n",
        "                                         recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                                         recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                                         dropout = params['LSTM_dropout'],\n",
        "                                         unroll = False,\n",
        "                                         use_bias = True\n",
        "                                         )\n",
        "                      )\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qOt_uQ89g-i"
      },
      "source": [
        "class AutoModelJXHL3:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'])\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2PPRZcv9g-j"
      },
      "source": [
        "autoParams2 = {'activation': ['relu'],\n",
        "               'LSTM_activation': ['tanh'],\n",
        "               'LSTM_recurrent_activation': ['sigmoid'],\n",
        "               'LSTM_recurrent_dropout': [0],\n",
        "               'LSTM_dropout': [0],\n",
        "               'batch_size': [32,64,128],\n",
        "               'dropout': [0],\n",
        "               'epochs': [10,100,1000],\n",
        "               'first_neuron': [1024],\n",
        "               'hidden_layers': [0],\n",
        "               'kernel_initializer': ['normal'],\n",
        "               'last_activation': ['sigmoid'],\n",
        "               'losses': ['binary_crossentropy'],\n",
        "               'lr': [0.5],\n",
        "               'network': ['lstm'],\n",
        "               'optimizer': [Adam],\n",
        "               'shapes': ['brick'],\n",
        "               'weight_regulizer':[None],\n",
        "               'emb_output_dims': [None],\n",
        "               'shape':['brick']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbDVw7A59g-j"
      },
      "source": [
        "autoParams2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfuisBZE9g-k"
      },
      "source": [
        "autoModel1= ta.autom8.AutoModel(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGuuglRS9g-k"
      },
      "source": [
        "autoModel1JXHL_W2V_501= AutoModelJXHL(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toyaAfBf9g-k"
      },
      "source": [
        "autoModel2JXHL_W2V_501= AutoModelJXHL2(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel2JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e8v9S9g9g-k"
      },
      "source": [
        "autoModel3JXHL_W2V_501= AutoModelJXHL3(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel3JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUK003Oh9u0M"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPlDAHdk9u0M"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa_aaEbG9u0M"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90W2V_501_data\n",
        "y = TrainDataSet90W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7EnotCJ9u0M"
      },
      "source": [
        "print (x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTlEFITx9u0N"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10W2V_501_data\n",
        "y_val = ValidationDataSet10W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TtpBakg9u0N"
      },
      "source": [
        "print (x_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpGky7qv9u0N",
        "outputId": "7015c473-00bf-414c-cd88-5e6e46cc451b"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=autoModel2JXHL_W2V_501,\n",
        "            params=autoParams2,\n",
        "            experiment_name='autoModel2JXHL_W2V_501',\n",
        "            # fraction_limit=0.5,\n",
        "            # time_limit = \"2020-11-22 16:00\",\n",
        "            # round_limit=2,\n",
        "            print_params=True,\n",
        "            reduction_metric='val_f1score',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_f1score',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'LSTM_activation': 'tanh', 'LSTM_dropout': 0, 'LSTM_recurrent_activation': 'sigmoid', 'LSTM_recurrent_dropout': 0, 'activation': 'relu', 'batch_size': 32, 'dropout': 0, 'emb_output_dims': None, 'epochs': 10, 'first_neuron': 1024, 'hidden_layers': 0, 'kernel_initializer': 'normal', 'last_activation': 'sigmoid', 'losses': 'binary_crossentropy', 'lr': 0.5, 'network': 'lstm', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'shape': 'brick', 'shapes': 'brick', 'weight_regulizer': None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 1/9 [04:40<37:20, 280.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 2, 2048)           10444800  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 35,620,865\n",
            "Trainable params: 35,620,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "{'LSTM_activation': 'tanh', 'LSTM_dropout': 0, 'LSTM_recurrent_activation': 'sigmoid', 'LSTM_recurrent_dropout': 0, 'activation': 'relu', 'batch_size': 32, 'dropout': 0, 'emb_output_dims': None, 'epochs': 100, 'first_neuron': 1024, 'hidden_layers': 0, 'kernel_initializer': 'normal', 'last_activation': 'sigmoid', 'losses': 'binary_crossentropy', 'lr': 0.5, 'network': 'lstm', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'shape': 'brick', 'shapes': 'brick', 'weight_regulizer': None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 2/9 [50:01<1:58:07, 1012.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_2 (Bidirection (None, 2, 2048)           10444800  \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 35,620,865\n",
            "Trainable params: 35,620,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "{'LSTM_activation': 'tanh', 'LSTM_dropout': 0, 'LSTM_recurrent_activation': 'sigmoid', 'LSTM_recurrent_dropout': 0, 'activation': 'relu', 'batch_size': 32, 'dropout': 0, 'emb_output_dims': None, 'epochs': 1000, 'first_neuron': 1024, 'hidden_layers': 0, 'kernel_initializer': 'normal', 'last_activation': 'sigmoid', 'losses': 'binary_crossentropy', 'lr': 0.5, 'network': 'lstm', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'shape': 'brick', 'shapes': 'brick', 'weight_regulizer': None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 3/9 [8:20:25<14:41:34, 8815.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_4 (Bidirection (None, 2, 2048)           10444800  \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 35,620,865\n",
            "Trainable params: 35,620,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "{'LSTM_activation': 'tanh', 'LSTM_dropout': 0, 'LSTM_recurrent_activation': 'sigmoid', 'LSTM_recurrent_dropout': 0, 'activation': 'relu', 'batch_size': 64, 'dropout': 0, 'emb_output_dims': None, 'epochs': 10, 'first_neuron': 1024, 'hidden_layers': 0, 'kernel_initializer': 'normal', 'last_activation': 'sigmoid', 'losses': 'binary_crossentropy', 'lr': 0.5, 'network': 'lstm', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'shape': 'brick', 'shapes': 'brick', 'weight_regulizer': None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 4/9 [8:23:04<8:38:14, 6218.86s/it] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_6 (Bidirection (None, 2, 2048)           10444800  \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 35,620,865\n",
            "Trainable params: 35,620,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "{'LSTM_activation': 'tanh', 'LSTM_dropout': 0, 'LSTM_recurrent_activation': 'sigmoid', 'LSTM_recurrent_dropout': 0, 'activation': 'relu', 'batch_size': 64, 'dropout': 0, 'emb_output_dims': None, 'epochs': 100, 'first_neuron': 1024, 'hidden_layers': 0, 'kernel_initializer': 'normal', 'last_activation': 'sigmoid', 'losses': 'binary_crossentropy', 'lr': 0.5, 'network': 'lstm', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'shape': 'brick', 'shapes': 'brick', 'weight_regulizer': None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 5/9 [8:47:39<5:19:42, 4795.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_8 (Bidirection (None, 2, 2048)           10444800  \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 35,620,865\n",
            "Trainable params: 35,620,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "{'LSTM_activation': 'tanh', 'LSTM_dropout': 0, 'LSTM_recurrent_activation': 'sigmoid', 'LSTM_recurrent_dropout': 0, 'activation': 'relu', 'batch_size': 64, 'dropout': 0, 'emb_output_dims': None, 'epochs': 1000, 'first_neuron': 1024, 'hidden_layers': 0, 'kernel_initializer': 'normal', 'last_activation': 'sigmoid', 'losses': 'binary_crossentropy', 'lr': 0.5, 'network': 'lstm', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'shape': 'brick', 'shapes': 'brick', 'weight_regulizer': None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 6/9 [12:53:06<6:28:45, 7775.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_10 (Bidirectio (None, 2, 2048)           10444800  \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 35,620,865\n",
            "Trainable params: 35,620,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "{'LSTM_activation': 'tanh', 'LSTM_dropout': 0, 'LSTM_recurrent_activation': 'sigmoid', 'LSTM_recurrent_dropout': 0, 'activation': 'relu', 'batch_size': 128, 'dropout': 0, 'emb_output_dims': None, 'epochs': 10, 'first_neuron': 1024, 'hidden_layers': 0, 'kernel_initializer': 'normal', 'last_activation': 'sigmoid', 'losses': 'binary_crossentropy', 'lr': 0.5, 'network': 'lstm', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'shape': 'brick', 'shapes': 'brick', 'weight_regulizer': None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 7/9 [12:54:46<3:02:25, 5472.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_12 (Bidirectio (None, 2, 2048)           10444800  \n",
            "_________________________________________________________________\n",
            "bidirectional_13 (Bidirectio (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 35,620,865\n",
            "Trainable params: 35,620,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "{'LSTM_activation': 'tanh', 'LSTM_dropout': 0, 'LSTM_recurrent_activation': 'sigmoid', 'LSTM_recurrent_dropout': 0, 'activation': 'relu', 'batch_size': 128, 'dropout': 0, 'emb_output_dims': None, 'epochs': 100, 'first_neuron': 1024, 'hidden_layers': 0, 'kernel_initializer': 'normal', 'last_activation': 'sigmoid', 'losses': 'binary_crossentropy', 'lr': 0.5, 'network': 'lstm', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'shape': 'brick', 'shapes': 'brick', 'weight_regulizer': None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 89%|████████▉ | 8/9 [13:09:33<1:08:16, 4096.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_14 (Bidirectio (None, 2, 2048)           10444800  \n",
            "_________________________________________________________________\n",
            "bidirectional_15 (Bidirectio (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 35,620,865\n",
            "Trainable params: 35,620,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "{'LSTM_activation': 'tanh', 'LSTM_dropout': 0, 'LSTM_recurrent_activation': 'sigmoid', 'LSTM_recurrent_dropout': 0, 'activation': 'relu', 'batch_size': 128, 'dropout': 0, 'emb_output_dims': None, 'epochs': 1000, 'first_neuron': 1024, 'hidden_layers': 0, 'kernel_initializer': 'normal', 'last_activation': 'sigmoid', 'losses': 'binary_crossentropy', 'lr': 0.5, 'network': 'lstm', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'shape': 'brick', 'shapes': 'brick', 'weight_regulizer': None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [15:35:58<00:00, 6239.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_16 (Bidirectio (None, 2, 2048)           10444800  \n",
            "_________________________________________________________________\n",
            "bidirectional_17 (Bidirectio (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 35,620,865\n",
            "Trainable params: 35,620,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8PuTULa9u0N"
      },
      "source": [
        "print (t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYL8GPwK9u0O"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtbcRiJS9u0O"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/20210212_autoModel2JXHL_W2V_501_BiLSTM_autoParams2_v6.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsFblof09u0O"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20210212_autoModel2JXHL_W2V_501_BiLSTM_autoParams2_v6.xlsx', sheet_name='20210212_autoModel2JXHL', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYoKvFsM9u0O"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGPYvUUKBRp3"
      },
      "source": [
        "###**Optimization Estimation**\n",
        "2021-04-11\n",
        "JUHO: LSTM Biderectional Architecture 2 Layers\n",
        "Data Sets:\n",
        "Train: TrainDataSet90W2V_501\n",
        "Validation: ValidationDataSet10W2V_501\n",
        "Preprocessing Method: W2V_501 Data Set Matrixes\n",
        "Using TALOS Library\n",
        "Tests using AutoModel, AutoParams, AutoScan\n",
        "Introducing New LSTM Hyperparameters Estimation:\n",
        "LSTM_activation = tanh\n",
        "LSTM_recurrent_activation = sigmoid\n",
        "LSTM_recurrent_dropout = 0\n",
        "LSTM_dropout = 0.3333, 0.5, 0.8333\n",
        "Using New Data Transformation Array for Data (3D) with:\n",
        "Samples: Number of Data Records\n",
        "Timesteps: 2\n",
        "Features: 250\n",
        "Using Embedding Layer\n",
        "Adding new Dense Layer before Output Layer Proposed By Teacher Gloria Alvarez"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osQ8bAcDsi2G"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuqZYRKNsi2H"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojzhzslbsi2I"
      },
      "source": [
        "# 0.1 - Import Python Libraries\n",
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptuayNXZsi2I"
      },
      "source": [
        "# 1- Import Python Libraries\n",
        "import numpy as np\n",
        "import gensim as gs\n",
        "import re as re\n",
        "import string as st\n",
        "import codecs as co\n",
        "import glob as gl\n",
        "import logging as logger\n",
        "import multiprocessing as mult\n",
        "import os as os\n",
        "import pprint as pp\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import whoosh as ws\n",
        "import random as ra\n",
        "import csv as csv\n",
        "import scipy as sc\n",
        "import json as js\n",
        "import itertools "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDxWskOOsi2I"
      },
      "source": [
        "# 2- Import Python Functions\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.support.levenshtein import levenshtein\n",
        "from whoosh.support.levenshtein import damerau_levenshtein\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kILaJx3rsi2I"
      },
      "source": [
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m8BZiWWsi2J"
      },
      "source": [
        "# 3- Import Python tensorflow library\n",
        "import tensorflow as tf\n",
        "print(tf.version)  # make sure the version is 2.x\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow.compat.v2.feature_column as fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjKye8fmsi2J"
      },
      "source": [
        "# 3.1- Import Tensorflow Python Functions for DNN, CNN, RNN. Keras\n",
        "from tensorflow import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHNoAoT4si2J",
        "outputId": "446bde4d-2fab-4073-ab48-9b25018c3d8f"
      },
      "source": [
        "# 3.2 - Importing drive from google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fENg4HGRsi2K"
      },
      "source": [
        "# 3- JXHALLO Personal Functions for Code\n",
        "\n",
        "# String Normalizer\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns normalized string without spaces and special characters using RegEx. Then sets lower case\n",
        "\n",
        "def norm (x):\n",
        "    return re.sub(r\"\\s|[^a-z0-9]\",\"\",x.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "# String Exact Comparison\n",
        "# Version 1.0: 2020-04-10\n",
        "# Returns boolean if string comparison is exact.\n",
        "\n",
        "def exact(a,b):\n",
        "    if a is b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record with the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.2: 2020-04-17\n",
        "\n",
        "def multiObjects(values,key):\n",
        "    val = None\n",
        "    for i in range(len(values)):\n",
        "        data = values[i]\n",
        "        for (k, v) in data.items():\n",
        "            if k == key:\n",
        "                val = list(v.split(\", \"))[0].replace(\"[\",\"\").replace(\"]\",\"\").strip()\n",
        "    return val\n",
        "    \n",
        "# Function to access a specific value within an object list\n",
        "# Returns the value of the record of the corresponding key.\n",
        "# Arguments: object that contain the list of keys and values, the key that you want to match to return the value\n",
        "# Special Note: This function will only return the first value of the list.\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "def identifiersList (column,key):\n",
        "    lineLst = []\n",
        "    for i in range (len(column)):\n",
        "        mpn = multiObjects(column[i],key)\n",
        "        lineLst.append(mpn)\n",
        "    return lineLst\n",
        "    \n",
        "# Damerau–Levenshtein distance similarity function for normalized strings. Caluculates the Damerau–Levenshtein distance between two strings\n",
        "# Then caluclates the similarity dividing the Damerau–Levenshtein distance over the length of the argument (string) with max length\n",
        "# Returns 0 if any of the strings in the function arguments is None\n",
        "# Version 1.2: 2020-10-23\n",
        "# Returns Damerau–Levenshtein distance similarity [0-1]\n",
        "\n",
        "def DL (x,y):\n",
        "    if x is None or y is None:\n",
        "        return 0\n",
        "    else:\n",
        "        x = norm (x)\n",
        "        y = norm (y)\n",
        "        z = max((len(x)),(len(y)))\n",
        "        #return (1-(damerau_levenshtein (x,y)/len(x)))\n",
        "        return abs((1-(damerau_levenshtein (x,y)/z)))\n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the label and stores them in a matrix [n,11] where n,11 is the label to identify if the reord is duplicated\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM1 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    #matrix1 = np.array(np.zeros(rows*11).reshape(rows,11),dtype=object)\n",
        "    matrix1 = np.zeros(rows*11).reshape(rows,11)\n",
        "    for i in range(rows):\n",
        "      if i % 10000 == 0:\n",
        "        print ('row number= ')\n",
        "        print (i)\n",
        "      for j in range(11):\n",
        "            if i % 10000 == 0:\n",
        "                print ('column number= ')\n",
        "                print (j)\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM2 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          left6,right6,\n",
        "          left7,right7,\n",
        "          left8,right8,\n",
        "          left9,right9,\n",
        "          left10,right10,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL(left5[i],right5[i])\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL(left6[i],right6[i])\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL(left7[i],right7[i])\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL(left8[i],right8[i])\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL(left9[i],right9[i])\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL(left10[i],right10[i])\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "    \n",
        "    \n",
        "# Calculates the DL for ten (10) pair attributes of a data set, and adds the pair_id and label and stores them in a matrix [n,12] where n,12 is the BIAS is the number of records\n",
        "# Returns the matrix with the calucalted DL result for each pair of attributes.\n",
        "# Special Note: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Version 1.3: 2020-04-25\n",
        "\n",
        "    \n",
        "\n",
        "def DLM3 (dset,\n",
        "          left1,right1,\n",
        "          left2,right2,\n",
        "          left3,right3,\n",
        "          left4,right4,\n",
        "          left5,right5,\n",
        "          key1,\n",
        "          key2,\n",
        "          key3,\n",
        "          key4,\n",
        "          key5,\n",
        "          key6,\n",
        "          pair_id,\n",
        "          label):\n",
        "    rows=len(dset)\n",
        "    matrix1 = np.zeros(rows*12).reshape(rows,12)\n",
        "    for i in range(rows):\n",
        "        for j in range(12):\n",
        "            if j == 0:\n",
        "                matrix1[i,j] = DL(left1[i],right1[i])\n",
        "            elif j == 1:\n",
        "                matrix1[i,j] = DL(left2[i],right2[i])\n",
        "            elif j == 2:\n",
        "                matrix1[i,j] = DL(left3[i],right3[i])\n",
        "            elif j == 3:\n",
        "                matrix1[i,j] = DL(left4[i],right4[i])\n",
        "            elif j == 4:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key1)),(multiObjects(right5[i],key1)))\n",
        "            elif j == 5:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key2)),(multiObjects(right5[i],key2)))\n",
        "            elif j == 6:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key3)),(multiObjects(right5[i],key3)))\n",
        "            elif j == 7:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key4)),(multiObjects(right5[i],key4)))\n",
        "            elif j == 8:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key5)),(multiObjects(right5[i],key5)))\n",
        "            elif j == 9:\n",
        "                matrix1[i,j] = DL((multiObjects(left5[i],key6)),(multiObjects(right5[i],key6)))\n",
        "            elif j == 10:\n",
        "                matrix1[i,j] = pair_id[i]\n",
        "            elif j == 11:\n",
        "                matrix1[i,j] = label[i]\n",
        "                \n",
        "    return matrix1\n",
        "\n",
        "# Function to create a new Data Frame with selected columns of a data set.\n",
        "# Returns the new data frame / data set with all the required columns of tuple.\n",
        "# Arguments: Data Set\n",
        "# Special Note: This function will only return as part of the new data frame:\n",
        "# Special Note2: This function has been designed specifically for the data sets of the Corpus: http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/\n",
        "# For the data sets: XX_train_[size].json, XX_js.json and its respective sample data sets.\n",
        "# Will bring data from: \"right\" and \"left\" dat set tuple\n",
        "# Version 1.0: 2020-04-24\n",
        "\n",
        "\n",
        "def dataFileAll (df1):\n",
        "    idLeft = df1.id_left\n",
        "    idRight = df1.id_right\n",
        "    brandListLeft = df1.brand_left\n",
        "    brandListRight = df1.brand_right\n",
        "    categoryListLeft = df1.category_left\n",
        "    categoryListRight = df1.category_right\n",
        "    descriptionListLeft = df1.description_left\n",
        "    descriptionListRight = df1.description_right\n",
        "    titleListLeft = df1.title_left\n",
        "    titleListRight = df1.title_right\n",
        "    gtin8ListLeft = identifiersList(df1.identifiers_left,\"/gtin8\")\n",
        "    gtin8ListRight = identifiersList(df1.identifiers_right,\"/gtin8\")\n",
        "    gtin12ListLeft = identifiersList(df1.identifiers_left,\"/gtin12\")\n",
        "    gtin12ListRight = identifiersList(df1.identifiers_right,\"/gtin12\")\n",
        "    gtin13ListLeft = identifiersList(df1.identifiers_left,\"/gtin13\")\n",
        "    gtin13ListRight = identifiersList(df1.identifiers_right,\"/gtin13\")\n",
        "    gtin14ListLeft = identifiersList(df1.identifiers_left,\"/gtin14\")\n",
        "    gtin14ListRight = identifiersList(df1.identifiers_right,\"/gtin14\")\n",
        "    mpnListLeft = identifiersList(df1.identifiers_left,\"/mpn\")\n",
        "    mpnListRight = identifiersList(df1.identifiers_right,\"/mpn\")\n",
        "    skuListLeft = identifiersList(df1.identifiers_left,\"/sku\")\n",
        "    skuListRight = identifiersList(df1.identifiers_right,\"/sku\")\n",
        "    pairIdList = df1.pair_id\n",
        "    labelList = df1.label\n",
        "    \n",
        "    \n",
        "    zipList = list(zip(idLeft\n",
        "                       ,idRight\n",
        "                       ,brandListLeft\n",
        "                       ,brandListRight\n",
        "                       ,categoryListLeft\n",
        "                       ,categoryListRight\n",
        "                       ,descriptionListLeft\n",
        "                       ,descriptionListRight\n",
        "                       ,titleListLeft\n",
        "                       ,titleListRight\n",
        "                       ,gtin8ListLeft\n",
        "                       ,gtin8ListRight\n",
        "                       ,gtin12ListLeft\n",
        "                       ,gtin12ListRight\n",
        "                       ,gtin13ListLeft\n",
        "                       ,gtin13ListRight\n",
        "                       ,gtin14ListLeft\n",
        "                       ,gtin14ListRight\n",
        "                       ,mpnListLeft\n",
        "                       ,mpnListRight\n",
        "                       ,skuListLeft\n",
        "                       ,skuListRight\n",
        "                       ,pairIdList\n",
        "                       ,labelList\n",
        "                      )\n",
        "                  )\n",
        "    dataObjectLeft = pd.DataFrame(zipList ,columns = [\"idLeft\"\n",
        "                       ,\"idRight\"\n",
        "                       ,\"brandListLeft\"\n",
        "                       ,\"brandListRight\"\n",
        "                       ,\"categoryListLeft\"\n",
        "                       ,\"categoryListRight\"\n",
        "                       ,\"descriptionListLeft\"\n",
        "                       ,\"descriptionListRight\"\n",
        "                       ,\"titleListLeft\"\n",
        "                       ,\"titleListRight\"\n",
        "                       ,\"gtin8ListLeft\"\n",
        "                       ,\"gtin8ListRight\"\n",
        "                       ,\"gtin12ListLeft\"\n",
        "                       ,\"gtin12ListRight\"\n",
        "                       ,\"gtin13ListLeft\"\n",
        "                       ,\"gtin13ListRight\"\n",
        "                       ,\"gtin14ListLeft\"\n",
        "                       ,\"gtin14ListRight\"\n",
        "                       ,\"mpnListLeft\"\n",
        "                       ,\"mpnListRight\"\n",
        "                       ,\"skuListLeft\"\n",
        "                       ,\"skuListRight\"\n",
        "                       ,\"pairIdList\"\n",
        "                       ,\"labelList\"])\n",
        "\n",
        "    return dataObjectLeft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHtnsh45svcZ"
      },
      "source": [
        "#ValidationDataSet10DL load numpy array npy in binary format\n",
        "ValidationDataSet10W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/ValidationDataSet10W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPQeh0pLsvcZ"
      },
      "source": [
        "#TrainDataSetDL load numpy array npy in binary format\n",
        "TrainDataSet90W2V_501 = np.load('/content/drive/My Drive/Python/20200720_DataAnalysis/TrainDataSet90W2V_501.npy',mmap_mode=None,allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWAL-BwUsvca"
      },
      "source": [
        "ValidationDataSet10W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nZdO7gQsvca"
      },
      "source": [
        "TrainDataSet90W2V_501[0,10].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IiyBWv6svca"
      },
      "source": [
        "ValidationDataSet10W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bNNBXDXsvcj"
      },
      "source": [
        "TrainDataSet90W2V_501.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DijJRig_svcj"
      },
      "source": [
        "ValidationDataSet10W2V_501_data,ValidationDataSet10W2V_501_label=np.split(ValidationDataSet10W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDMtPV_2svcj"
      },
      "source": [
        "TrainDataSet90W2V_501_data,TrainDataSet90W2V_501_label=np.split(TrainDataSet90W2V_501,[500],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-64EvJGCsvcj"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcVcQ94esvcr"
      },
      "source": [
        "print(np.count_nonzero(ValidationDataSet10W2V_501_label == 0))\n",
        "print(np.count_nonzero(ValidationDataSet10W2V_501_label == 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAL5-Qorsvcr"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM_0Updnx2Vf"
      },
      "source": [
        "**New TrainDataSet90W2V_501_data and ValidationDataSet10W2V_501_data**\n",
        "Reshape Numpy Array to (len(array),2,250)\n",
        "Used for New Input for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulW8QIHYxW9e"
      },
      "source": [
        "TrainDataSet90W2V_501_data = np.reshape(TrainDataSet90W2V_501_data, (len(TrainDataSet90W2V_501_data),2,250))\n",
        "ValidationDataSet10W2V_501_data = np.reshape(ValidationDataSet10W2V_501_data, (len(ValidationDataSet10W2V_501_data),2,250)) \n",
        "\n",
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8N-1In0tBZH"
      },
      "source": [
        "# Installing TALOS\n",
        "!pip install talos\n",
        "!pip install -U --no-deps talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ07BZgUtBZH"
      },
      "source": [
        "import keras as ke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY4fN6d2tBZI"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Dense, Input, InputLayer, LSTM, Lambda, Embedding, Masking, Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWILnaAztBZI"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/mikko/Documents/GitHub/talos')\n",
        "import talos as ta\n",
        "from talos.utils import lr_normalizer\n",
        "from talos.utils import hidden_layers\n",
        "from talos.utils import live\n",
        "from talos.utils import ExperimentLogCallback\n",
        "from talos.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVxy2neItBZI"
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from keras.activations import relu, sigmoid, softmax, softplus, softsign, tanh, selu, elu, exponential\n",
        "from keras.losses import binary_crossentropy, logcosh, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, KLDivergence, kullback_leibler_divergence\n",
        "from keras.metrics import AUC, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall, KLDivergence, BinaryAccuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as kb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOE0G_oTtBZI"
      },
      "source": [
        "# 2020-11-08 JXHALLO: Defininng Custom F1 Score Metric\n",
        "def custom_f1(y_true, y_pred, name='custom_f1'): #taken from old keras source code\n",
        "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
        "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
        "    recall = true_positives / (possible_positives + kb.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Db6FfxQtJZF"
      },
      "source": [
        "class AutoModelJXHL:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(Bidirectional(LSTM(params['first_neuron'],\n",
        "                                         activation = params['LSTM_activation'],\n",
        "                                         recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                                         recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                                         dropout = params['LSTM_dropout'],\n",
        "                                         unroll = False,\n",
        "                                         use_bias = True\n",
        "                                         ),\n",
        "                      input_shape =(x_train.shape[1],x_train.shape[2])\n",
        "                      )\n",
        "            )\n",
        "        \n",
        "        # model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # # add hidden layers to the model\n",
        "        # from talos.model.hidden_layers import hidden_layers\n",
        "        # hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0Eh-3g6tJZF"
      },
      "source": [
        "class AutoModelJXHL2:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(Bidirectional(LSTM(params['first_neuron'],\n",
        "                                         activation = params['LSTM_activation'],\n",
        "                                         recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                                         recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                                         dropout = params['LSTM_dropout'],\n",
        "                                         unroll = False,\n",
        "                                         use_bias = True,\n",
        "                                         return_sequences=True\n",
        "                                         ),\n",
        "                      input_shape =(x_train.shape[1],x_train.shape[2])\n",
        "                      )\n",
        "            )\n",
        "            model.add(Bidirectional(LSTM(params['first_neuron'],\n",
        "                                         activation = params['LSTM_activation'],\n",
        "                                         recurrent_activation = params['LSTM_recurrent_activation'],\n",
        "                                         recurrent_dropout = params['LSTM_recurrent_dropout'],\n",
        "                                         dropout = params['LSTM_dropout'],\n",
        "                                         unroll = False,\n",
        "                                         use_bias = True\n",
        "                                         )\n",
        "                      )\n",
        "            )\n",
        "\n",
        "        # model.add(Dropout(params['dropout']))\n",
        "        \n",
        "        # New Dense Layer Proposed by Teacher Gloria Alvarez\n",
        "        \n",
        "        model.add(Dense(((params['first_neuron'])/2),\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "        \n",
        "        # add hidden layers to the model\n",
        "        # from talos.model.hidden_layers import hidden_layers\n",
        "        # hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH1-gpbWtJZF"
      },
      "source": [
        "class AutoModelJXHL3:\n",
        "\n",
        "    def __init__(self, task, experiment_name, metric=None):\n",
        "\n",
        "        '''\n",
        "        Creates an input model for Scan(). Optimized for being used together\n",
        "        with Params(). For example:\n",
        "        p = talos.AutoParams().params\n",
        "        model = talos.AutoModel(task='binary').model\n",
        "        talos.Scan(x, y, p, model)\n",
        "        NOTE: the parameter space from Params() is very large, so use limits\n",
        "        in or reducers in Scan() accordingly.\n",
        "        task : string or None\n",
        "            If 'continuous' then mae is used for metric, if 'binary',\n",
        "            'multiclass', or 'multilabel', f1score is used. Accuracy is always\n",
        "            used.\n",
        "        experiment_name | str | Must be same as in `Scan()`\n",
        "        metric : None or list\n",
        "            You can also input a list with one or more custom metrics or names\n",
        "            of Keras or Talos metrics.\n",
        "        '''\n",
        "\n",
        "        from talos.utils.experiment_log_callback import ExperimentLogCallback\n",
        "\n",
        "        self.task = task\n",
        "        self.experiment_name = experiment_name\n",
        "        self.metric = metric\n",
        "\n",
        "        if self.task is not None:\n",
        "            self.metrics = self._set_metric()\n",
        "        elif self.metric is not None and isinstance(self.metric, list):\n",
        "            self.metrics = self.metric + ['acc']\n",
        "        else:\n",
        "            print(\"Either pick task or provide list as input for metric.\")\n",
        "\n",
        "        # create the model\n",
        "        self.model = self._create_input_model\n",
        "        self.callback = ExperimentLogCallback\n",
        "\n",
        "    def _set_metric(self):\n",
        "\n",
        "        \"\"\"Sets the metric for the model based on the experiment type\n",
        "        or a list of metrics from user.\"\"\"\n",
        "\n",
        "        import talos as ta\n",
        "\n",
        "        if self.task in ['binary', 'multiclass', 'multilabel']:\n",
        "            return [ta.utils.metrics.f1score, 'acc']\n",
        "        elif self.task == 'continuous':\n",
        "            return [ta.utils.metrics.mae, 'acc']\n",
        "\n",
        "    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "        import wrangle as wr\n",
        "\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dropout, Flatten\n",
        "        from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if params['network'] == 'lstm':\n",
        "            model.add(LSTM(params['first_neuron'],\n",
        "                           input_shape =(x_train.shape[1],x_train.shape[2]),                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'],                    \n",
        "                           return_sequences=True)\n",
        "            )\n",
        "            model.add(LSTM(params['first_neuron'])\n",
        "            )\n",
        "\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # add hidden layers to the model\n",
        "        from talos.model.hidden_layers import hidden_layers\n",
        "        hidden_layers(model, params, 1)\n",
        "\n",
        "        model.add(Dense(1,\n",
        "                        activation=params['last_activation'],\n",
        "                        kernel_initializer=params['kernel_initializer']))\n",
        "\n",
        "        # bundle the optimizer with learning rate changes\n",
        "        from talos.model.normalizers import lr_normalizer\n",
        "        optimizer = params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer']))\n",
        "\n",
        "        # compile the model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=params['losses'],\n",
        "                      metrics=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),ke.metrics.TruePositives(name='truePositives'),ke.metrics.TrueNegatives(name='trueNegatives'),ke.metrics.FalsePositives(name='falsePositives'),ke.metrics.FalseNegatives(name='falseNegatives'),ke.metrics.Precision(name='precision'),ke.metrics.Recall(name='recall'),ta.utils.metrics.f1score,custom_f1]\n",
        "                      )\n",
        "        \n",
        "        # plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        # model.summary()\n",
        "\n",
        "        # fit the model\n",
        "        out = model.fit(x_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        verbose=0,\n",
        "                        # callbacks=[self.callback(self.experiment_name, params)],\n",
        "                        validation_data=(x_val, y_val)\n",
        "                        )\n",
        "\n",
        "        plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "        model.summary()\n",
        "        # pass the output to Talos\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4BAejzLtJZG"
      },
      "source": [
        "autoParams2 = {'activation': ['relu'],\n",
        "               'LSTM_activation': ['relu'],\n",
        "               'LSTM_recurrent_activation': ['sigmoid'],\n",
        "               'LSTM_recurrent_dropout': [0.5],\n",
        "               'LSTM_dropout': [0],\n",
        "               'batch_size': [32],\n",
        "               'dropout': [0],\n",
        "               'epochs': [100],\n",
        "               'first_neuron': [1024],\n",
        "               'hidden_layers': [0],\n",
        "               'kernel_initializer': ['normal'],\n",
        "               'last_activation': ['sigmoid'],\n",
        "               'losses': ['binary_crossentropy'],\n",
        "               'lr': [0.5],\n",
        "               'network': ['lstm'],\n",
        "               'optimizer': [Adam],\n",
        "               'shapes': ['brick'],\n",
        "               'weight_regulizer':[None],\n",
        "               'emb_output_dims': [None],\n",
        "               'shape':['brick']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxXfHMeNtJZG"
      },
      "source": [
        "autoParams2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWbujAoztJZG"
      },
      "source": [
        "autoModel1= ta.autom8.AutoModel(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeblumOAtJZG"
      },
      "source": [
        "autoModel1JXHL_W2V_501= AutoModelJXHL(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel1JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6KaZU0ttJZG"
      },
      "source": [
        "autoModel2JXHL_W2V_501= AutoModelJXHL2(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel2JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b5HLueAtJZH"
      },
      "source": [
        "autoModel3JXHL_W2V_501= AutoModelJXHL3(task=None, \n",
        "                                metric=[ke.metrics.BinaryAccuracy(name='binaryAccuracy'),\n",
        "                                        ke.metrics.TruePositives(name='truePositives'),\n",
        "                                        ke.metrics.TrueNegatives(name='trueNegatives'),\n",
        "                                        ke.metrics.FalsePositives(name='falsePositives'),\n",
        "                                        ke.metrics.FalseNegatives(name='falseNegatives'),\n",
        "                                        ke.metrics.Precision(name='precision'),\n",
        "                                        ke.metrics.Recall(name='recall'),\n",
        "                                        ta.utils.metrics.f1score,\n",
        "                                        custom_f1],\n",
        "                                experiment_name = 'autoModel3JXHL_W2V_501').model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOJKncRCtXyi"
      },
      "source": [
        "print (ValidationDataSet10W2V_501_data.shape)\n",
        "print (ValidationDataSet10W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP-UdoowtXyj"
      },
      "source": [
        "print (TrainDataSet90W2V_501_data.shape)\n",
        "print (TrainDataSet90W2V_501_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv9tb-XFtXyj"
      },
      "source": [
        "# then we load the dataset\n",
        "x = TrainDataSet90W2V_501_data\n",
        "y = TrainDataSet90W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loq7Qp7itXyj"
      },
      "source": [
        "print (x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIXBWxcjtXyj"
      },
      "source": [
        "# then we load the dataset\n",
        "x_val = ValidationDataSet10W2V_501_data\n",
        "y_val = ValidationDataSet10W2V_501_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUMPpIAYtXyk"
      },
      "source": [
        "print (x_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW6HYkbFtXyk"
      },
      "source": [
        "# and run the experiment\n",
        "t = ta.Scan(x=x,\n",
        "            y=y,\n",
        "            # val_split=0.1,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            model=autoModel2JXHL_W2V_501,\n",
        "            params=autoParams2,\n",
        "            experiment_name='autoModel2JXHL_W2V_501',\n",
        "            # fraction_limit=0.5,\n",
        "            time_limit = \"2021-04-12 19:00\",\n",
        "            # round_limit=2,\n",
        "            print_params=True,\n",
        "            reduction_metric='val_f1score',\n",
        "            minimize_loss=False,\n",
        "            save_weights=True,\n",
        "            clear_session=False,\n",
        "            performance_target=['val_f1score',0.65,False]        \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMyldWCBtXyl"
      },
      "source": [
        "print (t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoLgd7YetXyl"
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = ta.Analyze(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv5M3g5RtXyl"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')\n",
        "# 2020-11-07 JUHO: Saving multi_layer_percetron_FFNN_analyze_object after talos scan for multi_layer_percetron_FFNN_W2V_501 with pArch parameters\n",
        "analyze_object.data.to_pickle('/content/drive/My Drive/Python/20200720_DataAnalysis/20210412_autoModel2JXHL_W2V_501_Opt_BiLSTM_autoParams2_v10.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dd5_3EltXyl"
      },
      "source": [
        "analyze_object.data.to_excel('/content/drive/MyDrive/Python/20200720_DataAnalysis/20210412_autoModel2JXHL_W2V_501_Opt_BiLSTM_autoParams2_v10.xlsx', sheet_name='20210412_autoModel2JXHL_W2V_Opt_Bi', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgJnFl1CtXyl"
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}